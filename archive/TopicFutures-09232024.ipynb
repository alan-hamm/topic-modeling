{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using chatCDC to a limited degree(~20%) for support programming and Q/A\n",
    "of various programming methods and questions about various programming solutions(e.g. \"of this list of code options to solve X what can \n",
    "you tell me about code A and Code B?\").\n",
    "\n",
    "Sources: Official Gensim and Gensim communities, Dask documentation and Dask communities were used for review, along with a Brobdingnagian crawl of virtual-space(e.g. blogs, personal sites, stack exchange, etc.) which were \n",
    "on more occasions than not *several* years to the n^th degree old.\n",
    "\n",
    "authors: alan hamm(pqn7)\n",
    "         bertha(chatCDC)\n",
    "         \n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim  # Library for interactive topic model visualization\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "from mpld3 import save_html\n",
    "\n",
    "\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel  # Computes coherence scores for topic models\n",
    "import pyLDAvis\n",
    "import IProgress \n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time, sleep # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import pandas as pd\n",
    "import logging # Logging module for generating log messages\n",
    "import sys # Provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter \n",
    "import shutil # High-level file operations such as copying and removal \n",
    "import zipfile # Provides tools to create, read, write, append, and list a ZIP file\n",
    "from tqdm.notebook import tqdm  # Creates progress bars in Jupyter Notebook environment\n",
    "from json import load\n",
    "import random\n",
    "import logging\n",
    "import csv\n",
    "import pprint as pp\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from typing import Union, List\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from dask.distributed import as_completed\n",
    "import dask   # Parallel computing library that scales Python workflows across multiple cores or machines \n",
    "from dask.distributed import Client, LocalCluster, wait   # Distributed computing framework that extends Dask functionality \n",
    "from dask.diagnostics import ProgressBar   # Visualizes progress of Dask computations\n",
    "from dask.distributed import progress\n",
    "from distributed import Future\n",
    "from dask.delayed import Delayed # Decorator for creating delayed objects in Dask computations\n",
    "#from dask.distributed import as_completed\n",
    "from dask.bag import Bag\n",
    "from dask import delayed\n",
    "import dask.config\n",
    "#from dask.distributed import wait\n",
    "from dask.distributed import performance_report, wait, as_completed #,print\n",
    "from distributed import get_worker\n",
    "import gc\n",
    "import hashlib\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "DECADE_TO_PROCESS ='2010s'\n",
    "LOG_DIRECTORY = f\"C:/_harvester/data/lda-models/{DECADE_TO_PROCESS}_html/log/\"\n",
    "# Ensure the LOG_DIRECTORY exists\n",
    "os.makedirs(LOG_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as per your requirement\n",
    "# Note: %w is the day of the week as a decimal (0=Sunday, 6=Saturday)\n",
    "#       %Y is the four-digit year\n",
    "#       %m is the two-digit month (01-12)\n",
    "#       %H%M is the hour (00-23) followed by minute (00-59) in 24hr format\n",
    "log_filename = now.strftime('log-%w-%m-%Y-%H%M.log')\n",
    "LOGFILE = os.path.join(LOG_DIRECTORY,log_filename)\n",
    "\n",
    "# Configure logging to write to a file with this name\n",
    "logging.basicConfig(\n",
    "    filename=LOGFILE,\n",
    "    filemode='a',  # Append mode if you want to keep adding to the same file during the day\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Now when you use logging.info(), logging.debug(), etc., it will write to that log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "from numpy import ComplexWarning\n",
    "\n",
    "# Suppress ComplexWarnings\n",
    "# generated in create_vis() function with js_PCoA use of matplotlib\n",
    "warnings.simplefilter('ignore', ComplexWarning)\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "# Filter out the specific warning message\n",
    "# Set the logging level for distributed.utils_perf to suppress warnings\n",
    "logging.getLogger('distributed.utils_perf').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", module=\"distributed.utils_perf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the range of number of topics for LDA and step size\n",
    "START_TOPICS = 20\n",
    "END_TOPICS = 120\n",
    "STEP_SIZE = 5\n",
    "\n",
    "# define the decade that is being modelled \n",
    "DECADE = DECADE_TO_PROCESS\n",
    "\n",
    "# In the case of this machine, since it has an Intel Core i9 processor with 8 physical cores (16 threads with Hyper-Threading), \n",
    "# it would be appropriate to set the number of workers in Dask Distributed LocalCluster to 8 or slightly lower to allow some CPU \n",
    "# resources for other tasks running on your system.\n",
    "CORES = 8\n",
    "MAXIMUM_CORES = 12\n",
    "\n",
    "THREADS_PER_CORE = 8\n",
    "\n",
    "RAM_MEMORY_LIMIT = \"16GB\" \n",
    "\n",
    "CPU_UTILIZATION_THRESHOLD = 125 # eg 85%\n",
    "MEMORY_UTILIZATION_THRESHOLD = 12 * (1024 ** 3)  # Convert GB to bytes\n",
    "\n",
    "# Specify the local directory path, spilling will be written here\n",
    "DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "# specify the number of passes for Gensim LdaModel\n",
    "PASSES = 15\n",
    "\n",
    "# specify the number of iterations\n",
    "ITERATIONS = 50\n",
    "\n",
    "# Number of documents to be iterated through for each update. \n",
    "# Set to 0 for batch learning, > 1 for online iterative learning.\n",
    "UPDATE_EVERY = 5\n",
    "\n",
    "# Log perplexity is estimated every that many updates. \n",
    "# Setting this to one slows down training by ~2x.\n",
    "EVAL_EVERY = 10\n",
    "\n",
    "RANDOM_STATE = 75\n",
    "\n",
    "PER_WORD_TOPICS = True\n",
    "\n",
    "# number of documents to extract from the JSON source file when testing and developing\n",
    "NUM_DOCUMENTS = 25\n",
    "\n",
    "# the number of documents to read from the JSON source file per batch\n",
    "FUTURES_BATCH_SIZE = 100\n",
    "\n",
    "# Constants for adaptive batching and retries\n",
    "# Number of futures to process per iteration\n",
    "BATCH_SIZE = 75 # number of documents\n",
    "MAX_BATCH_SIZE = 110 \n",
    "INCREASE_FACTOR = 1.05  # Increase batch size by p% upon success\n",
    "DECREASE_FACTOR = .10 # Decrease batch size by p% upon failure or timeout\n",
    "MAX_RETRIES = 5        # Maximum number of retries per task\n",
    "BASE_WAIT_TIME = 30     # Base wait time in seconds for exponential backoff\n",
    "\n",
    "\n",
    "# Load data from the JSON file\n",
    "DATA_SOURCE = \"C:/_harvester/data/tokenized-sentences/10s/2010-2014_min_six_word-w-bigrams.json\"\n",
    "TRAIN_RATIO = .80\n",
    "\n",
    "TIMEOUT = None #\"90 minutes\"\n",
    "\n",
    "EXTENDED_TIMEOUT = None #\"120 minutes\"\n",
    "\n",
    "# Enable serialization optimizations\n",
    "dask.config.set(scheduler='distributed', serialize=True)\n",
    "dask.config.set({'logging.distributed': 'error'})\n",
    "dask.config.set({\"distributed.scheduler.worker-ttl\": None})\n",
    "#dask.config.set({\"distributed.scheduler.worker-ttl\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def garbage_collection(development: bool, location: str):\n",
    "    if development:\n",
    "        # Enable debugging flags for leak statistics\n",
    "        gc.set_debug(gc.DEBUG_LEAK)\n",
    "\n",
    "    # Before calling collect, get a count of existing objects\n",
    "    before = len(gc.get_objects())\n",
    "\n",
    "    # Perform garbage collection\n",
    "    collected = gc.collect()\n",
    "\n",
    "    # After calling collect, get a new count of existing objects\n",
    "    after = len(gc.get_objects())\n",
    "\n",
    "    # Print or log before and after counts along with number collected\n",
    "    logging.info(f\"Garbage Collection at {location}:\")\n",
    "    logging.info(f\"  Before GC: {before} objects\")\n",
    "    logging.info(f\"  After GC: {after} objects\")\n",
    "    logging.info(f\"  Collected: {collected} objects\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# Define the top-level directory and subdirectories\n",
    "DECADE = \"2010s\"  # Replace with your actual decade value\n",
    "ROOT_DIR = f\"C:/_harvester/data/lda-models/{DECADE}_html\"\n",
    "LOG_DIR = os.path.join(ROOT_DIR, \"log\")\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"visuals\")\n",
    "PYLDA_DIR = os.path.join(IMAGE_DIR, 'pyLDAvis')\n",
    "PCOA_DIR = os.path.join(IMAGE_DIR, 'PCoA')\n",
    "METADATA_DIR = os.path.join(ROOT_DIR, \"metadata\")\n",
    "TEXTS_ZIP_DIR = os.path.join(ROOT_DIR, \"texts_zip\")\n",
    "\n",
    "# Ensure that all necessary directories exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(PYLDA_DIR, exist_ok=True)\n",
    "os.makedirs(PCOA_DIR, exist_ok=True)\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEXTS_ZIP_DIR, exist_ok=True)\n",
    "\n",
    "# Function to save text data to a zip file and return the path\n",
    "def save_text_to_zip(text_data):\n",
    "    # Generate a unique filename based on current timestamp\n",
    "    timestamp_str = pd.Timestamp.now().strftime('%Y%m%d%H%M%S%f')\n",
    "    text_zip_filename = f\"{timestamp_str}.zip\"\n",
    "    \n",
    "    # Write the text content to a zip file within TEXTS_ZIP_DIR\n",
    "    zip_path = os.path.join(TEXTS_ZIP_DIR, text_zip_filename)\n",
    "    with zipfile.ZipFile(zip_path, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.writestr(\"text.txt\", text_data)\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Function to save text data and model to single ZIP file\n",
    "def save_to_zip(time, text_data, ldamodel):\n",
    "    # Generate a unique filename based on current timestamp\n",
    "    timestamp_str = hashlib.md5(time.strftime('%Y%m%d%H%M%S%f').encode()).hexdigest()\n",
    "    text_zip_filename = f\"{timestamp_str}.zip\"\n",
    "    \n",
    "    # Write the text content and model to a zip file within TEXTS_ZIP_DIR\n",
    "    zip_path = os.path.join(TEXTS_ZIP_DIR, text_zip_filename)\n",
    "    with zipfile.ZipFile(zip_path, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.writestr(f\"doc_{text_zip_filename}.txt\", text_data)\n",
    "        ldamodel_bytes = pickle.dumps(ldamodel)\n",
    "        zf.writestr(f\"model_{text_zip_filename}.pkl\", ldamodel_bytes)\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# method to deserialize and return the LDA model object\n",
    "def load_pkl_from_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        pkl_files = [file for file in zf.namelist() if file.endswith('.pkl')]\n",
    "        if len(pkl_files) == 0:\n",
    "            raise ValueError(\"No pkl files found in the ZIP archive.\")\n",
    "        \n",
    "        pkl_file = pkl_files[0]\n",
    "        pkl_bytes = zf.read(pkl_file)\n",
    "        loaded_pkl = pickle.loads(pkl_bytes)\n",
    "    \n",
    "    return loaded_pkl\n",
    "\n",
    "# Function to add new model data to metadata Parquet file\n",
    "def add_model_data_to_metadata(model_data, workers, batchsize):\n",
    "    #print(\"we are in the add_model_data_to_metadata method()\")\n",
    "    # Save large body of text to zip and update model_data reference\n",
    "    texts_zipped = []\n",
    "    \n",
    "    #for text_list in model_data['text']:\n",
    "    for text_list in model_data['text']:\n",
    "        combined_text = ''.join([''.join(sent) for sent in text_list])  # Combine all sentences into one string\n",
    "        zip_path = save_to_zip(model_data['time'], combined_text, model_data['lda_model'])\n",
    "        texts_zipped.append(zip_path)\n",
    "    # Update model data with zipped paths\n",
    "    model_data['text'] = texts_zipped\n",
    "     # Ensure other fields are not lists, or if they are, they should have only one element per model\n",
    "    for key, value in model_data.items():\n",
    "        #if isinstance(value, list) and key != 'text':\n",
    "        if isinstance(value, list) and key not in ['text', 'top_words']:\n",
    "            assert len(value) == 1, f\"Field {key} has multiple elements\"\n",
    "            model_data[key] = value[0]  # Unwrap single-element list\n",
    "               \n",
    "    # Define the expected data types for each column\n",
    "    expected_dtypes = {\n",
    "        'type': str,\n",
    "        'num_workers': int,\n",
    "        'batch_size': int,\n",
    "        'text': object,  # Use object dtype for lists of strings (file paths)\n",
    "        'text_sha256': str,\n",
    "        'text_md5': str,\n",
    "        'corpus': object,\n",
    "        'dictionary': object,\n",
    "        'convergence': 'float32',\n",
    "        'perplexity': 'float32',\n",
    "        'coherence': 'float32',\n",
    "        'topics': int,\n",
    "        # Use pd.Categorical.dtype for categorical columns\n",
    "        # Ensure alpha and beta are already categorical when passed into this function\n",
    "        # They should not be wrapped again with CategoricalDtype here.\n",
    "        'alpha_str': str,\n",
    "        'n_alpha': 'float32',\n",
    "        'beta_str': str,\n",
    "        'n_beta': 'float32',\n",
    "        'passes': int,\n",
    "        'iterations': int,\n",
    "        'update_every': int,\n",
    "        'eval_every': int,\n",
    "        'chunksize': int,\n",
    "        'random_state': int,\n",
    "        'per_word_topics': bool,\n",
    "        'top_words': object,\n",
    "        'lda_model': object,\n",
    "        #'create_pylda': bool, \n",
    "        #'create_pcoa': bool, \n",
    "        # Enforce datetime type for time\n",
    "        'time': 'datetime64[ns]',\n",
    "    }   \n",
    "\n",
    "    \n",
    "    try:\n",
    "        #df_new_metadata = pd.DataFrame({key: [value] if not isinstance(value, list) else value \n",
    "        #                                for key, value in model_data.items()}).astype(expected_dtypes)\n",
    "        # Create a new DataFrame without enforcing dtypes initially\n",
    "        df_new_metadata = pd.DataFrame({key: [value] if not isinstance(value, list) else value \n",
    "                                        for key, value in model_data.items()})\n",
    "        \n",
    "        # Apply type conversion selectively\n",
    "        #for col_name in ['convergence', 'perplexity', 'coherence', 'n_beta', 'n_alpha']:\n",
    "        for col_name in ['convergence', 'perplexity', 'coherence', 'n_beta', 'n_alpha']:\n",
    "            df_new_metadata[col_name] = df_new_metadata[col_name].astype('float64')\n",
    "            \n",
    "        df_new_metadata['topics'] = df_new_metadata['topics'].astype(int)\n",
    "        #df_new_metadata['time'] = pd.to_datetime(df_new_metadata['time'])\n",
    "        df_new_metadata['batch_size'] = batchsize\n",
    "        df_new_metadata['num_workers'] = workers\n",
    "        #df_new_metadata['create_pylda'] = pylda_success\n",
    "        #df_new_metadata['create_pcoa'] = pcoa_success\n",
    "        # drop lda model from dataframe\n",
    "        df_new_metadata = df_new_metadata.drop('dictionary', axis=1)\n",
    "        df_new_metadata = df_new_metadata.drop('corpus', axis=1)\n",
    "        df_new_metadata = df_new_metadata.drop('lda_model', axis=1)\n",
    "    except ValueError as e:\n",
    "        # Initialize an error message list\n",
    "        error_messages = [f\"Error converting model_data to DataFrame with enforced dtypes: {e}\"]\n",
    "        \n",
    "        \n",
    "        # Iterate over each item in model_data to collect its key, expected dtype, and actual value\n",
    "        for key, value in model_data.items():\n",
    "            expected_dtype = expected_dtypes.get(key, 'No expected dtype specified')\n",
    "            actual_dtype = type(value).__name__\n",
    "            error_messages.append(f\"Column: {key}, Expected dtype: {expected_dtype}, Actual dtype: {actual_dtype}, Value: {value}\")\n",
    "        \n",
    "        # Join all error messages into a single string\n",
    "        full_error_message = \"\\n\".join(error_messages)\n",
    "\n",
    "        logging.error(full_error_message)\n",
    "\n",
    "        raise ValueError(\"Data type mismatch encountered during DataFrame conversion. Detailed log available.\")\n",
    "\n",
    "    # Path to the metadata Parquet file\n",
    "    parquet_file_path = os.path.join(METADATA_DIR, \"metadata.parquet\")\n",
    "\n",
    "    # Check if the Parquet file already exists\n",
    "    if os.path.exists(parquet_file_path): \n",
    "        # If it exists, read the existing metadata and append the new data \n",
    "        df_metadata = pd.read_parquet(parquet_file_path) \n",
    "        df_metadata = pd.concat([df_metadata, df_new_metadata], ignore_index=True) \n",
    "    else: \n",
    "        # If it doesn't exist, use the new data as the starting point \n",
    "        df_metadata = df_new_metadata\n",
    "\n",
    "\n",
    "    # Save updated metadata DataFrame back to Parquet file\n",
    "    df_metadata.to_parquet(parquet_file_path)\n",
    "    del df_metadata, df_new_metadata, model_data\n",
    "    #garbage_collection(True, 'add_model_data_to_metadata(...)')\n",
    "    #print(\"\\nthis is the value of the parquet file\")\n",
    "    #print(df_metadata)\n",
    "\n",
    "\n",
    "# Function to read a specific text from its zip file based on metadata query\n",
    "def get_text_from_zip(zip_path): \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf: \n",
    "        return zf.read('text.txt').decode('utf-8')\n",
    "\n",
    "# Example usage: Load metadata and retrieve texts based on some criteria\n",
    "def load_texts_for_analysis(metadata_path, coherence_threshold=0.7): \n",
    "    # Load the metadata into a DataFrame \n",
    "    df_metadata = pd.read_parquet(metadata_path)\n",
    "\n",
    "    # Filter metadata based on some criteria (e.g., coherence > threshold)\n",
    "    filtered_metadata = df_metadata[df_metadata['coherence'] > coherence_threshold]\n",
    "\n",
    "    # Retrieve and decompress associated texts from their zip files\n",
    "    texts = [get_text_from_zip(zip_path) for zip_path in filtered_metadata['text']]\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_topics = len(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE))\n",
    "\n",
    "# Calculate numeric_alpha for symmetric prior\n",
    "numeric_symmetric = 1.0 / num_topics\n",
    "# Calculate numeric_alpha for asymmetric prior (using best judgment)\n",
    "numeric_asymmetric = 1.0 / (num_topics + np.sqrt(num_topics))\n",
    "# Create the list with numeric values\n",
    "numeric_alpha = [numeric_symmetric, numeric_asymmetric] + np.arange(0.01, 1, 0.3).tolist()\n",
    "numeric_beta = [numeric_symmetric] + np.arange(0.01, 1, 0.3).tolist()\n",
    "\n",
    "\n",
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = ['symmetric', 'asymmetric']\n",
    "alpha_values += np.arange(0.01, 1, 0.3).tolist()\n",
    "\n",
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = ['symmetric']\n",
    "beta_values += np.arange(0.01, 1, 0.3).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def calculate_numeric_alpha(alpha_str, num_topics=num_topics):\n",
    "    if alpha_str == 'symmetric':\n",
    "        return Decimal('1.0') / num_topics\n",
    "    elif alpha_str == 'asymmetric':\n",
    "        return Decimal('1.0') / (num_topics + Decimal(num_topics).sqrt())\n",
    "    else:\n",
    "        # Use Decimal for arbitrary precision\n",
    "        return Decimal(alpha_str)\n",
    "\n",
    "def calculate_numeric_beta(beta_str, num_topics=num_topics):\n",
    "    if beta_str == 'symmetric':\n",
    "        return Decimal('1.0') / num_topics\n",
    "    else:\n",
    "        # Use Decimal for arbitrary precision\n",
    "        return Decimal(beta_str)\n",
    "\n",
    "def validate_alpha_beta(alpha_str, beta_str):\n",
    "    valid_strings = ['symmetric', 'asymmetric']\n",
    "    if isinstance(alpha_str, str) and alpha_str not in valid_strings:\n",
    "        logging.error(f\"Invalid alpha_str value: {alpha_str}. Must be 'symmetric', 'asymmetric', or a numeric value.\")\n",
    "        raise ValueError(f\"Invalid alpha_str value: {alpha_str}. Must be 'symmetric', 'asymmetric', or a numeric value.\")\n",
    "    if isinstance(beta_str, str) and beta_str not in valid_strings:\n",
    "        logging.error(f\"Invalid beta_str value: {beta_str}. Must be 'symmetric', or a numeric value.\")\n",
    "        raise ValueError(f\"Invalid beta_str value: {beta_str}. Must be 'symmetric', or a numeric value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!!! DO NOT EXECUTE THIS CELL OR ANY CELL USING IT WITHOUT FIRSST\n",
    "!!! UPDATING THE OUTPUT FILEPATH FOR THE TRAINING AND EVAL DATA\n",
    "\"\"\"\n",
    "import os\n",
    "from json import load\n",
    "import random\n",
    "\n",
    "def get_num_records(filename):\n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = load(jsonfile)\n",
    "        data = data\n",
    "        num_samples = len(data)  # Count the total number of samples\n",
    "    return num_samples\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import shuffle\n",
    "\n",
    "def load(jsonfile):\n",
    "    return json.load(jsonfile)\n",
    "\n",
    "def futures_create_lda_datasets(filename, train_ratio, batch_size=FUTURES_BATCH_SIZE):\n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = load(jsonfile)\n",
    "        print(f\"the number of records read from the JSON file: {len(data)}\")\n",
    "        num_samples = len(data)  # Count the total number of samples\n",
    "        #print(f\"the number of documents sampled from the JSON file: {len(data)}\\n\")\n",
    "        \n",
    "        # Shuffle data indices since we can't shuffle actual lines in a file efficiently\n",
    "        indices = list(range(num_samples))\n",
    "        shuffle(indices)\n",
    "        \n",
    "        num_train_samples = int(num_samples * train_ratio)  # Calculate number of samples for training\n",
    "        \n",
    "        cumulative_count = 0  # Initialize cumulative count\n",
    "        # Initialize counters for train and eval datasets\n",
    "        train_count = 0\n",
    "        eval_count = num_train_samples\n",
    "        \n",
    "        # Yield batches as dictionaries for both train and eval datasets along with their sample count\n",
    "        while train_count < num_train_samples or eval_count < num_samples:\n",
    "            if train_count < num_train_samples:\n",
    "                # Yield a training batch\n",
    "                train_indices_batch = indices[train_count:train_count + batch_size]\n",
    "                train_data_batch = [data[idx] for idx in train_indices_batch]\n",
    "                if len(train_data_batch) > 0:\n",
    "                    yield {\n",
    "                        'type': 'train',\n",
    "                        'data': train_data_batch,\n",
    "                        'indices_batch': train_indices_batch,\n",
    "                        'cumulative_count': train_count,\n",
    "                        'num_samples': num_train_samples,\n",
    "                        'whole_dataset': data[:num_train_samples]\n",
    "                    }\n",
    "                    train_count += len(train_data_batch)\n",
    "                    cumulative_count += train_count\n",
    "            \n",
    "            if (eval_count < num_samples or train_count >= num_train_samples):\n",
    "                # Yield an evaluation batch\n",
    "                #print(\"we are in the method to create the futures trying to create the eval data.\")\n",
    "                #print(f\"the eval count is {eval_count} and the train count is {train_count} and the num train samples is {num_train_samples}\\n\")\n",
    "                eval_indices_batch = indices[eval_count:eval_count + batch_size]\n",
    "                eval_data_batch = [data[idx] for idx in eval_indices_batch]\n",
    "                #print(f\"This is the size of the eval_data_batch from the create futures method {len(eval_data_batch)}\\n\")\n",
    "                if len(eval_data_batch) > 0:\n",
    "                    yield {\n",
    "                        'type': 'eval',\n",
    "                        'data': eval_data_batch,\n",
    "                        'indices_batch': eval_indices_batch,\n",
    "                        'cumulative_count': num_train_samples - eval_count,\n",
    "                        'num_samples': num_train_samples - num_samples,\n",
    "                        'whole_dataset': data[num_train_samples:]\n",
    "                    }\n",
    "                    eval_count += len(eval_data_batch)\n",
    "                    cumulative_count += eval_count\n",
    "                \n",
    "    #garbage_collection(False,'futures_create_lda_datasets(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and eval dictionaries used in train_model(...) method\n",
    "def create_dictionary(filename):\n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = load(jsonfile)\n",
    "        num_samples = len(data)  # Count the total number of samples\n",
    "        logging.info(f\"The min six with bigrams has {num_samples} sentences\")\n",
    "        return data\n",
    "#minfivedict = create_dictionary(DATA_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to interpret the JS-PCoA plot:\n",
    "\n",
    "Points: Each point in the plot represents a topic from your topic model.\n",
    "\n",
    "Distance Between Points: The Euclidean distance between any two points approximates the Jensen-Shannon Divergence between those topics' distributions. Topics that are closer together have more similar word distributions and are therefore more related to each other. \n",
    "\n",
    "Axes: The horizontal and vertical axes correspond to the first two principal coordinates that result from the PCoA. These axes do not have inherent meaning like \"x\" or \"y\" in spatial coordinates; instead, they are abstract dimensions that best preserve the pairwise distances (Jensen-Shannon Divergences) among topics.\n",
    "\n",
    "Explained Variance: Sometimes, alongside the plot, you may see information about how much variance each principal coordinate explains. This gives you an idea of how well these two dimensions capture differences among topics. \n",
    "\n",
    "Clusters: If you notice clusters of points, this suggests groups of topics with similar word distributions. \n",
    "\n",
    "It's important to note that while PCoA helps visualize complex relationships between topics by reducing dimensionality, some information is inevitably lost in this process. Therefore, while it provides valuable insights into how topics relate to one another overall, it should be complemented with other forms of analysis for comprehensive interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@dask.delayed\n",
    "def create_vis(ldaModel, filename, corpus, dictionary):\n",
    "    create_pylda = False\n",
    "    create_pcoa = False\n",
    "    PCoAfilename = filename\n",
    "    filename = filename + '.html'\n",
    "    \n",
    "\n",
    "    IMAGEFILE = os.path.join(PYLDA_DIR,filename)\n",
    "    PCoAIMAGEFILE = os.path.join(PCOA_DIR, PCoAfilename)\n",
    "\n",
    "    # Disable notebook mode since we're saving to HTML.\n",
    "    pyLDAvis.disable_notebook()\n",
    "    \n",
    "    # Prepare the visualization data.\n",
    "    # Note: sort_topics=False will prevent reordering topics after training.\n",
    "    try:\n",
    "        vis = pyLDAvis.gensim.prepare(ldaModel, corpus, dictionary,  n_jobs=int(CORES*(2/3)), sort_topics=False)\n",
    "\n",
    "        pyLDAvis.save_html(vis, IMAGEFILE)\n",
    "        create_pylda = True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"The pyLDAvis HTML could not be saved: {e}\")\n",
    "        create_pylda = False\n",
    "\n",
    "\n",
    "    # try Jensen-Shannon Divergence & Principal Coordinate Analysis (aka Classical Multidimensional Scaling)\n",
    "    topic_distributions = [ldaModel.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
    "\n",
    "    # Ensure all topics are represented even if their probability is 0\n",
    "    num_topics = ldaModel.num_topics\n",
    "    distributions_matrix = np.zeros((len(corpus), num_topics))\n",
    "\n",
    "    for i, doc_topics in enumerate(topic_distributions):\n",
    "        for topic_num, prob in doc_topics:\n",
    "            distributions_matrix[i, topic_num] = prob\n",
    "    \n",
    "    try: \n",
    "        pcoa_results = pyLDAvis.js_PCoA(distributions_matrix) \n",
    "\n",
    "        # Assuming pcoa_results is a NumPy array with shape (n_dists, 2)\n",
    "        x = pcoa_results[:, 0]  # X-coordinates\n",
    "        y = pcoa_results[:, 1]  # Y-coordinates\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(x, y)  # Create a scatter plot of the PCoA results\n",
    "        plt.title('PCoA Results')\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "\n",
    "        # Save the figure as an image\n",
    "        plt.savefig(f'{PCoAIMAGEFILE}.jpg')\n",
    "\n",
    "        # If you want to save it as an HTML file instead:\n",
    "        #save_html(plt.gcf(), f'{PCoAIMAGEFILE}.html')\n",
    "\n",
    "        plt.close('all')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        #garbage_collection(True,'Create Vis')\n",
    "        \n",
    "        create_pcoa = True\n",
    "    except Exception as e: \n",
    "        logging.error(f\"An error occurred during PCoA transformation: {e}\")\n",
    "        create_pcoa = False\n",
    "\n",
    "\n",
    "    return create_pylda, create_pcoa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "# specify the chunk size for LdaModel object\n",
    "# Number of documents to be used in each training chunk\n",
    "CHUNKSIZE = (get_num_records(DATA_SOURCE)//5)\n",
    "def train_model(n_topics: int, alpha_str: list, beta_str: list, data: list, train_eval: str, chunksize=CHUNKSIZE):\n",
    "        models_data = []\n",
    "        coherehce_score_list = []\n",
    "        corpus_batch = []\n",
    "        zipped_texts = []\n",
    "        time_of_method_call = pd.to_datetime('now')\n",
    "\n",
    "        #print(\"this is an investigation into the full datafile\")\n",
    "        #pp.pprint(full_datafile)\n",
    "        # Convert the Delayed object to a Dask Bag and compute it to get the actual data\n",
    "        try:\n",
    "            streaming_documents = dask.compute(*data)\n",
    "            chunksize = int(len(streaming_documents) // 5)\n",
    "            #print(\"these are the streaming documents\")\n",
    "            #print(streaming_documents)\n",
    "            #garbage_collection(False, 'train_model(): streaming_documents = dask.compute(*data)')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error computing streaming_documents data: {e}\")\n",
    "            raise\n",
    "        #print(f\"This is the dtype for 'streaming_documents' {type(streaming_documents)}.\\n\")  # Should output <class 'tuple'>\n",
    "        #print(streaming_documents[0][0])     # Check the first element to see if it's as expected\n",
    "\n",
    "        # Select documents for current batch\n",
    "        batch_documents = streaming_documents\n",
    "        \n",
    "        # Create a new Gensim Dictionary for the current batch\n",
    "        try:\n",
    "            dictionary_batch = Dictionary(list(batch_documents))\n",
    "            #print(\"The dictionary was cretaed.\")\n",
    "        except TypeError:\n",
    "            print(\"Error: The data structure is not correct.\")\n",
    "        #else:\n",
    "        #    print(\"Dictionary created successfully!\")\n",
    "\n",
    "        #if isinstance(batch_documents[0], list) and all(isinstance(doc, list) for doc in batch_documents[0]):\n",
    "        #bow_out = dictionary_batch.doc2bow(batch_documents[0])\n",
    "        flattened_batch = [item for sublist in batch_documents for item in sublist]\n",
    "        #bow_out = dictionary_batch.doc2bow(flattened_batch)\n",
    "        #else:\n",
    "        #    raise ValueError(f\"Expected batch_documents[0] to be a list of token lists. Instead received {type(batch_documents[0])} with value {batch_documents[0]}\\n\")\n",
    "\n",
    "        # Iterate over each document in batch_documents\n",
    "        number_of_documents = 0\n",
    "        for doc_tokens in batch_documents:\n",
    "            # Create the bag-of-words representation for the current document using the dictionary\n",
    "            bow_out = dictionary_batch.doc2bow(doc_tokens)\n",
    "            # Append this representation to the corpus\n",
    "            corpus_batch.append(bow_out)\n",
    "            number_of_documents += 1\n",
    "        logging.info(f\"There was a total of {number_of_documents} documents added to the corpus_batch.\")\n",
    "            \n",
    "        #logger.info(f\"HERE IS THE TEXT for corpus_batch using LOGGER: {corpus_batch}\\n\")\n",
    "        #except Exception as e:\n",
    "        #    logger.error(f\"An unexpected error occurred with BOW_OUT: {e}\")\n",
    "                \n",
    "        #if isinstance(texts_out[0], list):\n",
    "        #    texts_batch.append(texts_out[0])\n",
    "        #else:\n",
    "        #    logging.error(\"Expected texts_out to be a list of strings (words), got:\", texts_out[0])\n",
    "        #    raise ValueError(\"Expected texts_out to be a list of strings (words), got:\", texts_out[0])\n",
    "                \n",
    "        n_alpha = calculate_numeric_alpha(alpha_str)\n",
    "        n_beta = calculate_numeric_beta(beta_str)\n",
    "        try:\n",
    "            #logger.info(\"we are inside the try block at the beginning\")\n",
    "            lda_model_gensim = LdaModel(corpus=corpus_batch,\n",
    "                                                id2word=dictionary_batch,\n",
    "                                                num_topics=n_topics,\n",
    "                                                alpha= float(n_alpha),\n",
    "                                                eta= float(n_beta),\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                passes=PASSES,\n",
    "                                                iterations=ITERATIONS,\n",
    "                                                update_every=UPDATE_EVERY,\n",
    "                                                eval_every=EVAL_EVERY,\n",
    "                                                chunksize=chunksize,\n",
    "                                                per_word_topics=True)\n",
    "            #logger.info(\"we are inside the try block after the constructor\")\n",
    "\n",
    "                                          \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during LDA model training: {e}\")\n",
    "            raise  # Optionally re-raise the exception if you want it to propagate further      \n",
    "\n",
    "        # convert lda model to pickle for storage in output dictionary\n",
    "        ldamodel_bytes = pickle.dumps(lda_model_gensim)\n",
    "\n",
    "        #coherence_score = None  # Assign a default value\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            try:\n",
    "                #coherence_model_lda = CoherenceModel(model=lda_model_gensim, processes=math.floor(CORES*(2/3)), dictionary=dictionary_batch, texts=batch_documents[0], coherence='c_v') \n",
    "                coherence_model_lda = CoherenceModel(model=lda_model_gensim, processes=math.floor(CORES*(1/3)), dictionary=dictionary_batch, texts=batch_documents, coherence='c_v') \n",
    "                coherence_score = coherence_model_lda.get_coherence()\n",
    "                coherehce_score_list.append(coherence_score)\n",
    "            except Exception as e:\n",
    "                logging.error(\"there was an issue calculating coherence score. value '-Inf' has been assigned.\\n\")\n",
    "                coherence_score = float('-inf')\n",
    "                coherehce_score_list.append(coherence_score)\n",
    "                #sys.exit()\n",
    "\n",
    "            try:\n",
    "                convergence_score = lda_model_gensim.bound(corpus_batch)\n",
    "            except Exception as e:\n",
    "                logging.error(\"there was an issue calculating convergence score. value '-Inf' has been assigned.\\n\")\n",
    "                convergence_score = float('-inf')\n",
    "                        \n",
    "            try:\n",
    "                perplexity_score = lda_model_gensim.log_perplexity(corpus_batch)\n",
    "            except RuntimeWarning as e:\n",
    "                logging.info(\"there was an issue calculating perplexity score. value '-Inf' has been assigned.\\n\")\n",
    "                perplexity_score = float('-inf')\n",
    "                #sys.exit()\n",
    "\n",
    "        # Get top topics with their coherence scores\n",
    "        #topics_as_word_lists=[]\n",
    "        topics = lda_model_gensim.top_topics(texts=batch_documents, processes=math.floor(CORES*(1/3)))\n",
    "        # Extract the words as strings from each topic representation\n",
    "        topic_words = []\n",
    "        for topic in topics:\n",
    "            topic_representation = topic[0]\n",
    "            words = [word for _, word in topic_representation]\n",
    "            topic_words.append(words)\n",
    "            \n",
    "            # Append this list of words for current topic to the main list\n",
    "            #topics_as_word_lists.append(topic_words)\n",
    "            \n",
    "        #print(f\"type: {train_eval}, coherence: {coherence_score}, n_topics: {n_topics}, n_alpha: {n_alpha}, alpha_str: {alpha_str}, n_beta: {n_beta}, beta_str: {beta_str}\")\n",
    "        #logging.info(f\"type: {train_eval}, coherence: {coherence_score}, n_topics: {n_topics}, alpha_str: {alpha_str}, beta_str: {beta_str}, batch documents: {batch_documents}\")     \n",
    "\n",
    "        # transform list of tokens comprising the doc into a single string\n",
    "        string_result = ' '.join(map(str, flattened_batch))\n",
    "\n",
    "        # Convert numeric beta value to string if necessary\n",
    "        if isinstance(beta_str, float):\n",
    "            beta_str = str(beta_str)\n",
    "                \n",
    "        # Convert numeric alpha value to string if necessary\n",
    "        if isinstance(alpha_str, float):\n",
    "            alpha_str = str(alpha_str)\n",
    "\n",
    "        #success = False\n",
    "        #vis_bytes = None\n",
    "        #try:\n",
    "        #    success, vis = create_vis(lda_model_gensim, \\\n",
    "        #        f\"{hashlib.md5(time_of_method_call.strftime('%Y%m%d%H%M%S%f').encode()).hexdigest()}\", \\\n",
    "        #        corpus_batch, \\\n",
    "        #        dictionary_batch)\n",
    "            # convert pyLDAvis object to pickle for storage in output dictionary\n",
    "            #vis_bytes = pickle.dumps(vis)\n",
    "        #except Exception as e:\n",
    "        #    logging.error(f\"There was an issue in creating the pyLDAvis: {e}\")\n",
    "        #    logging.error(f\"Type: {train_eval}, coherence: {coherence_score}, n_topics: {n_topics}, alpha_str: {alpha_str}, beta_str: {beta_str}\")     \n",
    "\n",
    "        current_increment_data = {\n",
    "                'type': train_eval,\n",
    "                'num_workers': 0, # this value is set to 0 which will signify an error in assignment of adaptive-scaling worker count assigned in process_completed()\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'text': [string_result],\n",
    "                'text_sha256': hashlib.sha256(string_result.encode()).hexdigest(),\n",
    "                'text_md5': hashlib.md5(string_result.encode()).hexdigest(),\n",
    "                'corpus': pickle.dumps(corpus_batch),\n",
    "                'dictionary': pickle.dumps(dictionary_batch),\n",
    "                'convergence': convergence_score,\n",
    "                'perplexity': perplexity_score,\n",
    "                'coherence': coherence_score,\n",
    "                'topics': n_topics,\n",
    "                'alpha_str': [alpha_str],\n",
    "                'n_alpha': calculate_numeric_alpha(alpha_str),\n",
    "                'beta_str': [beta_str],\n",
    "                'n_beta': calculate_numeric_beta(beta_str),\n",
    "                'passes': PASSES,\n",
    "                'iterations': ITERATIONS,\n",
    "                'update_every': UPDATE_EVERY,\n",
    "                'eval_every': EVAL_EVERY,\n",
    "                'chunksize': chunksize,\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'per_word_topics': PER_WORD_TOPICS,\n",
    "                'top_words': [topic_words],\n",
    "                'lda_model': ldamodel_bytes,\n",
    "                #'create_pylda': False, \n",
    "                #'create_pcoa': False, \n",
    "                'time': time_of_method_call\n",
    "        }\n",
    "\n",
    "        models_data.append(current_increment_data)\n",
    "        #garbage_collection(False, 'train_model(): convergence and perplexity score calculations')\n",
    "        del batch_documents, streaming_documents, lda_model_gensim, dictionary_batch, current_increment_data #, vis, success\n",
    "\n",
    "        return models_data\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a delayed version of the train_model function\n",
    "@dask.delayed\n",
    "def delayed_train_model(n_topics, alpha_value, beta_value, scattered_data, train_eval_type):\n",
    "    # Call the train_model function here\n",
    "    train_model(n_topics, alpha_value, beta_value, scattered_data, train_eval_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                    - The `process_completed_future` function is called when all futures in a batch complete within the specified timeout. It \n",
    "                        can be used to continue with your program using both completed training and evaluation futures.\n",
    "                    - The `retry_processing` function is called when there are incomplete futures after iterating through a batch of \n",
    "                        data. It can be used to retry processing with those incomplete futures.\n",
    "                    - The code checks if there are any remaining futures in the lists after completing all iterations. If so, it \n",
    "                        waits for them to complete and handles them accordingly.\n",
    "\"\"\"\n",
    "\n",
    "# List to store parameters of models that failed to complete even after a retry\n",
    "failed_model_params = []\n",
    "\n",
    "# Mapping from futures to their corresponding parameters (n_topics, alpha_value, beta_value)\n",
    "future_to_params = {}\n",
    "def process_completed_futures(completed_train_futures, completed_eval_futures, workers, batchsize, log_dir):\n",
    "    #print(\"we are in the process_completed_futures method()\")\n",
    "    # Process training futures\n",
    "    #vis_futures = []\n",
    "    for future in completed_train_futures:\n",
    "        try:\n",
    "            # Retrieve the result of the training future\n",
    "            #if isinstance(future.result(), list):\n",
    "            models_data = future.result()  # This should be a list of dictionaries\n",
    "            if not isinstance(models_data, list):\n",
    "                models_data = list(future.result())  # This should be a list of dictionaries\n",
    "            #logging.info(f\"this is the value of the TRAIN MODELS_DATA within the process_completed method: {models_data}\")\n",
    "            #else:\n",
    "            #    models_data = list(future.result())\n",
    "            #print(\"this is the value of models data:\", models_data)\n",
    "            \n",
    "        except TypeError as e:\n",
    "            logging.error(f\"Error occurred during training: {e}\")\n",
    "            #sys.exit()\n",
    "        else:\n",
    "            # Iterate over each model's data and save it\n",
    "            #for model_data in models_data:\n",
    "                # Check if models_data is a non-empty list before iterating\n",
    "                if isinstance(models_data, list) and models_data:\n",
    "                    for model_data in models_data:\n",
    "                        #logging.info(f\"this is the value of model TRAIN data: {model_data}\")\n",
    "                        #save_model_and_log(model_data=model_data, log_dir=log_dir, train_or_eval=True)\n",
    "                        #pylda_success, pcoa_success = create_vis(pickle.loads(model_data['lda_model']), \\\n",
    "                        #           hashlib.md5(model_data['time'].strftime('%Y%m%d%H%M%S%f').encode()).hexdigest(), \n",
    "                        #           pickle.loads(model_data['corpus']), \n",
    "                        #           pickle.loads(model_data['dictionary']))\n",
    "                        #future = client.submit(create_vis, pickle.loads(model_data['lda_model']), model_data['text_md5'], model_data['corpus_batch'], model_data['dictionary_batch'])\n",
    "                        #vis_futures.append(future)\n",
    "                        #add_model_data_to_metadata(model_data, pylda_success, pcoa_success, batchsize)\n",
    "                        add_model_data_to_metadata(model_data, workers, batchsize)\n",
    "                    # Gather all results (this will trigger computation).\n",
    "                    #vis_results = client.gather(vis_futures)\n",
    "\n",
    "                    # If there are any delayed objects within results (like from create_vis),\n",
    "                    # compute them here. This will block until all visualizations are created.\n",
    "                    #dask.compute(*vis_results)\n",
    "                    #vis_futures.clear()\n",
    "                    #vis_results.clear()\n",
    "                else:\n",
    "                    # Handle the case where models_data is not as expected\n",
    "                    logging.error(f\"Received unexpected result from TRAIN future: {models_data}\")\n",
    "\n",
    "    # Process evaluation futures\n",
    "    #vis_futures = []\n",
    "    for future in completed_eval_futures:\n",
    "        try:\n",
    "            # Retrieve the result of the training future\n",
    "            #if isinstance(future.result(), list):\n",
    "            models_data = future.result()  # This should be a list of dictionaries\n",
    "            if not isinstance(models_data, list):\n",
    "                models_data = list(future.result())  # This should be a list of dictionaries\n",
    "            #logging.info(f\"this is the value of the EVAL MODELS_DATA within the process_completed method: {models_data}\")\n",
    "            #else:\n",
    "            #    models_data = list(future.result())\n",
    "            #print(\"this is the value of models data:\", models_data)\n",
    "        except TypeError as e:\n",
    "            logging.error(f\"Error occurred during evaluation: {e}\")\n",
    "            sys.exit()\n",
    "        else:\n",
    "            # Iterate over each model's data and save it\n",
    "            #for model_data in models_data:\n",
    "                # Check if models_data is a non-empty list before iterating\n",
    "                if isinstance(models_data, list) and models_data:\n",
    "                    for model_data in models_data:\n",
    "                        #logging.info(f\"this is the value of model EVAL data: {model_data}\")\n",
    "                        #save_model_and_log(model_data=model_data, log_dir=log_dir, train_or_eval=False)\n",
    "                        #pylda_success, pcoa_success = create_vis(pickle.loads(model_data['lda_model']), \\\n",
    "                        #           hashlib.md5(model_data['time'].strftime('%Y%m%d%H%M%S%f').encode()).hexdigest(), \n",
    "                        #           pickle.loads(model_data['corpus']), \n",
    "                        #           pickle.loads(model_data['dictionary']))\n",
    "                        #future = client.submit(create_vis, pickle.loads(model_data['lda_model']), model_data['text_md5'], model_data['corpus_batch'], model_data['dictionary_batch'])\n",
    "                        #vis_futures.append(future)\n",
    "                        add_model_data_to_metadata(model_data, workers, batchsize)\n",
    "                    # Gather all results (this will trigger computation).\n",
    "                    #vis_results = client.gather(vis_futures)\n",
    "\n",
    "                    # If there are any delayed objects within results (like from create_vis),\n",
    "                    # compute them here. This will block until all visualizations are created.\n",
    "                    #dask.compute(*vis_results)\n",
    "                    #vis_futures.clear()\n",
    "                    #vis_results.clear()\n",
    "                else:\n",
    "                    # Handle the case where models_data is not as expected\n",
    "                    logging.error(f\"Received unexpected result from EVAL future: {models_data}\")\n",
    "                    \n",
    "    del models_data            \n",
    "    #garbage_collection(True, 'process_completed_futures(...)')\n",
    "\n",
    "\n",
    "# Function to retry processing with incomplete futures\n",
    "def retry_processing(incomplete_train_futures, incomplete_eval_futures, timeout=None):\n",
    "    #print(\"we are in the retry_processing method()\")\n",
    "    # Retry processing with incomplete futures using an extended timeout\n",
    "    # Process completed ones after reattempting\n",
    "    #done_train = [f for f in done if f in train_futures]\n",
    "    #done_eval = [f for f in done if f in eval_futures]\n",
    "    # Wait for completion of eval_futures\n",
    "    done_eval, not_done_eval = wait(incomplete_eval_futures, timeout=timeout)  # return_when='FIRST_COMPLETED'\n",
    "    #print(f\"This is the size of the done_eval list: {len(done_eval)} and this is the size of the not_done_eval list: {len(not_done_eval)}\")\n",
    "\n",
    "    # Wait for completion of train_futures\n",
    "    done_train, not_done_train = wait(incomplete_train_futures, timeout=timeout)  # return_when='FIRST_COMPLETED'\n",
    "    #print(f\"This is the size of the done_train list: {len(done_train)} and this is the size of the not_done_train list: {len(not_done_train)}\")\n",
    "\n",
    "    done = done_train.union(done_eval)\n",
    "    not_done = not_done_eval.union(not_done_train)\n",
    "                \n",
    "    #print(f\"WAIT completed in {elapsed_time} minutes\")\n",
    "    #print(f\"This is the size of DONE {len(done)}. And this is the size of NOT_DONE {len(not_done)}\\n\")\n",
    "    #print(f\"this is the value of done_train {done_train}\")\n",
    "\n",
    "    completed_train_futures = [f for f in done_train]\n",
    "    #print(f\"We have completed the TRAIN list comprehension. The size is {len(completed_train_futures)}\")\n",
    "    #print(f\"This is the length of the TRAIN completed_train_futures var {len(completed_train_futures)}\")\n",
    "            \n",
    "    completed_eval_futures = [f for f in done_eval]\n",
    "    #print(f\"We have completed the EVAL list comprehension. The size is {len(completed_eval_futures)}\")\n",
    "    #print(f\"This is the length of the EVAL completed_eval_futures var {len(completed_eval_futures)}\")\n",
    "\n",
    "    #logging.info(f\"This is the size of completed_train_futures {len(completed_train_futures)} and this is the size of completed_eval_futures {len(completed_eval_futures)}\")\n",
    "    if len(completed_eval_futures) > 0 or len(completed_train_futures) > 0:\n",
    "        process_completed_futures(completed_train_futures, completed_eval_futures, LOG_DIR) \n",
    "    \n",
    "    # Record parameters of still incomplete futures for later review\n",
    "    failed_model_params.extend(future_to_params[future] for future in not_done)\n",
    "    print(\"We have exited the retry_preprocessing() method.\")\n",
    "    logging.info(f\"There were {len(not_done_eval)} EVAL documents that couldn't be processed in retry_processing().\")\n",
    "    logging.info(f\"There were {len(not_done_train)} TRAIN documents that couldn't be processed in retry_processing().\")\n",
    "\n",
    "    #garbage_collection(False, 'retry_processing(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to keep track of retries for each task\n",
    "task_retries = {}\n",
    "\n",
    "# Function to perform exponential backoff\n",
    "def exponential_backoff(attempt):\n",
    "    return BASE_WAIT_TIME * (2 ** attempt)\n",
    "\n",
    "# Function to handle failed futures and potentially retry them\n",
    "def handle_failed_future(future, future_to_params, train_futures, eval_futures, client):\n",
    "    logging.info(\"We are in the handle_failed_future() method.\\n\")\n",
    "    params = future_to_params[future]\n",
    "    attempt = task_retries.get(params, 0)\n",
    "    \n",
    "    if attempt < MAX_RETRIES:\n",
    "        logging.info(f\"Retrying task {params} (attempt {attempt + 1}/{MAX_RETRIES})\")\n",
    "        wait_time = exponential_backoff(attempt)\n",
    "        sleep(wait_time)  \n",
    "        \n",
    "        task_retries[params] = attempt + 1\n",
    "        \n",
    "        new_future_train = client.submit(train_model, *params)\n",
    "        new_future_eval = client.submit(train_model, *params)\n",
    "        \n",
    "        future_to_params[new_future_train] = params\n",
    "        future_to_params[new_future_eval] = params\n",
    "        \n",
    "        train_futures.append(new_future_train)\n",
    "        eval_futures.append(new_future_eval)\n",
    "    else:\n",
    "        logging.info(f\"Task {params} failed after {MAX_RETRIES} attempts. No more retries.\")\n",
    "\n",
    "    #garbage_collection(False,'handle_failed_future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os\n",
    "from tqdm import tqdm\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=CORES,\n",
    "            threads_per_worker=THREADS_PER_CORE,\n",
    "            processes=False,\n",
    "            memory_limit=RAM_MEMORY_LIMIT,\n",
    "            local_directory=DASK_DIR,\n",
    "            #dashboard_address=None,\n",
    "            dashboard_address=\":8787\",\n",
    "            protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    client.cluster.adapt(minimum=CORES, maximum=MAXIMUM_CORES)\n",
    "    \n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = RAM_MEMORY_LIMIT\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "        print(\"The system is shutting down.\")\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        sys.exit()\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(f\"{CORES} Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "        print(\"The system is shutting down.\")\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        sys.exit()\n",
    "\n",
    "    print(\"Creating training and evaluation samples...\")\n",
    "    \n",
    "    started = time()\n",
    "    \n",
    "    scattered_train_data_futures = []\n",
    "    scattered_eval_data_futures = []\n",
    "\n",
    "    total_num_samples = get_num_records(DATA_SOURCE)\n",
    "\n",
    "    whole_train_dataset = None\n",
    "    whole_eval_dataset = None\n",
    "\n",
    "    with tqdm(total=total_num_samples) as pbar:\n",
    "        # Process each batch as it is generated\n",
    "        for batch_info in futures_create_lda_datasets(DATA_SOURCE, TRAIN_RATIO):\n",
    "            if batch_info['type'] == 'train':\n",
    "                # Handle training data\n",
    "                #print(\"We are inside the IF/ELSE block for producing TRAIN scatter.\")\n",
    "                try:\n",
    "                    scattered_future = client.scatter(batch_info['data'])\n",
    "                    scattered_train_data_futures.append(scattered_future)\n",
    "                except Exception as e:\n",
    "                    print(\"there was an issue with creating the TRAIN scattered_future list\")\n",
    "                \n",
    "                if whole_train_dataset is None:\n",
    "                    whole_train_dataset = batch_info['whole_dataset']\n",
    "            elif batch_info['type'] == 'eval':\n",
    "                # Handle evaluation data\n",
    "                #print(\"We are inside the IF/ELSE block for producing EVAL scatter.\")\n",
    "                try:\n",
    "                    scattered_future = client.scatter(batch_info['data'])\n",
    "                    scattered_eval_data_futures.append(scattered_future)\n",
    "                except Exception as e:\n",
    "                    print(\"there was an issue with creating the EVAL scattererd_future list.\")\n",
    "                    print(e)\n",
    "                    \n",
    "                \n",
    "                if whole_eval_dataset is None:\n",
    "                    whole_eval_dataset = batch_info['whole_dataset']\n",
    "\n",
    "            # Update the progress bar with the cumulative count of samples processed\n",
    "            #pbar.update(batch_info['cumulative_count'] - pbar.n)\n",
    "            pbar.update(len(batch_info['data']))\n",
    "\n",
    "        pbar.close()  # Ensure closure of the progress bar\n",
    "\n",
    "    print(f\"Completed creation of training and evaluation documents in {round((time() - started)/60,2)} minutes.\\n\")\n",
    "   \n",
    "    print(\"Data scatter complete...\\n\")\n",
    "    #garbage_collection(False, 'scattering training and eval data')\n",
    "    #del scattered_future\n",
    "    #del whole_train_dataset, whole_eval_dataset # these variables are not used at all\n",
    "\n",
    "    train_futures = []  # List to store futures for training\n",
    "    eval_futures = []  # List to store futures for evaluation\n",
    "   \n",
    "    num_topics = len(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE))\n",
    "    num_alpha_values = len(alpha_values)\n",
    "    num_beta_values = len(beta_values)\n",
    "\n",
    "    TOTAL_MODELS = (num_topics * num_alpha_values * num_beta_values) * 2\n",
    "\n",
    "    #progress_bar = tqdm(total=TOTAL_MODELS, desc=\"Creating and saving models\")\n",
    "\n",
    "    train_eval = ['eval', 'train']\n",
    "\n",
    "    # Create a list of all combinations of n_topics, alpha_value, beta_value, and train_eval\n",
    "    combinations = list(itertools.product(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE), alpha_values, beta_values, train_eval))\n",
    "\n",
    "    # Separate the combinations into two lists based on 'train' and 'eval'\n",
    "    train_combinations = [combo for combo in combinations if combo[-1] == 'train']\n",
    "    eval_combinations = [combo for combo in combinations if combo[-1] == 'eval']\n",
    "\n",
    "    # Calculate the sample size for each category\n",
    "    sample_size = min(len(train_combinations), len(eval_combinations))\n",
    "\n",
    "    # Select random combinations from each category\n",
    "    random_train_combinations = random.sample(train_combinations, sample_size)\n",
    "    random_eval_combinations = random.sample(eval_combinations, sample_size)\n",
    "\n",
    "    # Combine the randomly selected train and eval combinations\n",
    "    random_combinations = random_eval_combinations+ random_train_combinations\n",
    "    sample_size = max(1, int(len(combinations) * 0.375))\n",
    "\n",
    "    # Select random_combinations conditionally\n",
    "    random_combinations = random.sample(combinations, sample_size) if sample_size < len(combinations) else combinations\n",
    "    #progress_bar = tqdm(total=len(random_combinations), desc=\"Creating and saving models\")\n",
    "    progress_bar = tqdm(desc=\"Creating and saving models\")\n",
    "    print(f\"The random sample combinations contains {len(random_combinations)}\")\n",
    "\n",
    "    # Determine which combinations were not drawn by using set difference\n",
    "    undrawn_combinations = list(set(combinations) - set(random_combinations))\n",
    "\n",
    "    print(f\"this leaves {len(undrawn_combinations)} undrawn combinations\\n\")\n",
    "\n",
    "    # number of futures that complete in WAIT method\n",
    "    #completed_tasks = 0\n",
    "    # Create empty lists to store all future objects for training and evaluation\n",
    "    train_futures = []\n",
    "    eval_futures = []\n",
    "    \n",
    "    # Iterate over the combinations and submit tasks\n",
    "    for n_topics, alpha_value, beta_value, train_eval_type in random_combinations:\n",
    "\n",
    "        # determine if throttling is needed\n",
    "        logging.info(\"\\nEvaluating if adaptive throttling is necessary (method exponential backoff)...\")\n",
    "        started, throttle_attempt = time(), 0\n",
    "\n",
    "        # https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os\n",
    "        while throttle_attempt < MAX_RETRIES:\n",
    "            scheduler_info = client.scheduler_info()\n",
    "            all_workers_below_cpu_threshold = all(worker['metrics']['cpu'] < CPU_UTILIZATION_THRESHOLD for worker in scheduler_info['workers'].values())\n",
    "            all_workers_below_memory_threshold = all(worker['metrics']['memory'] < MEMORY_UTILIZATION_THRESHOLD for worker in scheduler_info['workers'].values())\n",
    "\n",
    "            if not (all_workers_below_cpu_threshold and all_workers_below_memory_threshold):\n",
    "                logging.info(f\"Adaptive throttling (attempt {throttle_attempt} of {MAX_RETRIES-1})\")\n",
    "                # Uncomment the next line if you want to log hyperparameters information as well.\n",
    "                #logging.info(f\"for LdaModel hyperparameters combination -- type: {train_eval_type}, topic: {n_topics}, ALPHA: {alpha_value} and ETA {beta_value}\")\n",
    "                sleep(exponential_backoff(throttle_attempt))\n",
    "                throttle_attempt += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if throttle_attempt == MAX_RETRIES:\n",
    "            logging.error(\"Maximum retries reached. The workers are still above the CPU or Memory threshold.\")\n",
    "            garbage_collection(True, 'Max Retries - throttling attempt')\n",
    "        else:\n",
    "            logging.info(\"Proceeding with workload as workers are below the CPU and Memory thresholds.\")\n",
    "\n",
    "        #logging.info(f\"for LdaModel hyperparameters combination -- type: {train_eval_type}, topic: {n_topics}, ALPHA: {alpha_value} and ETA {beta_value}\")\n",
    "        # Submit a future for each scattered data object in the training list\n",
    "        #if train_eval_type == 'train':\n",
    "        # Submit a future for each scattered data object in the training list\n",
    "        for scattered_data in scattered_train_data_futures:\n",
    "            future = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_data, 'train')\n",
    "            train_futures.append(future)\n",
    "            logging.info(f\"The training value is being appended to the train_futures list. Size: {len(train_futures)}\")\n",
    "\n",
    "        # Submit a future for each scattered data object in the evaluation list\n",
    "        #if train_eval_type == 'eval':\n",
    "        for scattered_data in scattered_eval_data_futures:\n",
    "            future = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_data, 'eval')\n",
    "            eval_futures.append(future)\n",
    "            logging.info(f\"The evaluation value is being appended to the eval_futures list. Size: {len(eval_futures)}\")\n",
    "        #garbage_collection(False, 'client.submit(train_model(...) train and eval)')\n",
    "\n",
    "\n",
    "        # Map the created futures to their parameters so we can identify them later if needed\n",
    "        for future in train_futures:\n",
    "            future_to_params[future] = ('train',n_topics, alpha_value, beta_value)\n",
    "\n",
    "        # Do the same for eval_futures\n",
    "        for future in eval_futures:\n",
    "            future_to_params[future] = ('eval', n_topics, alpha_value, beta_value)\n",
    "\n",
    "        #train_futures.append(all_train_futures)\n",
    "        #eval_futures.append(all_eval_futures)\n",
    "        #print(f\"This is the size of the eval_futures {len(eval_futures)}\")\n",
    "        #print(f\"this is the eval futures: {eval_futures}\\n\\n\")\n",
    "            \n",
    "        # Check if it's time to process futures based on BATCH_SIZE\n",
    "        train_eval_count = len(train_futures) + len(eval_futures)\n",
    "        if train_eval_count >= BATCH_SIZE:\n",
    "            time_of_vis_call = pd.to_datetime('now')\n",
    "            time_of_vis_call = time_of_vis_call.strftime('%Y%m%d%H%M%S%f')\n",
    "            PERFORMANCE_TRAIN_LOG = os.path.join(LOG_DIR, f\"train_perf_{time_of_vis_call}.html\")\n",
    "            del time_of_vis_call\n",
    "            with performance_report(filename=PERFORMANCE_TRAIN_LOG):\n",
    "                logging.info(\"In holding pattern until WAIT completes.\")\n",
    "                started = time()\n",
    "                    \n",
    "                #done, not_done = wait(train_futures + eval_futures, timeout=None)        # Wait for all reattempted futures with an extended timeout (e.g., 120 seconds)\n",
    "\n",
    "                # Process completed ones after reattempting\n",
    "                #done_train = [f for f in done if f in train_futures]\n",
    "                #done_eval = [f for f in done if f in eval_futures]\n",
    "                # Wait for completion of eval_futures\n",
    "                done_eval, not_done_eval = wait(eval_futures, timeout=None)  # return_when='FIRST_COMPLETED'\n",
    "                logging.info(f\"This is the size of the done_eval list: {len(done_eval)} and this is the size of the not_done_eval list: {len(not_done_eval)}\")\n",
    "\n",
    "                # Wait for completion of train_futures\n",
    "                done_train, not_done_train = wait(train_futures, timeout=None)  # return_when='FIRST_COMPLETED'\n",
    "                logging.info(f\"This is the size of the done_train list: {len(done_train)} and this is the size of the not_done_train list: {len(not_done_train)}\")\n",
    "\n",
    "                done = done_train.union(done_eval)\n",
    "                not_done = not_done_eval.union(not_done_train)\n",
    "                    \n",
    "                elapsed_time = round(((time() - started) / 60), 2)\n",
    "                logging.info(f\"WAIT completed in {elapsed_time} minutes\")\n",
    "                print(f\"This is the size of DONE {len(done)}. And this is the size of NOT_DONE {len(not_done)}\\n\")\n",
    "                #print(f\"this is the value of done_train {done_train}\")\n",
    "\n",
    "                # Now clear references to these completed futures by filtering them out of your lists\n",
    "                train_futures = [f for f in train_futures if f not in done_train]\n",
    "                eval_futures = [f for f in eval_futures if f not in done_eval]\n",
    "                \n",
    "                completed_train_futures = [f for f in done_train]\n",
    "                #print(f\"We have completed the TRAIN list comprehension. The size is {len(completed_train_futures)}\")\n",
    "                #print(f\"This is the length of the TRAIN completed_train_futures var {len(completed_train_futures)}\")\n",
    "                \n",
    "                completed_eval_futures = [f for f in done_eval]\n",
    "                #print(f\"We have completed the EVAL list comprehension. The size is {len(completed_eval_futures)}\")\n",
    "                #print(f\"This is the length of the EVAL completed_eval_futures var {len(completed_eval_futures)}\")\n",
    "\n",
    "            num_workers = len(client.scheduler_info()[\"workers\"])\n",
    "            process_completed_futures(completed_train_futures, completed_eval_futures, num_workers, BATCH_SIZE, LOG_DIR)\n",
    "            #for f in completed_train_futures:\n",
    "            #    client.cancel(f)\n",
    "            #for f in completed_eval_futures:\n",
    "            #    client.cancel(f)\n",
    "            #del completed_eval_futures, completed_train_futures\n",
    "                \n",
    "            ########################\n",
    "            # PROCESS VISUALIZATIONS\n",
    "            ########################\n",
    "            time_of_vis_call = pd.to_datetime('now')\n",
    "            time_of_vis_call = time_of_vis_call.strftime('%Y%m%d%H%M%S%f')\n",
    "            PERFORMANCE_TRAIN_LOG = os.path.join(IMAGE_DIR, f\"vis_perf_{time_of_vis_call}.html\")\n",
    "            del time_of_vis_call\n",
    "            with performance_report(filename=PERFORMANCE_TRAIN_LOG):\n",
    "                logging.info(\"\\nIn holding pattern until process TRAIN and EVAL visualizations completes.\")\n",
    "                started = time()\n",
    "                # To get the results from the completed futures\n",
    "                logging.info(\"Gathering DONE_TRAIN futures.\")\n",
    "                results = [d.result() for d in done_train if  isinstance(d, Future)]       \n",
    "                logging.info(\"Completed gathering DONE_TRAIN futures.\") \n",
    "                if len(results) != len(done_train):\n",
    "                    logging.error(\"All DONE TRAIN futures could not be resolved.\")\n",
    "\n",
    "                # Now you can process these results and submit new tasks based on them\n",
    "                create_visualizations = []\n",
    "                for r in results:\n",
    "                    for result in r:\n",
    "                        # Process your result here and define a new task based on it\n",
    "                        new_task = client.submit(create_vis, pickle.loads(result['lda_model']), \\\n",
    "                                                    hashlib.md5(result['time'].strftime('%Y%m%d%H%M%S%f').encode()).hexdigest(), \\\n",
    "                                                    pickle.loads(result['corpus']), \\\n",
    "                                                    pickle.loads(result['dictionary'])   )\n",
    "                        create_visualizations.append(new_task)\n",
    "\n",
    "                logging.info(\"Executing WAIT on TRAIN create_visualizations futures.\")\n",
    "                done_new_tasks, not_done_new_tasks = wait(create_visualizations)\n",
    "                if len(not_done_new_tasks) > 0:\n",
    "                    logging.error(f\"All TRAIN visualizations couldn't be generated. There were {len(not_done_new_tasks)} not created.\")\n",
    "\n",
    "                # Gather the results from the completed visualization tasks\n",
    "                logging.info(\"Gathering completed TRAIN visualization results futures.\")\n",
    "                completed_visualization_results = client.gather(done_new_tasks)\n",
    "                #del completed_visualization_results\n",
    "                logging.info(\"Completed gathering TRAIN visualization results futures.\")\n",
    "\n",
    "                #defensive programming to ensure WAIT output list of futures are empty\n",
    "                for f in done_train:\n",
    "                    client.cancel(f)\n",
    "                for f in create_visualizations:\n",
    "                    client.cancel(f)\n",
    "                for f in completed_visualization_results:\n",
    "                    client.cancel(f)\n",
    "\n",
    "\n",
    "                # create visualizations for evaluation data\n",
    "                logging.info(\"Gathering DONE_EVAL futures.\")\n",
    "                results = [d.result() for d in done_eval if isinstance(d, Future)]           \n",
    "                logging.info(\"Complted gathering DONE_EVAL futures.\")  \n",
    "                if len(results) != len(done_eval):\n",
    "                    logging.error(\"All DONE EVAL futures could not be resolved.\")\n",
    "\n",
    "                # Now you can process these results and submit new tasks based on them\n",
    "                create_visualizations = []\n",
    "                for r in results:\n",
    "                    for result in r:\n",
    "                        # Process your result here and define a new task based on it\n",
    "                        new_task = client.submit(create_vis, pickle.loads(result['lda_model']), \\\n",
    "                                                    hashlib.md5(result['time'].strftime('%Y%m%d%H%M%S%f').encode()).hexdigest(), \\\n",
    "                                                    pickle.loads(result['corpus']), \\\n",
    "                                                    pickle.loads(result['dictionary'])   )\n",
    "                        create_visualizations.append(new_task)\n",
    "\n",
    "                logging.info(\"Executing WAIT on EVAL create_visualizations futures.\")\n",
    "                done_new_tasks, not_done_new_tasks = wait(create_visualizations)\n",
    "                if len(not_done_new_tasks) > 0:\n",
    "                    logging.error(f\"All EVAL visualizations couldn't be generated. There were {len(not_done_new_tasks)} not created.\")\n",
    "\n",
    "                # Gather the results from the completed visualization tasks\n",
    "                logging.info(\"Gathering completed EVAL visualization results futures.\")\n",
    "                completed_visualization_results = client.gather(done_new_tasks)\n",
    "                #del completed_visualization_results\n",
    "                logging.info(\"Completed gathering EVAL visualization results futures.\")\n",
    "\n",
    "                #defensive programming to ensure WAIT output list of futures are empty\n",
    "                for f in done_eval:\n",
    "                    client.cancel(f)\n",
    "                for f in done_new_tasks:\n",
    "                    client.cancel(f)\n",
    "                for f in results:\n",
    "                    client.cancel(f)\n",
    "                for f in completed_visualization_results:\n",
    "                    client.cancel(f)             \n",
    "\n",
    "                elapsed_time = round(((time() - started) / 60), 2)\n",
    "                logging.info(f\"Create visualizations for TRAIN and EVAL data completed in {elapsed_time} minutes\")\n",
    "            # close performance report encapsulation of visualization performance analysis\n",
    "\n",
    "            #############################\n",
    "            # END PROCESS VISUALIZATIONS\n",
    "            #############################\n",
    "                \n",
    "            #logging.info(f\"This is the size of completed_train_futures {len(completed_train_futures)} and this is the size of completed_eval_futures {len(completed_eval_futures)}\")\n",
    "            progress_bar.update(len(done))\n",
    "\n",
    "            # Handle failed futures using the previously defined function\n",
    "            for future in not_done:\n",
    "                failed_future_timer = time()\n",
    "                logging.error(\"Handling of failed WAIT method has been initiated.\")\n",
    "                handle_failed_future(future, future_to_params, train_futures,  eval_futures, client)\n",
    "                elapsed_time = round(((time() - started) / 60), 2)\n",
    "                logging.error(f\"It took {elapsed_time} minutes to handle {len(train_futures)} train futures and {len(eval_futures)} evaluation futures the failed future.\")\n",
    "\n",
    "\n",
    "            # If no tasks are pending (i.e., all have been processed), consider increasing BATCH_SIZE.\n",
    "            #completed_tasks += len(done_train) + len(done_eval)\n",
    "\n",
    "            # monitor system resource usage and adjust batch size accordingly\n",
    "            scheduler_info = client.scheduler_info()\n",
    "            all_workers_below_cpu_threshold = all(worker['metrics']['cpu'] < CPU_UTILIZATION_THRESHOLD for worker in scheduler_info['workers'].values())\n",
    "            all_workers_below_memory_threshold = all(worker['metrics']['memory'] < MEMORY_UTILIZATION_THRESHOLD for worker in scheduler_info['workers'].values())\n",
    "\n",
    "            if (all_workers_below_cpu_threshold and all_workers_below_memory_threshold):\n",
    "                BATCH_SIZE = int(math.ceil(BATCH_SIZE * INCREASE_FACTOR)) if int(math.ceil(BATCH_SIZE * INCREASE_FACTOR)) < MAX_BATCH_SIZE else MAX_BATCH_SIZE\n",
    "                logging.info(f\"Increasing batch size to {BATCH_SIZE}\")\n",
    "            else:\n",
    "                BATCH_SIZE = max(1, int(BATCH_SIZE * (1-DECREASE_FACTOR))) if max(1, int(BATCH_SIZE * (1-DECREASE_FACTOR))) < BATCH_SIZE else BATCH_SIZE\n",
    "                logging.info(f\"Decreasing batch size to {BATCH_SIZE}\")\n",
    "                garbage_collection(True, 'Batch Size Decrease')\n",
    "\n",
    "            #defensive programming to ensure WAIT output list of futures are empty\n",
    "            for f in done:\n",
    "                client.cancel(f)\n",
    "            for f in completed_train_futures:\n",
    "                client.cancel(f)\n",
    "            for f in completed_eval_futures:\n",
    "                client.cancel(f)\n",
    "            \n",
    "            del done, not_done, done_train, done_eval, not_done_eval, not_done_train\n",
    "            del create_visualizations, done_new_tasks, not_done_new_tasks, new_task, results, completed_visualization_results\n",
    "            client.rebalance() \n",
    "            garbage_collection(True,'End of a batch being processed.')\n",
    "         \n",
    "    #garbage_collection(False, \"Cleaning WAIT -> done, not_done\")     \n",
    "    progress_bar.close()\n",
    "\n",
    "    # After all loops have finished running...\n",
    "    if len(train_futures) > 0 or len(eval_futures) > 0:\n",
    "        print(\"we are in the first IF statement for retry_processing()\")\n",
    "        retry_processing(train_futures, eval_futures, TIMEOUT)\n",
    "    del train_futures, eval_futures\n",
    "\n",
    "    # Now give one more chance with extended timeout only to those that were incomplete previously\n",
    "    if len(failed_model_params) > 0:\n",
    "        print(\"Retrying incomplete models with extended timeout...\")\n",
    "        \n",
    "        # Create new lists for retrying futures\n",
    "        retry_train_futures = []\n",
    "        retry_eval_futures = []\n",
    "\n",
    "        # Resubmit tasks only for those that failed in the first attempt\n",
    "        for params in failed_model_params:\n",
    "            n_topics, alpha_value, beta_value = params\n",
    "            \n",
    "            #with performance_report(filename=PERFORMANCE_TRAIN_LOG):\n",
    "            future_train_retry = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_train_data_futures, 'train')\n",
    "            future_eval_retry = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_eval_data_futures, 'eval')\n",
    "\n",
    "            retry_train_futures.append(future_train_retry)\n",
    "            retry_eval_futures.append(future_eval_retry)\n",
    "\n",
    "            # Keep track of these new futures as well\n",
    "            future_to_params[future_train_retry] = params\n",
    "            future_to_params[future_eval_retry] = params\n",
    "\n",
    "        # Clear the list of failed model parameters before reattempting\n",
    "        failed_model_params.clear()\n",
    "\n",
    "        # Wait for all reattempted futures with an extended timeout (e.g., 120 seconds)\n",
    "        done, not_done = wait(retry_train_futures + retry_eval_futures, timeout=None) #, timeout=EXTENDED_TIMEOUT)\n",
    "\n",
    "        # Process completed ones after reattempting\n",
    "        process_completed_futures([f for f in done if f in retry_train_futures],\n",
    "                                [f for f in done if f in retry_eval_futures],\n",
    "                                LOG_DIR)\n",
    "        \n",
    "        #progress_bar.update(len(done))\n",
    "\n",
    "        # Record parameters of still incomplete futures after reattempting for later review\n",
    "        for future in not_done:\n",
    "            failed_model_params.append(future_to_params[future])\n",
    "\n",
    "        # At this point `failed_model_params` contains the parameters of all models that didn't complete even after a retry\n",
    "\n",
    "    #client.close()\n",
    "    print(\"The training and evaluation loop has completed.\")\n",
    "    logging.info(\"The training and evaluation loop has completed.\")\n",
    "\n",
    "    if len(failed_model_params) > 0:\n",
    "        # You can now review `failed_model_params` to see which models did not complete successfully.\n",
    "        logging.error(\"The following model parameters did not complete even after a second attempt:\")\n",
    "    #    perf_logger.info(\"The following model parameters did not complete even after a second attempt:\")\n",
    "        for params in failed_model_params:\n",
    "            logging.error(params)\n",
    "    #        perf_logger.info(params)\n",
    "            \n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pa\n",
    "\n",
    "# Uncomment the next two lines if you want to view the file's schema.\n",
    "# parquetFile = pa.ParquetFile('test.parquet')\n",
    "# print(parquetFile.schema)\n",
    "\n",
    "df = pd.read_parquet(r'C:\\_harvester\\data\\lda-models\\2010s_html\\metadata\\metadata.parquet')\n",
    "df.to_csv(r'C:\\_harvester\\data\\lda-models\\2010s_html\\metadata\\metadata-09232024.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
