{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim  # Library for interactive topic model visualization\n",
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel  # Computes coherence scores for topic models\n",
    "\n",
    "import pickle\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time, sleep # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import pandas as pd\n",
    "import logging # Logging module for generating log messages\n",
    "import sys # Provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter \n",
    "import shutil # High-level file operations such as copying and removal \n",
    "import zipfile # Provides tools to create, read, write, append, and list a ZIP file\n",
    "from tqdm.notebook import tqdm  # Creates progress bars in Jupyter Notebook environment\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import csv\n",
    "from dask.distributed import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure root logger level (this will affect all loggers unless overridden)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Create a file handler that logs messages to a file.\n",
    "file_handler = logging.FileHandler('C:/_harvester/dask-logs/dask_logs.log')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Get the logger for distributed.utils_perf and add the file handler.\n",
    "perf_logger = logging.getLogger('distributed.utils_perf')\n",
    "perf_logger.addHandler(file_handler)\n",
    "perf_logger.setLevel(logging.INFO)  # Adjust this level as needed\n",
    "\n",
    "# Remove all handlers associated with the root logger (including default StreamHandler)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "# Filter out the specific warning message\n",
    "warnings.filterwarnings(\"ignore\", module=\"distributed.utils_perf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range of number of topics for LDA and step size\n",
    "START_TOPICS = 1\n",
    "END_TOPICS = 2\n",
    "STEP_SIZE = 1\n",
    "\n",
    "# define the decade that is being modelled \n",
    "DECADE = '2010s'\n",
    "\n",
    "# In the case of this machine, since it has an Intel Core i9 processor with 8 physical cores (16 threads with Hyper-Threading), \n",
    "# it would be appropriate to set the number of workers in Dask Distributed LocalCluster to 8 or slightly lower to allow some CPU \n",
    "# resources for other tasks running on your system.\n",
    "CORES = 6\n",
    "THREADS_PER_CORE = 1\n",
    "\n",
    "# specify the number of passes for Gensim LdaModel\n",
    "PASSES = 15\n",
    "\n",
    "# specify the number of iterations\n",
    "ITERATIONS = 50\n",
    "\n",
    "# specify the chunk size for LdaModel object\n",
    "CHUNKSIZE = 4000\n",
    "\n",
    "# Number of documents to process per iteration\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create folder structure\n",
    "log_dir = f\"C:/_harvester/data/lda-models/{DECADE}_html/log/\"\n",
    "model_dir = f\"C:/_harvester/data/lda-models/2010s_html/train-eval-data/\"\n",
    "image_dir = f\"C:/_harvester/data/lda-models/{DECADE}_html/visuals/\"\n",
    "\n",
    "# Check if the directories exist and contain data\n",
    "if os.path.exists(log_dir) and os.path.exists(model_dir) and os.path.exists(image_dir):\n",
    "    log_files = os.listdir(log_dir)\n",
    "    model_files = os.listdir(model_dir)\n",
    "    image_files = os.listdir(image_dir)\n",
    "\n",
    "    # Check if the directories are not empty\n",
    "    if log_files or model_files or image_files:\n",
    "        # Find an available filename for the archive\n",
    "        counter = 0\n",
    "        while True:\n",
    "            archive_file = f\"C:/_harvester/data/lda-models/{DECADE}_html/archive{counter:04d}.zip\"\n",
    "            if not os.path.exists(archive_file):\n",
    "                break\n",
    "            counter += 1\n",
    "\n",
    "        # Create the zip file for archiving existing folders\n",
    "        with zipfile.ZipFile(archive_file, 'w') as zipf:\n",
    "            # Add log files to the zip file\n",
    "            for log_file in log_files:\n",
    "                zipf.write(os.path.join(log_dir, log_file), arcname=os.path.join(\"log\", log_file))\n",
    "            \n",
    "            # Add model files to the zip file\n",
    "            for model_file in model_files:\n",
    "                zipf.write(os.path.join(model_dir, model_file), arcname=os.path.join(\"model\", model_file))\n",
    "            \n",
    "            # Add image files to the zip file\n",
    "            for image_file in image_files:\n",
    "                zipf.write(os.path.join(image_dir, image_file), arcname=os.path.join(\"image\", image_file))\n",
    "\n",
    "        # Remove existing subdirectories after archiving them\n",
    "        for subdir in [log_dir, model_dir, image_dir]:\n",
    "            if os.path.exists(subdir):\n",
    "                subfiles = os.listdir(subdir)\n",
    "                for subfile in subfiles:\n",
    "                    filepath = os.path.join(subdir, subfile)\n",
    "                    if os.path.isdir(filepath):\n",
    "                        os.rmdir(filepath)\n",
    "\n",
    "# Create fresh directories for the new run\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "alpha_values += ['symmetric', 'asymmetric']\n",
    "\n",
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "beta_values += ['symmetric']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "The data_generator function is defined as a generator. It opens the specified JSON file (filename) \n",
    "and iterates over its lines using a for loop. Each line is parsed using json.loads() to convert it \n",
    "into a Python object (e.g., dictionary). The yield keyword is used instead of return to create a \n",
    "generator that produces one parsed JSON object at a time.\n",
    "\n",
    "The num_samples variable counts the total number of lines in the JSON file by opening it (open(filename)) \n",
    "and iterating over its lines using a generator expression (sum(1 for _ in open(filename))). This gives \n",
    "us an estimate of how many samples are present in the dataset.\n",
    "\n",
    "The num_train_samples variable calculates the desired number of samples for training based on the provided \n",
    "train_ratio. It multiplies num_samples by train_ratio, converting it to an integer using int().\n",
    "\n",
    "Two empty lists, train_data and eval_data, are initialized to store training and evaluation datasets, respectively.\n",
    "\n",
    "An instance of the `data_generator\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def futures_create_lda_datasets(filename, train_ratio):\n",
    "    # Get the file size in bytes\n",
    "    file_size = os.path.getsize(filename)\n",
    "\n",
    "    # Get the last modified timestamp of the file\n",
    "    last_modified = os.path.getmtime(filename)\n",
    "\n",
    "    # Print the metadata\n",
    "    print(\"\\nFile Metadata:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Size: {file_size} bytes\")\n",
    "    print(f\"Last Modified: {last_modified}\\n\")\n",
    "    \n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    \n",
    "    num_samples = len(data)  # Count the total number of samples\n",
    "    num_train_samples = int(num_samples * train_ratio)  # Calculate the number of samples for training\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_data = data[:num_train_samples]  # Assign a portion of data for training\n",
    "    eval_data = data[num_train_samples:]  # Assign the remaining data for evaluation\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_data)}\")\n",
    "    print(f\"Number of eval samples: {len(eval_data)}\")\n",
    "\n",
    "    # Create delayed objects for train and eval datasets\n",
    "    future_train_data = dask.delayed(train_data)\n",
    "    future_eval_data = dask.delayed(eval_data)\n",
    "\n",
    "    return future_train_data, future_eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This method trains a Latent Dirichlet Allocation (LDA) model using the Gensim library. Here is a breakdown of the steps involved:\n",
    "\n",
    "    (1)The method takes in parameters such as the number of topics (n_topics), alpha and beta hyperparameters, data (a list of documents), \n",
    "        and train_eval (a boolean indicating whether it's training or evaluation).\n",
    "\n",
    "    (2)If train_eval is True, a logging configuration is set up to log training information to a file named \"train-model.log\". \n",
    "        Otherwise, it logs to \"eval-model.log\".\n",
    "\n",
    "    (3) Two empty lists, combined_corpus and combined_text, are initialized to store the combined corpus and text.\n",
    "\n",
    "    (4) The number of passes for training the LDA model is set to 11.\n",
    "\n",
    "    (5) A loop iterates over each document in the data list. Inside the loop:\n",
    "            - A Gensim Dictionary object is created from the current document.\n",
    "            - The document is converted into a bag-of-words representation using doc2bow().\n",
    "            - A PerplexityMetric object is created to track perplexity during training.\n",
    "            - If combined_text is empty, indicating that it's the first iteration:\n",
    "                The initial LDA model is trained using LdaModel() with parameters such as corpus, \n",
    "                id2word (the dictionary), num_topics, alpha, beta, random_state, passes, iterations, chunksize, and per_word_topics.\n",
    "\n",
    "            - Otherwise:\n",
    "                The existing LDA model is updated with new data using lda_model_gensim.update(corpus).\n",
    "                The current document's text and corpus are added to combined_text and combined_corpus respectively.\n",
    "\n",
    "    (6) Logging is shut down.\n",
    "\n",
    "    (7) Finally, the trained LDA model (lda_model_gensim), combined_corpus, and combined_text are returned.\n",
    "\"\"\"\n",
    "\n",
    "def train_model(n_topics: int, alpha: list, beta: list, data: list, chunksize=BATCH_SIZE):\n",
    "    combined_corpus = []  # Initialize list to store combined corpus\n",
    "    combined_text = []\n",
    "    \n",
    "    # Convert the Delayed object to a Dask Bag and compute it to get the actual data\n",
    "    streaming_documents = data.compute()\n",
    "    \n",
    "    # Load or create a dictionary outside the loop to track word IDs across batches\n",
    "    dictionary_global = Dictionary()\n",
    "\n",
    "    num_documents = len(streaming_documents)\n",
    "    \n",
    "    model_data = {\n",
    "        'lda_model': [], # lda_model_gensim,\n",
    "        'corpus': [], # combined_corpus,\n",
    "        'text': [], #combined_text,\n",
    "        'convergence': [], #convergence_score,\n",
    "        'perplexity': [], # perplexity_score,\n",
    "        'coherence': [], #coherence_score,\n",
    "        'dictionary': [], #dictionary_global,\n",
    "        'topics': [], #n_topics,\n",
    "        'alpha': [], #alpha,\n",
    "        'beta': []  #beta ,\n",
    "    }\n",
    "\n",
    "    batch_size = chunksize  # Number of documents to process per iteration\n",
    "    \n",
    "    for start_index in range(0, num_documents, batch_size):\n",
    "        end_index = min(start_index + batch_size, num_documents)\n",
    "        \n",
    "        batch_documents = streaming_documents[start_index:end_index]\n",
    "\n",
    "        for texts_out in batch_documents:\n",
    "            dictionary_global.add_documents([texts_out])\n",
    "            corpus_single_doc = [dictionary_global.doc2bow(texts_out)]\n",
    "            \n",
    "            lda_model_gensim = LdaModel(corpus=corpus_single_doc,\n",
    "                                        id2word=dictionary_global,\n",
    "                                        num_topics=n_topics,\n",
    "                                        alpha=alpha,\n",
    "                                        eta=beta,\n",
    "                                        random_state=75,\n",
    "                                        passes=PASSES,\n",
    "                                        iterations=ITERATIONS,\n",
    "                                        chunksize=CHUNKSIZE,\n",
    "                                        per_word_topics=True)\n",
    "\n",
    "            combined_text.extend(texts_out)\n",
    "            combined_corpus.extend(corpus_single_doc)\n",
    "\n",
    "            # calculate metrics\n",
    "            convergence_score = None\n",
    "            perplexity_score = None\n",
    "            coherence_score = None\n",
    "\n",
    "            convergence_score = lda_model_gensim.bound(corpus_single_doc)\n",
    "            perplexity_score = lda_model_gensim.log_perplexity(corpus_single_doc)\n",
    "\n",
    "            # Check for runtime warnings during topic coherence calculation\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                coherence_model = CoherenceModel(model=lda_model_gensim, texts=texts_out, dictionary=dictionary_global, coherence='c_v')\n",
    "                coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "            model_data['lda_model'].append(lda_model_gensim)\n",
    "            model_data['corpus'].append(combined_corpus)\n",
    "            model_data['text'].append(combined_text)\n",
    "            model_data['convergence'].append(convergence_score)\n",
    "            model_data['perplexity'].append(perplexity_score)\n",
    "            model_data['coherence'].append(coherence_score)\n",
    "            model_data['dictionary'].append(dictionary_global)\n",
    "            model_data['topics'].append(n_topics)\n",
    "            model_data['alpha'].append(alpha)\n",
    "            model_data['beta'].append(beta)\n",
    "\n",
    "    # Verify that combined_text contains all the original text\n",
    "    original_tokens = sum((len(doc) for doc in streaming_documents), 0)\n",
    "    assert len(combined_text) == original_tokens, \"Combined text does not contain all the original text\"\n",
    "\n",
    "    return (lda_model_gensim, \\\n",
    "            n_topics, \\\n",
    "            alpha, \\\n",
    "            beta, \\\n",
    "            model_data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Define a function to save models and log data\n",
    "def save_model_and_log(model_data, n_topics, alpha, beta, lda_model, model_dir, log_dir, train_or_eval=None):\n",
    "    # Normalize alpha and beta values into strings suitable for filenames\n",
    "    alpha_str = '_'.join(map(str, alpha)) if isinstance(alpha, list) else str(alpha)\n",
    "    beta_str = '_'.join(map(str, beta)) if isinstance(beta, list) else str(beta)\n",
    "\n",
    "    # Construct a unique filename for each model using its parameters\n",
    "    if train_or_eval == True:\n",
    "        filename = f\"train-lda_model_topics{n_topics}_alpha{alpha_str}_beta{beta_str}.model\"\n",
    "    else:\n",
    "        filename = f\"eval-lda_model_topics{n_topics}_alpha{alpha_str}_beta{beta_str}.model\"\n",
    "    \n",
    "    # Ensure that any special characters are removed or replaced in filename components\n",
    "    filename = filename.replace('.', 'p').replace('/', '-').replace('\\\\', '-')\n",
    "    \n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    \n",
    "    # Save the model\n",
    "    lda_model.save(filepath)\n",
    "\n",
    "    # Specify the filename for the CSV file\n",
    "    if train_or_eval == True:\n",
    "        csv_filename = \"train-lda-model-train-data.csv\"\n",
    "    else:\n",
    "        csv_filename = 'eval-lda-model-train-data.csv'\n",
    "        \n",
    "    csv_path = os.path.join(log_dir, csv_filename)\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "    # Write or append data to the CSV file\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write header row if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writerow(model_data.keys())\n",
    "        \n",
    "        # Append data rows\n",
    "        writer.writerow(model_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client is connected to a scheduler.\n",
      "Dask workers are running.\n",
      "Creating training and evaluation samples...\n",
      "Completed creation of training and evaluation samples in 0.0 minutes.\n",
      "\n",
      "\n",
      "File Metadata:\n",
      "Filename: C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-w-bigrams.json\n",
      "Size: 81447561 bytes\n",
      "Last Modified: 1713177730.7095945\n",
      "\n",
      "Number of training samples: 333967\n",
      "Number of eval samples: 83492\n",
      "Beginning data scatter...\n",
      "Data scatter complete...\n",
      "\n",
      "Model training and evaluation tasks submitted...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pqn7\\.conda\\envs\\nlp\\lib\\site-packages\\distributed\\client.py:3157: UserWarning: Sending large graph of size 14.38 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pqn7\\.conda\\envs\\nlp\\lib\\site-packages\\distributed\\client.py:3157: UserWarning: Sending large graph of size 57.34 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-04-18 08:49:51,800 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:49:59,494 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:50:26,403 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:50:41,110 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:50:49,854 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:51:02,393 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:51:24,662 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:51:42,859 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:51:50,908 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:52:04,212 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:52:21,081 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:52:36,195 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:52:44,262 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:03,012 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:16,762 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:27,045 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:34,512 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:52,161 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:53:58,811 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:54:24,576 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:54:33,878 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:54:45,640 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:54:53,761 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:55:09,895 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:55:25,045 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:55:32,442 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:56:00,093 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:56:06,844 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:56:19,494 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:56:33,494 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:56:43,844 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:57:00,542 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:57:18,793 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:57:26,810 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:57:40,310 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:58:01,210 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:58:21,777 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:58:29,998 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:58:43,527 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:59:00,408 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:59:07,065 - distributed.utils_perf - WARNING - full garbage collections took 15% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:59:34,304 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:59:42,266 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 08:59:55,947 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:00:06,766 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:00:34,278 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:00:42,061 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:00:54,481 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:01:11,263 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:01:20,098 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:01:35,548 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:01:50,156 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:02:03,091 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:02:11,916 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:02:36,794 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:02:44,599 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:02:59,825 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:03:07,613 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:03:29,297 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:03:39,829 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:03:49,080 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:04:00,396 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:04:15,014 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "c:\\Users\\pqn7\\.conda\\envs\\nlp\\lib\\site-packages\\gensim\\models\\ldamodel.py:850: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n",
      "2024-04-18 09:04:29,818 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:04:46,915 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:04:57,328 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:05:04,346 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:05:24,279 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:05:42,263 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:05:54,164 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:06:01,200 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:06:19,290 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:06:38,663 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:06:45,381 - distributed.utils_perf - WARNING - full garbage collections took 19% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:06:58,045 - distributed.utils_perf - WARNING - full garbage collections took 19% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:07:26,739 - distributed.utils_perf - WARNING - full garbage collections took 19% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:07:37,345 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:07:52,981 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:08:06,011 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:08:17,812 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:08:41,578 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:08:54,078 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:09:05,961 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:09:17,362 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:09:30,361 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:09:47,877 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:10:06,877 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:10:16,945 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:10:25,511 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:10:51,311 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:11:04,775 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:11:20,278 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:11:31,011 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:11:53,776 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:12:03,626 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:12:16,243 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:12:30,367 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:12:39,594 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:12:54,092 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:13:10,382 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:13:24,162 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:13:41,916 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:13:49,642 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:14:16,048 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:14:22,916 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:14:36,142 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:14:56,402 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:15:15,643 - distributed.utils_perf - WARNING - full garbage collections took 18% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:15:29,204 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:15:40,293 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:15:54,465 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:16:10,757 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:16:29,774 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:16:42,820 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:16:55,623 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:17:10,024 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:17:22,750 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:17:46,572 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:17:56,702 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:18:14,230 - distributed.utils_perf - WARNING - full garbage collections took 16% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:18:24,459 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:18:49,897 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:18:57,853 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:19:11,885 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:19:35,534 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:20:03,714 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:20:11,567 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:20:25,515 - distributed.utils_perf - WARNING - full garbage collections took 17% CPU time recently (threshold: 10%)\n",
      "2024-04-18 09:24:25,770 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    import dask   # Parallel computing library that scales Python workflows across multiple cores or machines \n",
    "    from dask.distributed import Client, LocalCluster, wait   # Distributed computing framework that extends Dask functionality \n",
    "    from dask.diagnostics import ProgressBar   # Visualizes progress of Dask computations\n",
    "    from dask.distributed import progress\n",
    "    from dask.delayed import Delayed # Decorator for creating delayed objects in Dask computations\n",
    "    from dask.distributed import as_completed\n",
    "    from dask.bag import Bag\n",
    "    from dask import delayed\n",
    "    import dask.config\n",
    "    from dask.distributed import wait\n",
    "\n",
    "    # Specify the local directory path\n",
    "    DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    # https://medium.com/@aryan.gupta18/end-to-end-recommender-systems-with-merlin-part-1-89fabe2fa05b\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify GPU device IDs\n",
    "    protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "    num_gpus = 1\n",
    "    NUM_GPUS=[0]\n",
    "    visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Select devices to place workers\n",
    "    device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "    device_pool_frac = 0.8\n",
    "    part_mem_frac = 0.15\n",
    "\n",
    "    # Manually specify the total device memory size (in bytes)\n",
    "    device_size = 10 * 1024 * 1024 * 1024  # GPU has 12GB but setting at 10GB\n",
    "            \n",
    "    ram_memory_limit = \"100GB\" # Set the RAM memory limit (per worker)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=CORES,\n",
    "            threads_per_worker=THREADS_PER_CORE,\n",
    "            processes=False,\n",
    "            memory_limit=ram_memory_limit,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=None,\n",
    "            #dashboard_address=\":8787\",\n",
    "            #protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = ram_memory_limit\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "    \n",
    "    # Load data from the JSON file\n",
    "    filename = \"C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-w-bigrams.json\"\n",
    "    train_ratio = 0.8\n",
    "    \n",
    "\n",
    "    # create training and evaluation data\n",
    "    print(\"Creating training and evaluation samples...\")\n",
    "    started = time()\n",
    "    future = client.submit(futures_create_lda_datasets, filename, train_ratio)\n",
    "    print(f\"Completed creation of training and evaluation samples in {round((time()- started)/60,2)} minutes.\\n\")\n",
    "\n",
    "    # Wait for future to complete and retrieve results\n",
    "    train_data, eval_data = future.result()\n",
    "\n",
    "    # Scatter the computed training and evaluation data across workers\n",
    "    print(\"Beginning data scatter...\")\n",
    "    scattered_train_data_future = client.scatter(train_data)\n",
    "    scattered_eval_data_future = client.scatter(eval_data)\n",
    "    print(\"Data scatter complete...\\n\")\n",
    "    \n",
    "\n",
    "    train_futures = []  # List to store futures for training\n",
    "    eval_futures = []  # List to store futures for evaluation\n",
    "\n",
    "\n",
    "# Loop over different values of n_topics, alpha_value, and beta_value\n",
    "for n_topics in range(START_TOPICS, END_TOPICS + 1, STEP_SIZE):\n",
    "    for alpha_value in alpha_values:\n",
    "        for beta_value in beta_values:\n",
    "            #print(f\"Training model with n_topics={n_topics}, alpha={alpha_value}, beta={beta_value}\")\n",
    "\n",
    "            # Submit train_model function directly with scattered futures\n",
    "            future_train = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_train_data_future)\n",
    "            future_eval = client.submit(train_model, n_topics, alpha_value, beta_value, scattered_eval_data_future)\n",
    "\n",
    "            # Training models\n",
    "            def callback_train(future):\n",
    "                try:\n",
    "                    # Retrieve the result of the training future\n",
    "                    result_train = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred during training: {e}\")\n",
    "                else:\n",
    "                    # Save the trained model and log data\n",
    "                    save_model_and_log(*result_train[1:], model_dir=model_dir,\n",
    "                                    log_dir=log_dir, train_or_eval=True)\n",
    "\n",
    "            # Evaluation models\n",
    "            def callback_eval(future):\n",
    "                try:\n",
    "                    # Retrieve the result of the evaluation future\n",
    "                    result_eval = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred during evaluation: {e}\")\n",
    "                else:\n",
    "                    # Save the evaluation model and log data\n",
    "                    save_model_and_log(*result_eval[1:], model_dir=model_dir,\n",
    "                                    log_dir=log_dir, train_or_eval=False)\n",
    "\n",
    "\n",
    "            # Add a callback to save the model once training is complete\n",
    "            future_train.add_done_callback(callback_train)\n",
    "            future_eval.add_done_callback(callback_eval)\n",
    "\n",
    "            # Append futures to separate lists for training and evaluation models\n",
    "            train_futures.append(future_train)\n",
    "            eval_futures.append(future_eval)\n",
    "\n",
    "    print(\"Model training and evaluation tasks submitted...\\n\")\n",
    "\n",
    "    # Now, instead of waiting for all futures to complete with wait(),\n",
    "    # we can process them as they complete using as_completed.\n",
    "    #for future in as_completed(train_futures + eval_futures):\n",
    "    for future in as_completed(train_futures + eval_futures):\n",
    "        # As each future completes, its result will be processed by the callback function.\n",
    "        pass  # The 'pass' statement is just a placeholder since the actual work is done in callbacks.\n",
    "\n",
    "    # Gather all trained models back to the client (optional)\n",
    "    print(\"gathering all trained models back to the client...\")\n",
    "    trained_models = [future.result() for future in train_futures]\n",
    "    evaluation_models = [future.result() for future in eval_futures]\n",
    "\n",
    "    # Save each trained model with specific filenames based on its parameters outside of the loop\n",
    "    #print(\"Saving models...\")\n",
    "    for model_data in tqdm(trained_models, desc=\"Saving training models...\", total=len(trained_models)):\n",
    "        model_data, n_topics, alpha_value, beta_value,\n",
    "        lda_model = model_data[0]  # Assuming the trained model is the last element in the result tuple\n",
    "        save_model_and_log(model_data, n_topics, alpha_value, beta_value, lda_model, model_dir=model_dir, log_dir=log_dir, train_or_eval=True)\n",
    "\n",
    "    # Save each trained model with specific filenames based on its parameters outside of the loop\n",
    "    #print(\"Saving models...\")\n",
    "    for model_data in tqdm(evaluation_models, desc=\"Saving evaluation models...\", total=len(evaluation_models)):\n",
    "        model_data, n_topics, alpha_value, beta_value,\n",
    "        lda_model = model_data[0]  # Assuming the trained model is the last element in the result tuple\n",
    "        save_model_and_log(model_data, n_topics, alpha_value, beta_value, lda_model, model_dir=model_dir, log_dir=log_dir, train_or_eval=False)\n",
    "\n",
    "    print(\"Completed saving models and logging data.\")\n",
    "\n",
    "    # Close Dask client after all operations are complete\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
