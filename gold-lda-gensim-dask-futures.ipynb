{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim  # Library for interactive topic model visualization\n",
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel  # Computes coherence scores for topic models\n",
    "\n",
    "import pickle\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time, sleep # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import pandas as pd\n",
    "import logging # Logging module for generating log messages\n",
    "import sys # Provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter \n",
    "import shutil # High-level file operations such as copying and removal \n",
    "import zipfile # Provides tools to create, read, write, append, and list a ZIP file\n",
    "from tqdm.notebook import tqdm  # Creates progress bars in Jupyter Notebook environment\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import csv\n",
    "from dask.distributed import as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range of number of topics for LDA and step size\n",
    "START_TOPICS = 1\n",
    "END_TOPICS = 2\n",
    "STEP_SIZE = 1\n",
    "\n",
    "# define the decade that is being modelled \n",
    "DECADE = '2010s'\n",
    "\n",
    "# In the case of this machine, since it has an Intel Core i9 processor with 8 physical cores (16 threads with Hyper-Threading), \n",
    "# it would be appropriate to set the number of workers in Dask Distributed LocalCluster to 8 or slightly lower to allow some CPU \n",
    "# resources for other tasks running on your system.\n",
    "CORES = 6\n",
    "\n",
    "THREADS_PER_CORE = 1\n",
    "\n",
    "RAM_MEMORY_LIMIT = \"100GB\" \n",
    "\n",
    "# Specify the local directory path\n",
    "DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "# specify the number of passes for Gensim LdaModel\n",
    "PASSES = 15\n",
    "\n",
    "# specify the number of iterations\n",
    "ITERATIONS = 50\n",
    "\n",
    "# specify the chunk size for LdaModel object\n",
    "CHUNKSIZE = 4000\n",
    "\n",
    "# Number of documents to process per iteration\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "# Load data from the JSON file\n",
    "DATA_SOURCE = \"C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-w-bigrams.json\"\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "TIMEOUT = 90\n",
    "\n",
    "EXTENDED_TIMEOUT = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create folder structure\n",
    "LOG_DIR = f\"C:/_harvester/data/lda-models/{DECADE}_html/log/\"\n",
    "MODEL_DIR = f\"C:/_harvester/data/lda-models/2010s_html/train-eval-data/\"\n",
    "IMAGE_DIR = f\"C:/_harvester/data/lda-models/{DECADE}_html/visuals/\"\n",
    "\n",
    "# Check if the directories exist and contain data\n",
    "if os.path.exists(LOG_DIR) and os.path.exists(MODEL_DIR) and os.path.exists(IMAGE_DIR):\n",
    "    log_files = os.listdir(LOG_DIR)\n",
    "    model_files = os.listdir(MODEL_DIR)\n",
    "    image_files = os.listdir(IMAGE_DIR)\n",
    "\n",
    "    # Check if the directories are not empty\n",
    "    if log_files or model_files or image_files:\n",
    "        # Find an available filename for the archive\n",
    "        counter = 0\n",
    "        while True:\n",
    "            archive_file = f\"C:/_harvester/data/lda-models/{DECADE}_html/archive{counter:04d}.zip\"\n",
    "            if not os.path.exists(archive_file):\n",
    "                break\n",
    "            counter += 1\n",
    "\n",
    "        # Create the zip file for archiving existing folders\n",
    "        with zipfile.ZipFile(archive_file, 'w') as zipf:\n",
    "            # Add log files to the zip file\n",
    "            for log_file in log_files:\n",
    "                zipf.write(os.path.join(LOG_DIR, log_file), arcname=os.path.join(\"log\", log_file))\n",
    "            \n",
    "            # Add model files to the zip file\n",
    "            for model_file in model_files:\n",
    "                zipf.write(os.path.join(MODEL_DIR, model_file), arcname=os.path.join(\"model\", model_file))\n",
    "            \n",
    "            # Add image files to the zip file\n",
    "            for image_file in image_files:\n",
    "                zipf.write(os.path.join(IMAGE_DIR, image_file), arcname=os.path.join(\"image\", image_file))\n",
    "\n",
    "        # Remove existing subdirectories after archiving them\n",
    "        for subdir in [LOG_DIR, MODEL_DIR, IMAGE_DIR]:\n",
    "            if os.path.exists(subdir):\n",
    "                subfiles = os.listdir(subdir)\n",
    "                for subfile in subfiles:\n",
    "                    filepath = os.path.join(subdir, subfile)\n",
    "                    if os.path.isdir(filepath):\n",
    "                        os.rmdir(filepath)\n",
    "\n",
    "# Create fresh directories for the new run\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure root logger level (this will affect all loggers unless overridden)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Create a file handler that logs messages to a file.\n",
    "file_handler = logging.FileHandler('C:/_harvester/dask-logs/dask-logs.log')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Get the logger for distributed.utils_perf and add the file handler.\n",
    "perf_logger = logging.getLogger('distributed.utils_perf')\n",
    "perf_logger.addHandler(file_handler)\n",
    "perf_logger.setLevel(logging.INFO)  # Adjust this level as needed\n",
    "\n",
    "# Remove all handlers associated with the root logger (including default StreamHandler)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "# Filter out the specific warning message\n",
    "warnings.filterwarnings(\"ignore\", module=\"distributed.utils_perf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "alpha_values += ['symmetric', 'asymmetric']\n",
    "\n",
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "beta_values += ['symmetric']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def futures_create_lda_datasets(filename, train_ratio):\n",
    "    # Get the file size in bytes\n",
    "    file_size = os.path.getsize(filename)\n",
    "\n",
    "    # Get the last modified timestamp of the file\n",
    "    last_modified = os.path.getmtime(filename)\n",
    "\n",
    "    # Print the metadata\n",
    "    print(\"\\nFile Metadata:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Size: {file_size} bytes\")\n",
    "    print(f\"Last Modified: {last_modified}\\n\")\n",
    "    \n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    \n",
    "    num_samples = len(data)  # Count the total number of samples\n",
    "    num_train_samples = int(num_samples * train_ratio)  # Calculate the number of samples for training\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_data = data[:num_train_samples]  # Assign a portion of data for training\n",
    "    eval_data = data[num_train_samples:]  # Assign the remaining data for evaluation\n",
    "\n",
    "    print(f\"Number of training samples: {len(train_data)}\")\n",
    "    print(f\"Number of eval samples: {len(eval_data)}\")\n",
    "\n",
    "    # Create delayed objects for train and eval datasets\n",
    "    future_train_data = dask.delayed(train_data)\n",
    "    future_eval_data = dask.delayed(eval_data)\n",
    "\n",
    "    return future_train_data, future_eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_log(model_data, n_topics, alpha, beta, lda_model, model_dir, log_dir, train_or_eval=None):\n",
    "    # Normalize alpha and beta values into strings suitable for filenames\n",
    "    alpha_str = '_'.join(map(str, alpha)) if isinstance(alpha, list) else str(alpha)\n",
    "    beta_str = '_'.join(map(str, beta)) if isinstance(beta, list) else str(beta)\n",
    "\n",
    "    # Construct a unique filename for each model using its parameters\n",
    "    if train_or_eval:\n",
    "        filename = f\"train-lda_model_topics{n_topics}_alpha{alpha_str}_beta{beta_str}.model\"\n",
    "    else:\n",
    "        filename = f\"eval-lda_model_topics{n_topics}_alpha{alpha_str}_beta{beta_str}.model\"\n",
    "\n",
    "    # Ensure that any special characters are removed or replaced in filename components\n",
    "    filename = filename.replace('.', 'p').replace('/', '-').replace('\\\\', '-')\n",
    "\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # Save the model\n",
    "    lda_model.save(filepath)\n",
    "\n",
    "    # Specify the filename for the CSV file\n",
    "    if train_or_eval:\n",
    "        csv_filename = \"train-lda-model-train-data.csv\"\n",
    "    else:\n",
    "        csv_filename = 'eval-lda-model-train-data.csv'\n",
    "\n",
    "    csv_path = os.path.join(log_dir, csv_filename)\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "    # Write or append data to the CSV file using a context manager\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header row if file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writerow(model_data.keys())\n",
    "\n",
    "        # Append data rows\n",
    "        writer.writerow(model_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This method trains a Latent Dirichlet Allocation (LDA) model using the Gensim library. Here is a breakdown of the steps involved:\n",
    "\n",
    "    (1)The method takes in parameters such as the number of topics (n_topics), alpha and beta hyperparameters, data (a list of documents), \n",
    "        and train_eval (a boolean indicating whether it's training or evaluation).\n",
    "\n",
    "    (2)If train_eval is True, a logging configuration is set up to log training information to a file named \"train-model.log\". \n",
    "        Otherwise, it logs to \"eval-model.log\".\n",
    "\n",
    "    (3) Two empty lists, combined_corpus and combined_text, are initialized to store the combined corpus and text.\n",
    "\n",
    "    (4) The number of passes for training the LDA model is set to 11.\n",
    "\n",
    "    (5) A loop iterates over each document in the data list. Inside the loop:\n",
    "            - A Gensim Dictionary object is created from the current document.\n",
    "            - The document is converted into a bag-of-words representation using doc2bow().\n",
    "            - A PerplexityMetric object is created to track perplexity during training.\n",
    "            - If combined_text is empty, indicating that it's the first iteration:\n",
    "                The initial LDA model is trained using LdaModel() with parameters such as corpus, \n",
    "                id2word (the dictionary), num_topics, alpha, beta, random_state, passes, iterations, chunksize, and per_word_topics.\n",
    "\n",
    "            - Otherwise:\n",
    "                The existing LDA model is updated with new data using lda_model_gensim.update(corpus).\n",
    "                The current document's text and corpus are added to combined_text and combined_corpus respectively.\n",
    "\n",
    "    (6) Logging is shut down.\n",
    "\n",
    "    (7) Finally, the trained LDA model (lda_model_gensim), combined_corpus, and combined_text are returned.\n",
    "\"\"\"\n",
    "def train_model(n_topics: int, alpha: list, beta: list, data: list, chunksize=BATCH_SIZE):\n",
    "    combined_corpus = []  # Initialize list to store combined corpus\n",
    "    combined_text = []\n",
    "    \n",
    "    # Convert the Delayed object to a Dask Bag and compute it to get the actual data\n",
    "    streaming_documents = data.compute()\n",
    "    \n",
    "    # Load or create a dictionary outside the loop to track word IDs across batches\n",
    "    dictionary_global = Dictionary()\n",
    "\n",
    "    num_documents = len(streaming_documents)\n",
    "    \n",
    "    model_data = {\n",
    "        'text': [],  # combined_text,\n",
    "        'convergence': [],  # convergence_score,\n",
    "        'perplexity': [],  # perplexity_score,\n",
    "        'coherence': [],  # coherence_score,\n",
    "        'dictionary': [],  # dictionary_global,\n",
    "        'topics': [],  # n_topics,\n",
    "        'alpha': [],  # alpha,\n",
    "        'beta': []  # beta ,\n",
    "    }\n",
    "\n",
    "    batch_size = chunksize  # Number of documents to process per iteration\n",
    "\n",
    "    for start_index in range(0, num_documents, batch_size):\n",
    "        end_index = min(start_index + batch_size, num_documents)\n",
    "\n",
    "        batch_documents = streaming_documents[start_index:end_index]\n",
    "\n",
    "        for texts_out in batch_documents:\n",
    "            dictionary_global.add_documents([texts_out])\n",
    "            corpus_single_doc = [dictionary_global.doc2bow(texts_out)]\n",
    "\n",
    "            lda_model_gensim = LdaModel(corpus=corpus_single_doc,\n",
    "                                        id2word=dictionary_global,\n",
    "                                        num_topics=n_topics,\n",
    "                                        alpha=alpha,\n",
    "                                        eta=beta,\n",
    "                                        random_state=75,\n",
    "                                        passes=PASSES,\n",
    "                                        iterations=ITERATIONS,\n",
    "                                        chunksize=CHUNKSIZE,\n",
    "                                        per_word_topics=True)\n",
    "\n",
    "            combined_text.extend(texts_out)\n",
    "            combined_corpus.extend(corpus_single_doc)\n",
    "\n",
    "            convergence_score = lda_model_gensim.bound(corpus_single_doc)\n",
    "\n",
    "            try:\n",
    "                perplexity_score = lda_model_gensim.log_perplexity(corpus_single_doc)\n",
    "            except RuntimeWarning:\n",
    "                perf_logger.warn(\"Perplexity calculation encountered an overflow error. Setting perplexity_score = float('inf')\")\n",
    "                perplexity_score = float('inf')\n",
    "\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                coherence_model = CoherenceModel(model=lda_model_gensim, texts=texts_out, dictionary=dictionary_global, coherence='c_v')\n",
    "                coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "            model_data['text'].append(combined_text)\n",
    "            model_data['convergence'].append(convergence_score)\n",
    "            model_data['perplexity'].append(perplexity_score)\n",
    "            model_data['coherence'].append(coherence_score)\n",
    "            model_data['dictionary'].append(dictionary_global)\n",
    "            model_data['topics'].append(n_topics)\n",
    "            model_data['alpha'].append(alpha)\n",
    "            model_data['beta'].append(beta)\n",
    "\n",
    "    # Verify that combined_text contains all the original text\n",
    "    original_tokens = sum((len(doc) for doc in streaming_documents), 0)\n",
    "    try:\n",
    "        assert len(combined_text) == original_tokens, \"Combined text does not yet contain all the original text\"\n",
    "    except AssertionError as e:\n",
    "        perf_logger.info(str(e))\n",
    "    \n",
    "    return (model_data, n_topics, alpha, beta, lda_model_gensim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models\n",
    "def callback_train(future):\n",
    "    try:\n",
    "        # Retrieve the result of the training future\n",
    "        result_train = future.result()\n",
    "        # Save the trained model and log data immediately\n",
    "        save_model_and_log(*result_train[:], model_dir=MODEL_DIR, log_dir=LOG_DIR, train_or_eval=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during training: {e}\")\n",
    "    else:\n",
    "        # Update progress bar after saving each model (only if no exception occurred)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Evaluation models\n",
    "def callback_eval(future):\n",
    "    try:\n",
    "        # Retrieve the result of the evaluation future\n",
    "        result_eval = future.result()\n",
    "        # Save the evaluation model and log data immediately\n",
    "        save_model_and_log(*result_eval[:], model_dir=MODEL_DIR, log_dir=LOG_DIR, train_or_eval=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during evaluation: {e}\")\n",
    "    else:\n",
    "        # Update progress bar after saving each model (only if no exception occurred)\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                    - The `process_completed_future` function is called when all futures in a batch complete within the specified timeout. It \n",
    "                        can be used to continue with your program using both completed training and evaluation futures.\n",
    "                    - The `retry_processing` function is called when there are incomplete futures after iterating through a batch of \n",
    "                        data. It can be used to retry processing with those incomplete futures.\n",
    "                    - The code checks if there are any remaining futures in the lists after completing all iterations. If so, it \n",
    "                        waits for them to complete and handles them accordingly.\n",
    "\"\"\"\n",
    "# List to store parameters of models that failed to complete even after a retry\n",
    "failed_model_params = []\n",
    "\n",
    "# Mapping from futures to their corresponding parameters (n_topics, alpha_value, beta_value)\n",
    "future_to_params = {}\n",
    "\n",
    "# Function to process completed futures\n",
    "def process_completed_futures(completed_train_futures, completed_eval_futures):\n",
    "    # Continue with the program using both completed train and eval futures\n",
    "    pass\n",
    "\n",
    "# Function to retry processing with incomplete futures\n",
    "def retry_processing(incomplete_train_futures, incomplete_eval_futures, timeout):\n",
    "    # Retry processing with incomplete futures using an extended timeout\n",
    "    done, not_done = dask.wait(incomplete_train_futures , incomplete_eval_futures, timeout=timeout)\n",
    "    \n",
    "    # Process completed ones\n",
    "    process_completed_futures([f for f in done if f in incomplete_train_futures],\n",
    "                              [f for f in done if f in incomplete_eval_futures])\n",
    "    \n",
    "    # Record parameters of still incomplete futures for later review\n",
    "    failed_model_params.extend(future_to_params[future] for future in not_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Execution as said by Brunhilda:\n",
    "\n",
    "Asynchronous execution allows you to execute tasks concurrently, without waiting for each task to complete before moving on \\\n",
    "to the next one. This can improve the overall efficiency and speed of your program.\n",
    "\n",
    "In the given code snippet, asynchronous execution is achieved using Dask's as_completed function. This function takes a list \\\n",
    "of futures (representing tasks) and returns an iterator that yields futures as they complete.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(1) First, you submit all your training and evaluation tasks using client.submit(). These tasks are represented by futures. \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(2) You add callback functions (callback_train and callback_eval) to these futures using the add_done_callback() method. These callbacks will be executed when their respective futures complete.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(3) You create two lists, train_futures and eval_futures, to store the futures for training and evaluation models respectively.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(4) After submitting all the tasks, you enter a loop where you iterate over the range of values for n_topics, alpha_value, and beta_value.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(5) Inside this loop, you submit the training and evaluation tasks for each combination of parameters using client.submit(). These new futures are added to their respective lists.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(6) Next, you use the as_completed function to iterate over both lists of futures (train_futures and eval_futures). This function returns an iterator that yields completed futures as they become available.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(7) As each future completes, its associated callback function (callback_train or callback_eval) is executed.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(8) Inside these callback functions, you retrieve the result of the completed future using .result(). You can then save the trained or evaluated model using the provided save_model_and_log function.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(9) The loop continues until all combinations of parameters have been processed.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;(10) Finally, after all models have been saved and logged, you close the Dask client. \n",
    "\n",
    "By utilizing asynchronous execution with Dask's as_completed, your program can process multiple tasks concurrently while still ensuring that each model is saved once its associated task has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    import dask   # Parallel computing library that scales Python workflows across multiple cores or machines \n",
    "    from dask.distributed import Client, LocalCluster, wait   # Distributed computing framework that extends Dask functionality \n",
    "    from dask.diagnostics import ProgressBar   # Visualizes progress of Dask computations\n",
    "    from dask.distributed import progress\n",
    "    from dask.delayed import Delayed # Decorator for creating delayed objects in Dask computations\n",
    "    from dask.distributed import as_completed\n",
    "    from dask.bag import Bag\n",
    "    from dask import delayed\n",
    "    import dask.config\n",
    "    from dask.distributed import wait\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=CORES,\n",
    "            threads_per_worker=THREADS_PER_CORE,\n",
    "            processes=False,\n",
    "            memory_limit=RAM_MEMORY_LIMIT,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=None,\n",
    "            #dashboard_address=\":8787\",\n",
    "            #protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = RAM_MEMORY_LIMIT\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "\n",
    "    # create training and evaluation data\n",
    "    print(\"Creating training and evaluation samples...\")\n",
    "    started = time()\n",
    "    future = client.submit(futures_create_lda_datasets, DATA_SOURCE, TRAIN_RATIO)\n",
    "    print(f\"Completed creation of training and evaluation samples in {round((time()- started)/60,2)} minutes.\\n\")\n",
    "\n",
    "    # Wait for future to complete and retrieve results\n",
    "    train_data, eval_data = future.result()\n",
    "\n",
    "    # Scatter the computed training and evaluation data across workers\n",
    "    print(\"Beginning data scatter...\")\n",
    "    scattered_train_data_future = client.scatter(train_data)\n",
    "    scattered_eval_data_future = client.scatter(eval_data)\n",
    "    print(\"Data scatter complete...\\n\")\n",
    "    \n",
    "\n",
    "    train_futures = []  # List to store futures for training\n",
    "    eval_futures = []  # List to store futures for evaluation\n",
    "    \n",
    "    num_topics = len(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE))\n",
    "    num_alpha_values = len(alpha_values)\n",
    "    num_beta_values = len(beta_values)\n",
    "\n",
    "    TOTAL_MODELS = (num_topics * num_alpha_values * num_beta_values) * 2\n",
    "\n",
    "    progress_bar = tqdm(total=TOTAL_MODELS, desc=\"Creating and saving models\")\n",
    "    \n",
    "    for n_topics in range(START_TOPICS, END_TOPICS + 1, STEP_SIZE):\n",
    "        for alpha_value in alpha_values:\n",
    "            for beta_value in beta_values:\n",
    "                future_train = client.submit(train_model, n_topics, alpha_value, beta_value,\n",
    "                                             scattered_train_data_future)\n",
    "                future_eval = client.submit(train_model, n_topics, alpha_value, beta_value,\n",
    "                                            scattered_eval_data_future)\n",
    "                \n",
    "                # Map the created futures to their parameters so we can identify them later if needed\n",
    "                future_to_params[future_train] = (n_topics, alpha_value, beta_value)\n",
    "                future_to_params[future_eval] = (n_topics, alpha_value, beta_value)\n",
    "\n",
    "                #future_train.add_done_callback(callback_train)\n",
    "                #future_eval.add_done_callback(callback_eval)\n",
    "\n",
    "                train_futures.append(future_train)\n",
    "                eval_futures.append(future_eval)\n",
    "\n",
    "\n",
    "                if len(train_futures) >= BATCH_SIZE:\n",
    "                    # Wait for all futures in both lists\n",
    "                    done, not_done = dask.wait(train_futures + eval_futures, timeout=TIMEOUT)\n",
    "                    \n",
    "                    # Separate out completed train and eval futures\n",
    "                    completed_train_futures = [f for f in done if f in train_futures]\n",
    "                    completed_eval_futures = [f for f in done if f in eval_futures]\n",
    "                    \n",
    "                    # Process only the completed ones\n",
    "                    process_completed_futures(completed_train_futures, completed_eval_futures)\n",
    "                    progress_bar.update(len(completed_train_futures) + len(completed_eval_futures))\n",
    "                    \n",
    "                    # Record parameters of still incomplete futures for later review\n",
    "                    failed_model_params.extend(future_to_params[future] for future in not_done)\n",
    "\n",
    "                    # Clear the lists for the next batch regardless of completion status\n",
    "                    train_futures.clear()\n",
    "                    eval_futures.clear()\n",
    "\n",
    "    # After all loops have finished running...\n",
    "    if len(train_futures) > 0 or len(eval_futures) > 0:\n",
    "        retry_processing(train_futures, eval_futures, TIMEOUT)\n",
    "\n",
    "\n",
    "    # Now give one more chance with extended timeout only to those that were incomplete previously\n",
    "    if len(failed_model_params) > 0:\n",
    "        print(\"Retrying incomplete models with extended timeout...\")\n",
    "        \n",
    "        # Create new lists for retrying futures\n",
    "        retry_train_futures = []\n",
    "        retry_eval_futures = []\n",
    "\n",
    "        # Resubmit tasks only for those that failed in the first attempt\n",
    "        for params in failed_model_params:\n",
    "            n_topics, alpha_value, beta_value = params\n",
    "            \n",
    "            future_train_retry = client.submit(train_model, n_topics, alpha_value, beta_value,\n",
    "                                            scattered_train_data_future)\n",
    "            future_eval_retry = client.submit(train_model, n_topics, alpha_value, beta_value,\n",
    "                                            scattered_eval_data_future)\n",
    "\n",
    "            retry_train_futures.append(future_train_retry)\n",
    "            retry_eval_futures.append(future_eval_retry)\n",
    "\n",
    "            # Keep track of these new futures as well\n",
    "            future_to_params[future_train_retry] = params\n",
    "            future_to_params[future_eval_retry] = params\n",
    "\n",
    "        # Clear the list of failed model parameters before reattempting\n",
    "        failed_model_params.clear()\n",
    "\n",
    "        # Wait for all reattempted futures with an extended timeout (e.g., 120 seconds)\n",
    "        done, not_done = dask.wait(retry_train_futures + retry_eval_futures, timeout=EXTENDED_TIMEOUT)\n",
    "\n",
    "        # Process completed ones after reattempting\n",
    "        process_completed_futures([f for f in done if f in retry_train_futures],\n",
    "                                [f for f in done if f in retry_eval_futures])\n",
    "        \n",
    "        progress_bar.update(len(done))\n",
    "\n",
    "        # Record parameters of still incomplete futures after reattempting for later review\n",
    "        for future in not_done:\n",
    "            failed_model_params.append(future_to_params[future])\n",
    "\n",
    "        # At this point `failed_model_params` contains the parameters of all models that didn't complete even after a retry\n",
    "\n",
    "    progress_bar.close()\n",
    "    client.close()\n",
    "\n",
    "    # You can now review `failed_model_params` to see which models did not complete successfully.\n",
    "    print(\"The following model parameters did not complete even after a second attempt:\")\n",
    "    perf_logger.info(\"The following model parameters did not complete even after a second attempt:\")\n",
    "    for params in failed_model_params:\n",
    "        print(params)\n",
    "        perf_logger.info(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is part of a larger program that appears to be running machine learning model training and evaluation tasks in parallel using Dask, a flexible parallel computing library for analytic computing. The code manages the execution of these tasks, handling retries for incomplete tasks, and tracking failures.\n",
    "\n",
    "The script begins by initializing an empty list called failed_model_params to store the parameters of models that fail to complete even after a retry. It also creates a dictionary named future_to_params to map \"futures\" (a representation of an asynchronous execution) to their corresponding model parameters.\n",
    "\n",
    "Two functions are defined: process_completed_futures, which processes completed futures, and retry_processing, which attempts to reprocess incomplete futures with an extended timeout period.\n",
    "\n",
    "The main part of the script sets up multiple training and evaluation tasks across different combinations of hyperparameters (n_topics, alpha_value, and beta_value). These tasks are submitted to a Dask client asynchronously using the client.submit method. Each task returns a future, which is then mapped to its parameters in the future_to_params dictionary for later reference.\n",
    "\n",
    "The script uses batch processing controlled by a variable called BATCH_SIZE. Once enough futures have been accumulated, or when all loops have finished running, it waits for all futures within each batch to complete using Dask's wait function with a specified timeout (TIMEOUT). Completed futures are processed while those that remain incomplete are recorded in the failed_model_params list for further action.\n",
    "\n",
    "After processing each batch, if there are any remaining futures (either from incomplete batches or from the final iteration), they are retried using the previously defined retry_processing function with the same timeout value.\n",
    "\n",
    "If there are still models that failed after this first attempt, they get one more chance. The script prints out a message indicating it will retry these incomplete models with an extended timeout (EXTENDED_TIMEOUT). It resubmits these tasks and waits again for completion. Any models that remain incomplete after this second attempt are added back into the failed_model_params.\n",
    "\n",
    "Finally, once all retries have been exhausted and progress has been tracked via a progress bar (tqdm), the Dask client is closed. The script prints out and logs information about any model parameters that did not complete successfully even after two attempts.\n",
    "\n",
    "In summary, this code automates the process of submitting parallelized machine learning training and evaluation jobs over various hyperparameter combinations, handles timeouts by retrying incomplete jobs,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
