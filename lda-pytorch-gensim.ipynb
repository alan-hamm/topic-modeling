{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using CDC AI Chatbot. A variety of prompts were used, including questions and prompts to \n",
    "    correct bugs, memory issues(ie too little resources available), generate comments, etc.\n",
    "\n",
    "maintenance: alan hamm(pqn7)\n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Converts text documents into numerical representations\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # Implements Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import pickle  # Allows objects to be serialized and deserialized to/from disk\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time  # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "#from scipy.sparse.linalg import triu\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "import dask\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster #, LocalCUDACluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.bag as db\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import cupy as cp\n",
    "import webbrowser\n",
    "from torchtext.vocab import GloVe\n",
    "from gensim.models import KeyedVectors\n",
    "import torchtext.vocab as vocab\n",
    "import logging\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'square() method' was deprecated in Bokeh 3.4.0 and will be removed, use \"scatter(marker='square', ...) instead\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics for LDA and step size\n",
    "start_topics = 74\n",
    "end_topics = 102\n",
    "step_size = 2\n",
    "\n",
    "MIN_YEAR = 2010\n",
    "MAX_YEAR = 2020\n",
    "\n",
    "# Specify output directories for log file, model outputs, and images generated.\n",
    "log_dir = \"C:/_harvester/data/lda-models/2010s_html.json/\"\n",
    "model_dir = \"C:/_harvester/data/lda-models/2010s_html.json/lda-models/\"\n",
    "image_dir = \"C:/_harvester/data/lda-models/2010s_html.json/visuals/\"\n",
    "\n",
    "# Create directories if they don't exist.\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        # Get the properties of each GPU device\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        \n",
    "        print(f\"\\nGPU Device {i} Properties:\")\n",
    "        print(f\"Device Name: {gpu_properties.name}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Multiprocessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"CUDA Capability Major Version: {gpu_properties.major}\")\n",
    "        print(f\"CUDA Capability Minor Version: {gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# verify if CUDA is being used or the CPU\n",
    "if device is not None:\n",
    "    # Check if PyTorch is currently using the GPU\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        print(\"PyTorch is using the GPU.\")\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(\"CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"PyTorch is using the CPU.\")\n",
    "else:\n",
    "    print(\"The device is neither using the GPU nor CPU. An error has ocurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "alpha_values += ['symmetric', 'asymmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "beta_values += ['symmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset as a list of a list of tokenized sentences or load data from a file\n",
    "def get_texts_out(year):\n",
    "    year = int(year)\n",
    "    with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.pkl\", \"rb\") as fp:\n",
    "        texts_out = pickle.load(fp)\n",
    "\n",
    "    print(f\"This is the get_texts_out() function. The size of the return is {len(texts_out)}\")\n",
    "    return texts_out\n",
    "\n",
    "#pp.pprint(get_texts_out(2010))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "def coherence_score(X: List[List[str]], n_topics: int, metric: str = 'c_v', vectorizer: Optional[str] = None, glove: Optional[GloVe] = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute the coherence score for a given set of topics and documents.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of documents.\n",
    "        topics (list): List of topic assignments for each document.\n",
    "        metric (str, optional): Coherence metric to use. Defaults to 'c_v'.\n",
    "        vectorizer (str, optional): Vectorizer to use. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: Coherence score.\n",
    "\n",
    "    \"\"\"\n",
    "    if vectorizer == 'glove':\n",
    "        # Load pre-trained GloVe embeddings\n",
    "        # load the scattered embedding vectors from across Dask workers\n",
    "        #glove = GloVe(vectors=embedding_vectors)\n",
    "\n",
    "        # Move the embeddings to the GPU device if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if device.type == \"cuda\":\n",
    "            print(\"CUDA is being used by GloVe.\")\n",
    "            print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "            print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "            \n",
    "        else:\n",
    "            print(\"CUDA is not being used by GloVe. Using CPU instead.\")\n",
    "\n",
    "        # Convert X to a list of documents\n",
    "        documents = [list(doc) for doc in X]\n",
    "\n",
    "        # Convert documents into numerical representations using GloVe\n",
    "        document_vectors = []\n",
    "        \n",
    "        batch_size = 1000  # Set the batch size\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            doc_vectors = [[glove[word] for word in doc] for doc in batch_docs]\n",
    "            document_vectors.extend(doc_vectors)\n",
    "        \n",
    "        X_gpu = []\n",
    "        \n",
    "        num_vectors_to_print = 5  # Number of vectors to print\n",
    "        \n",
    "        for doc_vecs in document_vectors:\n",
    "            doc_gpu = [vec.to(device) for vec in doc_vecs]\n",
    "            X_gpu.append(doc_gpu)\n",
    "            \n",
    "            if num_vectors_to_print > 0:\n",
    "                # Verify if tensors are on GPU and print their devices\n",
    "                for vec in doc_gpu:\n",
    "                    print(\"The vector is on the GPU:\", vec.device)\n",
    "                    num_vectors_to_print -= 1\n",
    "    \n",
    "    else:\n",
    "        print(\"Vectorizer is not set to 'glove'.\")\n",
    "\n",
    "\n",
    "    # Create a dictionary and corpus from the documents\n",
    "    dictionary = Dictionary(X)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in X]\n",
    "\n",
    "    # Create a topic model using the given topics\n",
    "    topic_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, random_state=42)\n",
    "\n",
    "    # Compute the coherence score using the CoherenceModel\n",
    "    coherence_model = CoherenceModel(model=topic_model, texts=X, dictionary=dictionary, coherence=metric)\n",
    "\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def check_port_in_use(port):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(1)  # Set a timeout for the connection attempt\n",
    "    try:\n",
    "        sock.connect(('localhost', port))  # Connect to the specified port\n",
    "        sock.close()  # Close the socket connection\n",
    "        return True  # Port is in use\n",
    "    except ConnectionRefusedError:\n",
    "        return False  # Port is not in use or closed\n",
    "\n",
    "def close_port(port):\n",
    "    if check_port_in_use(port):\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(1)  # Set a timeout for the connection attempt\n",
    "        try:\n",
    "            sock.connect(('localhost', port))  # Connect to the specified port\n",
    "            sock.close()  # Close the socket connection\n",
    "            print(f\"Port {port} is now closed.\")\n",
    "        except ConnectionRefusedError:\n",
    "            print(f\"Port {port} could not be closed.\")\n",
    "    else:\n",
    "        print(f\"Port {port} is already closed or not in use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "def create_lda_batches(filename, batch_size):\n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    \n",
    "    # Create a dictionary from the data\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    \n",
    "    num_batches = len(data) // batch_size\n",
    "    remainder = len(data) % batch_size\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = data[start_idx:end_idx]\n",
    "        \n",
    "        # Convert the text data into bag-of-words representation using the dictionary\n",
    "        batch_bow = [dictionary.doc2bow(text) for text in batch_data]\n",
    "        \n",
    "        batches.append(batch_bow)\n",
    "    \n",
    "    if remainder > 0:\n",
    "        last_batch_data = data[-remainder:]\n",
    "        \n",
    "        # Convert the text data into bag-of-words representation using the dictionary\n",
    "        last_batch_bow = [dictionary.doc2bow(text) for text in last_batch_data]\n",
    "        \n",
    "        batches.append(last_batch_bow)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_sentences(batches):\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for batch in batches:\n",
    "        batch_sentences = []\n",
    "        \n",
    "        for doc in batch:\n",
    "            sentence_tokens = [token[0] for token in doc]\n",
    "            batch_sentences.append(sentence_tokens)\n",
    "        \n",
    "        tokenized_sentences.append(batch_sentences)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.delayed\n",
    "import logging\n",
    "#logging.basicConfig(filename=f\"C:/_harvester/data/lda-models/2010s_html.json/logs/model_callback.log\",\n",
    "#                                format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "#                                level=logging.NOTSET)\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    # Create a multiprocessing context using the \"spawn\" method\n",
    "    # This method is recommended for certain platforms, such as Windows or Jupyter Notebook, to avoid conflicts\n",
    "    #ctx = multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "    # Create a Pool of worker processes using the multiprocessing context\n",
    "    # The number of worker processes is cores - 1\n",
    "    # This ensures that one CPU core is left available for other tasks or system operations\n",
    "    #pool = ctx.Pool(cores - 1)\n",
    "\n",
    "    try:\n",
    "        # Check if the Dask client is connected to a scheduler\n",
    "        if client.status == \"running\":\n",
    "            # Close the Dask client\n",
    "            client.close()\n",
    "            print(\"Dask client closed preemptively.\")\n",
    "        else:\n",
    "            print(\"Dask client is not connected to a scheduler.\")\n",
    "    except Exception as e:\n",
    "        print(f\"The Dask client was not connected: {e}\")\n",
    "\n",
    "    # Load the saved embedding vectors from TorchText GloVe library\n",
    "    glove = vocab.Vectors('glove.840B.300d.txt', 'C:/_harvester/GloVe/')\n",
    "\n",
    "    # Get the embedding vectors and vocabulary from TorchText GloVe library\n",
    "    embedding_vectors = glove.vectors\n",
    "\n",
    "    # Move the embeddings to the GPU device if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    embedding_vectors = embedding_vectors.to(device)\n",
    "\n",
    "    # Verify if CUDA is being used by checking the device type\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"CUDA is being used by GloVe.\")\n",
    "    else:\n",
    "        print(\"CUDA is not being used by GloVe. Using CPU instead.\")\n",
    "\n",
    "    # Convert embedding vectors to a NumPy array (on CPU)\n",
    "    embedding_array = embedding_vectors.cpu().numpy()\n",
    "\n",
    "    # Dictionary to hold the metrics that are generated\n",
    "    metrics_csv = {\n",
    "        'n_topics': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'cv_score': [],\n",
    "        'convergence_score': [],\n",
    "        'log_perplexity': [],\n",
    "        'time_to_complete': []\n",
    "    }\n",
    "\n",
    "    # close the port if it's open\n",
    "    #close_port(8787)\n",
    "    \n",
    "    # Specify the local directory path\n",
    "    DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "    # specify Dask dashboard port\n",
    "    #DASHBOARD_PORT = \"60481\"\n",
    "    \"\"\"\n",
    "    # Set the GPU memory limit\n",
    "    gpu_memory_limit = \"10GB\"\n",
    "    # Set the CUDA_VISIBLE_DEVICES environment variable to specify which GPUs to use\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Specify GPU device IDs\n",
    "    # Create a Dask local cluster with the specified local directory and GPU memory limit\n",
    "    #cluster = LocalCluster(local_directory=DASK_DIR, device_memory_limit=gpu_memory_limit)\n",
    "    cluster = LocalCluster(local_directory=DASK_DIR)\n",
    "    client = Client(cluster)\n",
    "    \"\"\"\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    # https://medium.com/@aryan.gupta18/end-to-end-recommender-systems-with-merlin-part-1-89fabe2fa05b\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify GPU device IDs\n",
    "    protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "    num_gpus = 1\n",
    "    NUM_GPUS=[0]\n",
    "    cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer\n",
    "    visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Select devices to place workers\n",
    "    device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "    device_pool_frac = 0.8\n",
    "    part_mem_frac = 0.15\n",
    "\n",
    "    # Manually specify the total device memory size (in bytes)\n",
    "    device_size = 10 * 1024 * 1024 * 1024  # GPU has 12GB but setting at 10GB\n",
    "            \n",
    "    ram_memory_limit = \"75GB\" # Set the RAM memory limit (per worker)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=(multiprocessing.cpu_count()-2),\n",
    "            threads_per_worker=2,\n",
    "            #processes=False,\n",
    "            memory_limit=ram_memory_limit,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=\":8787\",\n",
    "            protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = ram_memory_limit\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # verify that Dask is being used in your code, you can check the following:\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "    #@dask.delayed\n",
    "    def train_model(n_topics, alpha, beta):\n",
    "        #dictionary = Dictionary()  # Create an empty dictionary\n",
    "        combined_corpus = []  # Initialize list to store combined corpus\n",
    "        combined_text = []\n",
    "\n",
    "        passes = 11  # Number of passes\n",
    "\n",
    "        #print(\"We are before the loop.\")\n",
    "        for year in tqdm(range(MIN_YEAR, MAX_YEAR), desc=\"Training LDA models\"):\n",
    "            #print(f\"This is the year value that is extracted from the Range {year}\")\n",
    "            with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.json\", \"r\") as fp:\n",
    "                texts_out =  json.load(fp)\n",
    "\n",
    "            dictionary = Dictionary(texts_out)\n",
    "\n",
    "            if len(dictionary) > 0:\n",
    "                corpus = [dictionary.doc2bow(doc) for doc in texts_out]\n",
    "\n",
    "                #perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "                if year == MIN_YEAR:\n",
    "                    # Enable progress bar during LDA model training\n",
    "                    pbar = tqdm(total=passes, desc=f\"Training LDA model for year {year}\")\n",
    "                    lda_model_gensim = LdaModel(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=n_topics,\n",
    "                                                alpha=alpha,\n",
    "                                                eta=beta,\n",
    "                                                random_state=75,\n",
    "                                                passes=passes,\n",
    "                                                #workers = cores,\n",
    "                                                chunksize=4000,\n",
    "                                                per_word_topics=True,\n",
    "                                                #callbacks=[pbar]\n",
    "                                            )\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    lda_model_gensim.update(corpus)\n",
    "\n",
    "                combined_text += texts_out\n",
    "                dictionary.add_documents(texts_out)  # Update the dictionary with new documents\n",
    "                combined_corpus.extend(corpus)  # Extend the combined corpus with current year's corpus\n",
    "                \n",
    "                # Convert tensors to strings in combined_text\n",
    "                #documents = [[str(w.item()) if isinstance(w, torch.Tensor) else str(w) for w in doc] for doc in combined_text]\n",
    "\n",
    "                # Create a new dictionary using modified documents\n",
    "                dictionary = Dictionary(combined_text)\n",
    "        return lda_model_gensim, combined_corpus, combined_text, dictionary\n",
    "    \n",
    "        \n",
    "    results = []\n",
    "    corpus_output = []\n",
    "\n",
    "    # Calculate the total number of iterations for the progress bar\n",
    "    total_iterations = len(range(start_topics, end_topics + 1, step_size)) * len(alpha_values) * len(beta_values)\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    #progress_bar = tqdm(total=total_iterations, desc=\"Training LDA models\")\n",
    "\n",
    "    print(\"Training LDA models.\")\n",
    "    for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "        for alpha, beta in itertools.product(alpha_values, beta_values):\n",
    "            # Submit train_model function as a task to Dask cluster and get future object\n",
    "            future = dask.delayed(train_model)(n_topics, alpha, beta)\n",
    "            results.append(future)\n",
    "\n",
    "    # Compute lda_models using Dask\n",
    "    with dask.config.set(scheduler='distributed'):\n",
    "        try:\n",
    "            lda_models = dask.compute(*results, progressbar=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\n\")\n",
    "            for result in results:\n",
    "                if not isinstance(result, dask.delayed.Delayed):\n",
    "                    print(\"Invalid element found in results:\", result)\n",
    "\n",
    "    progress_bar = tqdm(total=len(lda_models), desc=\"Calculating metrics\")\n",
    "    for (lda_model_gensim, combined_corpus, combined_text, dictionary), \\\n",
    "            (alpha, beta) in zip(lda_models, itertools.product(alpha_values, beta_values)):\n",
    "        # Compute convergence score\n",
    "        convergence_score = lda_model_gensim.bound(combined_corpus)\n",
    "\n",
    "        # Compute perplexity score\n",
    "        perplexity_score = lda_model_gensim.log_perplexity(combined_corpus)\n",
    "\n",
    "        # Compute coherence score\n",
    "        c_v_score = coherence_score(X=combined_text, n_topics=n_topics,\n",
    "                                    vectorizer='glove', glove=glove)\n",
    "\n",
    "        # Save the best Gensim LDA model\n",
    "        print(\"Saving the Gensim LDA model.\")\n",
    "        best_model_gensim_filename = os.path.join(model_dir, f\"gensim_topics({n_topics})_alpha({alpha})_beta({beta}).model\")\n",
    "        lda_model_gensim.save(best_model_gensim_filename)\n",
    "\n",
    "        # Add metrics to dictionary\n",
    "        metrics_csv['n_topics'].append(n_topics)\n",
    "        metrics_csv['alpha'].append(alpha)\n",
    "        metrics_csv['beta'].append(beta)\n",
    "        metrics_csv['cv_score'].append(c_v_score)\n",
    "        metrics_csv['convergence_score'].append(convergence_score)\n",
    "        metrics_csv['log_perplexity'].append(perplexity_score)\n",
    "\n",
    "        # Log metrics to a file\n",
    "        log_filename_txt = os.path.join(log_dir, \"lda_metrics.txt\")\n",
    "\n",
    "        with open(log_filename_txt, 'a') as log_file:\n",
    "                log_file.write(f\"Number of Topics: {n_topics}  |  \")\n",
    "                log_file.write(f\"Alpha: {alpha}  |  \")\n",
    "                log_file.write(f\"Beta: {beta}  |  \")\n",
    "                log_file.write(f\"Coherence Value (c_v) - Gensim: {c_v_score}  |  \")\n",
    "                log_file.write(f\"Convergence Score - Gensim: {convergence_score}  |  \")\n",
    "                log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}\\n\")\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    pd.DataFrame(metrics_csv).to_pickle('C:/_harvester/data/lda-models/2010s_html.json/2010s-lda_tuning_results.pkl')\n",
    "    pd.DataFrame(metrics_csv).to_csv('C:/_harvester/data/lda-models/2010s_html.json/2010s-lda_tuning_results.csv', index=False)   \n",
    "\n",
    "    # Close the Dask client and cluster when done\n",
    "    client.close()\n",
    "    #cluster.close(timeout=60)\n",
    "    cluster.close()\n",
    "    logging.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Dask client and cluster when done\n",
    "client.close()\n",
    "cluster.close(timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(topic_word_distributions, feature_names, n_topics):\n",
    "    \"\"\"\n",
    "    Generates a t-SNE plot for the given topic-word distributions.\n",
    "    \n",
    "    Args:\n",
    "        topic_word_distributions (ndarray): Topic-word distributions from LDA model.\n",
    "        feature_names (list): List of feature names from CountVectorizer.\n",
    "        n_topics (int): Number of topics in LDA model.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(topic_word_distributions.T)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i in range(n_topics):\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=f\"Topic {i+1}\")\n",
    "        \n",
    "        for j, txt in enumerate(feature_names):\n",
    "            plt.annotate(txt, (tsne_results[j, 0], tsne_results[j, 1]))\n",
    "            \n",
    "    plt.title(\"t-SNE Plot of Topic-Word Distributions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(topic_distribution, feature_names, topic_idx):\n",
    "   \"\"\"\n",
    "   Generates a word cloud based on the given topic distribution and feature names.\n",
    "\n",
    "   Args:\n",
    "       topic_distribution (ndarray): Topic distribution from LDA model.\n",
    "       feature_names (list): List of feature names from CountVectorizer.\n",
    "       topic_idx (int): Index of the topic.\n",
    "   \"\"\"\n",
    "   # Create a dictionary of words and their corresponding weights in the topic distribution\n",
    "   word_weights = {feature_names[i]: weight for i, weight in enumerate(topic_distribution)}\n",
    "\n",
    "   # Generate word cloud visualization\n",
    "   wc = WordCloud(background_color='white')\n",
    "   wc.generate_from_frequencies(word_weights)\n",
    "\n",
    "   # Plot the word cloud\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   plt.imshow(wc, interpolation='bilinear')\n",
    "   plt.axis('off')\n",
    "   plt.title(f\"Word Cloud for Topic {topic_idx + 1}\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def compare_models_sklearn_gensim(sklearn_models, gensim_models, data):\n",
    "    \"\"\"\n",
    "    Compares scikit-learn's LatentDirichletAllocation (LDA) models with gensim's LdaModel.\n",
    "    \n",
    "    Args:\n",
    "        sklearn_models (list): List of scikit-learn LDA models.\n",
    "        gensim_models (list): List of gensim LdaModel.\n",
    "        data (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Convert tokenized sentences to text documents by joining tokens with space separator\n",
    "    documents = [' '.join(tokens) for tokens in data]\n",
    "\n",
    "    # Convert text data to numerical representation using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Create a PyTorch tensor from the sparse matrix and move it to the device\n",
    "    X_tensor = torch.from_numpy(X.toarray()).float()\n",
    "\n",
    "    # Create a Gensim Dictionary from the tokenized sentences\n",
    "    dictionary = Dictionary(data)\n",
    "    \n",
    "    for i, (sk_model, gs_model) in enumerate(zip(sklearn_models, gensim_models)):\n",
    "        print(f\"Comparison for Model {i+1}:\")\n",
    "        \n",
    "        # Compare coherence values using Gensim's CoherenceModel\n",
    "        coherence_sk = sk_model.score(X)\n",
    "        \n",
    "        pbar = tqdm(total=len(data), desc=\"Calculating Coherence Value - Gensim\")\n",
    "        coherence_gs = 0\n",
    "        \n",
    "        for doc in data:\n",
    "            bow = dictionary.doc2bow(doc)\n",
    "            coherence_gs += gs_model.log_perplexity([bow])\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        coherence_gs /= len(data)\n",
    "        \n",
    "        print(f\"Coherence Value - scikit-learn: {coherence_sk}\")\n",
    "        print(f\"Coherence Value - Gensim: {coherence_gs}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "sklearn_models = [lda_model_100_topics, lda_model_200_topics]\n",
    "gensim_models = [lda_gensim_100_topics, lda_gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
