{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using CDC AI Chatbot. A variety of prompts were used, including questions and prompts to \n",
    "    correct bugs, memory issues(ie too little resources available), generate comments, etc.\n",
    "\n",
    "maintenance: alan hamm(pqn7)\n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Converts text documents into numerical representations\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # Implements Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import pickle  # Allows objects to be serialized and deserialized to/from disk\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time  # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "#from scipy.sparse.linalg import triu\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "import dask\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster #, LocalCUDACluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.bag as db\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import cupy as cp\n",
    "import webbrowser\n",
    "from torchtext.vocab import GloVe\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'square() method' was deprecated in Bokeh 3.4.0 and will be removed, use \"scatter(marker='square', ...) instead\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics for LDA and step size\n",
    "start_topics = 74\n",
    "end_topics = 82\n",
    "step_size = 2\n",
    "\n",
    "# Specify output directories for log file, model outputs, and images generated.\n",
    "log_dir = \"C:/_harvester/data/lda-models/2010s_html.json/\"\n",
    "model_dir = \"C:/_harvester/data/lda-models/2010s_html.json/lda-models/\"\n",
    "image_dir = \"C:/_harvester/data/lda-models/2010s_html.json/visuals/\"\n",
    "\n",
    "# Create directories if they don't exist.\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "\n",
      "GPU Device 0 Properties:\n",
      "Device Name: NVIDIA RTX A3000 12GB Laptop GPU\n",
      "Total Memory: 12.00 GB\n",
      "Multiprocessor Count: 32\n",
      "CUDA Capability Major Version: 8\n",
      "CUDA Capability Minor Version: 6\n",
      "PyTorch is using the GPU.\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        # Get the properties of each GPU device\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        \n",
    "        print(f\"\\nGPU Device {i} Properties:\")\n",
    "        print(f\"Device Name: {gpu_properties.name}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Multiprocessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"CUDA Capability Major Version: {gpu_properties.major}\")\n",
    "        print(f\"CUDA Capability Minor Version: {gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# verify if CUDA is being used or the CPU\n",
    "if device is not None:\n",
    "    # Check if PyTorch is currently using the GPU\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        print(\"PyTorch is using the GPU.\")\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(\"CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"PyTorch is using the CPU.\")\n",
    "else:\n",
    "    print(\"The device is neither using the GPU nor CPU. An error has ocurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_threshold_values = np.arange(0.001, 0.011, 0.001).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset as a list of a list of tokenized sentences or load data from a file\n",
    "def get_texts_out(year):\n",
    "    year = int(year)\n",
    "    with open(f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.pkl\", \"rb\") as fp:\n",
    "        texts_out = pickle.load(fp)\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "#pp.pprint(get_texts_out(2010))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "def coherence_score(X: List[List[str]], topics: List[int], metric: str = 'c_v', vectorizer: Optional[str] = None, glove: Optional[GloVe] = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute the coherence score for a given set of topics and documents.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of documents.\n",
    "        topics (list): List of topic assignments for each document.\n",
    "        metric (str, optional): Coherence metric to use. Defaults to 'c_v'.\n",
    "        vectorizer (str, optional): Vectorizer to use. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: Coherence score.\n",
    "\n",
    "    \"\"\"\n",
    "    if vectorizer == 'glove':\n",
    "        # Load pre-trained GloVe embeddings\n",
    "        # load the scattered embedding vectors from across Dask workers\n",
    "        #glove = GloVe(vectors=embedding_vectors)\n",
    "\n",
    "        # Move the embeddings to the GPU device\n",
    "        #glove.vectors = glove.vectors.to(device)\n",
    "\n",
    "        # Convert X to a list of documents\n",
    "        documents = [list(doc) for doc in X]\n",
    "\n",
    "        # Convert documents into numerical representations using GloVe\n",
    "        document_vectors = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_vector = [glove[word] for word in doc]\n",
    "            document_vectors.append(doc_vector)\n",
    "        \n",
    "        X = document_vectors\n",
    "\n",
    "    # Create a dictionary and corpus from the documents\n",
    "    dictionary = Dictionary(X)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in X]\n",
    "\n",
    "    # Create a topic model using the given topics\n",
    "    topic_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=len(set(topics)), random_state=42)\n",
    "\n",
    "    # Compute the coherence score using the CoherenceModel\n",
    "    coherence_model = CoherenceModel(model=topic_model, texts=X, dictionary=dictionary, coherence=metric)\n",
    "\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is being used by GloVe.\n",
      "Dask client is connected to a scheduler.\n",
      "Embedded vectors are scattered across Dask workers.\n",
      "Dask workers are running.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d45c0f1f0244c3683a8bb7ced03a064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LDA model with 74 topics.:   0%|          | 0/21681 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sentences for year 2010. (1/10 remaining.)\n",
      "Vectorizing sentences for year 2011. (2/10 remaining.)\n",
      "Vectorizing sentences for year 2012. (3/10 remaining.)\n",
      "Vectorizing sentences for year 2013. (4/10 remaining.)\n",
      "Vectorizing sentences for year 2014. (5/10 remaining.)\n",
      "Vectorizing sentences for year 2015. (6/10 remaining.)\n",
      "Vectorizing sentences for year 2016. (7/10 remaining.)\n",
      "Vectorizing sentences for year 2017. (8/10 remaining.)\n",
      "Vectorizing sentences for year 2018. (9/10 remaining.)\n",
      "Vectorizing sentences for year 2019. (10/10 remaining.)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Create a multiprocessing context using the \"spawn\" method\n",
    "    # This method is recommended for certain platforms, such as Windows or Jupyter Notebook, to avoid conflicts\n",
    "    ctx = multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "    # Create a Pool of worker processes using the multiprocessing context\n",
    "    # The number of worker processes is cores - 1\n",
    "    # This ensures that one CPU core is left available for other tasks or system operations\n",
    "    pool = ctx.Pool(cores - 1)\n",
    "\n",
    "    # check to see if GloVe embeddings are saved\n",
    "    #def is_folder_empty(folder_path):\n",
    "    #    return len(os.listdir(folder_path)) == 0\n",
    "\n",
    "    # initialize Dask cluster to None\n",
    "    cluster = None\n",
    "\n",
    "    # flag variable to control how embedding is scattered across Dask\n",
    "    DASK_RUNNING = False\n",
    "\n",
    "    # Load the saved embedding vectors\n",
    "    # https://nlp.stanford.edu/projects/glove/\n",
    "    import torchtext.vocab as vocab\n",
    "    glove = vocab.Vectors('glove.840B.300d.txt', 'C:/_harvester/GloVe/')\n",
    "    # Get the embedding vectors and vocabulary\n",
    "    embedding_vectors_ = glove.vectors\n",
    "    #vocabulary = glove.stoi\n",
    "    # Move the embeddings to the GPU device\n",
    "    glove.vectors = glove.vectors.to(device)\n",
    "\n",
    "    # Verify if CUDA is being used by checking the device type\n",
    "    if glove.vectors.device.type == \"cuda\":\n",
    "        print(\"CUDA is being used by GloVe.\")\n",
    "    else:\n",
    "        print(\"CUDA is not being used by GloVe. Using CPU instead.\")\n",
    "\n",
    "    # Specify the local directory path\n",
    "    DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "    # specify Dask dashboard port\n",
    "    #DASHBOARD_PORT = \"60481\"\n",
    "    \"\"\"\n",
    "    # Set the GPU memory limit\n",
    "    gpu_memory_limit = \"10GB\"\n",
    "    # Set the CUDA_VISIBLE_DEVICES environment variable to specify which GPUs to use\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Specify GPU device IDs\n",
    "    # Create a Dask local cluster with the specified local directory and GPU memory limit\n",
    "    #cluster = LocalCluster(local_directory=DASK_DIR, device_memory_limit=gpu_memory_limit)\n",
    "    cluster = LocalCluster(local_directory=DASK_DIR)\n",
    "    client = Client(cluster)\n",
    "    \"\"\"\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    # https://medium.com/@aryan.gupta18/end-to-end-recommender-systems-with-merlin-part-1-89fabe2fa05b\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify GPU device IDs\n",
    "    protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "    num_gpus = 1\n",
    "    NUM_GPUS=[0]\n",
    "    cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer\n",
    "    visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Select devices to place workers\n",
    "    device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "    device_pool_frac = 0.8\n",
    "    part_mem_frac = 0.15\n",
    "\n",
    "    # Manually specify the total device memory size (in bytes)\n",
    "    device_size = 10 * 1024 * 1024 * 1024  # GPU has 12GB but setting at 10GB\n",
    "            \n",
    "    ram_memory_limit = \"75GB\" # Set the RAM memory limit (per worker)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "    if cluster is None:\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=cores,\n",
    "            threads_per_worker=1,\n",
    "            #processes=False,\n",
    "            memory_limit=ram_memory_limit,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=\":8787\",\n",
    "            protocol=\"tcp\",\n",
    "        )\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = ram_memory_limit\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # verify that Dask is being used in your code, you can check the following:\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "        glove = client.scatter(embedding_vectors_)\n",
    "        print(\"Embedded vectors are scattered across Dask workers.\")\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "    # dictionary to hold the metrics that are generated\n",
    "    metrics_csv = {\n",
    "            'n_topics': [],\n",
    "            'alpha': [],\n",
    "            'beta': [],\n",
    "            'gamma': [],\n",
    "            'median_cv': [],\n",
    "            'convergence_score': [],\n",
    "            'log_perplexity': [],\n",
    "            'time_to_complete': []\n",
    "            }\n",
    "    \n",
    "    for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "        ###########################\n",
    "        #   MODEL WITH GENSIM     #\n",
    "        ###########################\n",
    "        for alpha, beta, gamma_threshold in itertools.product(alpha_values, beta_values, gamma_threshold_values):\n",
    "            \n",
    "            # initialize timer. this is used to output to the metrics_csv dictionary.\n",
    "            started = time()\n",
    "\n",
    "            dictionary = Dictionary(get_texts_out(2010))\n",
    "            corpus10 = [dictionary.doc2bow(doc) for doc in get_texts_out(2010)]\n",
    "\n",
    "            passes = 11  # Number of passes\n",
    "            total_iterations = passes * int(len(corpus10) / cores + 1)\n",
    "\n",
    "            with tqdm(total=total_iterations, desc=f\"Training LDA model with {n_topics} topics.\") as pbar:\n",
    "                lda_model_gensim = LdaMulticore(corpus=corpus10, \n",
    "                                                id2word=dictionary, \n",
    "                                                num_topics=n_topics,\n",
    "                                                alpha=alpha, \n",
    "                                                eta=beta, \n",
    "                                                random_state=75,\n",
    "                                                passes=passes,\n",
    "                                                workers=cores,\n",
    "                                                chunksize=int(len(corpus10)/cores+1),\n",
    "                                                gamma_threshold=gamma_threshold, \n",
    "                                                per_word_topics=True)\n",
    "                \n",
    "                for i in range(passes):\n",
    "                    lda_model_gensim.update(corpus10)\n",
    "                    pbar.update(int(len(corpus10) / cores + 1))\n",
    "\n",
    "            def get_texts(year):\n",
    "                # Define your implementation for getting texts out based on year\n",
    "                # Load data from disk or any other source as needed\n",
    "                \n",
    "                # Example implementation assuming you have a pickle file:\n",
    "                filename = f\"C:/_harvester/data/tokenized-sentences/10s/{year}-tokenized_sents-w-bigrams.pkl\"\n",
    "                with open(filename, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                \n",
    "                return list(itertools.chain.from_iterable(data))\n",
    "\n",
    "            def process_year(year):\n",
    "                def process_sentence(sentence):\n",
    "                    # Convert tokens to numerical representations (word embeddings)\n",
    "                    numerical_representations = [glove[token] for token in sentence]\n",
    "\n",
    "                    return numerical_representations\n",
    "\n",
    "                corpus_data_year = get_texts(year)  # Get tokenized sentences for one year\n",
    "\n",
    "                processed_sentences_results = (dask.delayed(process_sentence)(sentence) for sentence in corpus_data_year)\n",
    "\n",
    "                # Compute the results within a Dask computation context\n",
    "                corpus = dask.compute(*processed_sentences_results)\n",
    "\n",
    "                return corpus  # Return the generated corpus\n",
    "\n",
    "\n",
    "            # Call process_year function for each year from 2010 to 2019\n",
    "            # https://docs.dask.org/en/latest/delayed-best-practices.html\n",
    "            years = range(2010, 2020)\n",
    "            # Create a progress bar using tqdm\n",
    "            #with tqdm(total=len(years), desc=\"Vectorizing sentences into numerical representations.\") as pbar:\n",
    "            results = []\n",
    "            counter = 0\n",
    "            #    for year in years:\n",
    "            #        result = dask.delayed(process_year)(year)\n",
    "            #        results.append(result)\n",
    "            #        pbar.set_postfix({\"Year\": year, \"Status\": \"Processing\"})\n",
    "            #        pbar.update(1)\n",
    "            for year in years:\n",
    "                result = dask.delayed(process_year)(year)\n",
    "                results.append(result)\n",
    "                counter += 1\n",
    "                print(f\"Vectorizing sentences for year {year}. ({counter}/{len(years)} remaining.)\")\n",
    "\n",
    "            # Pause and wait for computations to complete\n",
    "            corpus_data_list = dask.compute(*results)\n",
    "\n",
    "            # Pause and wait for computations to complete\n",
    "            #client.gather(corpus_data_list)\n",
    "\n",
    "            # Combine individual corpora into one corpus\n",
    "            corpus = [doc for sublist in corpus_data_list for doc in sublist]\n",
    "\n",
    "\n",
    "            #convergence_list = list()\n",
    "            convergence_score = lda_model_gensim.bound(corpus)\n",
    "\n",
    "            #perplexity_list = list()\n",
    "            perplexity_score = lda_model_gensim.log_perplexity(corpus)\n",
    "\n",
    "            # Get topic-word distributions from trained Gensim LDA model\n",
    "            topic_word_distributions_gensim = lda_model_gensim.get_topics()\n",
    "        \n",
    "            #c_v_score_gensim = 0\n",
    "            c_v_scores = []\n",
    "            pbar_coherence = tqdm(total=len(corpus), desc=\"Calculating Coherence Value - Gensim\")\n",
    "            \n",
    "            whole_dict = gensim.corpora.Dictionary(corpus)\n",
    "            for doc in corpus:\n",
    "                bow = dictionary.doc2bow(doc)\n",
    "                c_v_scores.append(coherence_score(X=corpus, topics=lda_model_gensim.get_document_topics(bow), \n",
    "                                                  vectorizer='glove', glove=glove))\n",
    "                pbar_coherence.update(1)\n",
    "                    \n",
    "            pbar_coherence.close()\n",
    "                \n",
    "            #c_v_score_gensim /= len(corpus)\n",
    "            c_v_score_gensim = np.median(c_v_scores)\n",
    "\n",
    "            #print(f\"Median Coherence Value (c_v) - Gensim: {c_v_score_gensim}\")\n",
    "\n",
    "            # Save the best Gensim LDA model\n",
    "            best_model_gensim_filename = os.path.join(model_dir, f\"best_model_gensim_{n_topics}_topics.model\")\n",
    "            lda_model_gensim.save(best_model_gensim_filename)\n",
    "\n",
    "            # Generate and save a visualization for the best Gensim LDA model\n",
    "            #vis_data = pyLDAvis.gensim.prepare(lda_model_gensim, corpus, whole_dict)\n",
    "            #vis_html_filename = os.path.join(image_dir, f\"lda_visualization_{n_topics}_topics.html\")\n",
    "            #pyLDAvis.save_html(vis_data, vis_html_filename)\n",
    "\n",
    "            # calculate time to complete this run\n",
    "            time_to_complete = round((time() - started) / 60, 2)\n",
    "\n",
    "            # add metrics to dictionary\n",
    "            metrics_csv['n_topics'].append(n_topics)\n",
    "            metrics_csv['alpha'].append(alpha)\n",
    "            metrics_csv['beta'].append(beta)\n",
    "            metrics_csv['gamma'].append(gamma_threshold)\n",
    "            metrics_csv['median_cv'].append(c_v_score_gensim)\n",
    "            metrics_csv['convergence_score'].append(convergence_score)\n",
    "            metrics_csv['log_perplexity'].append(perplexity_score)\n",
    "            metrics_csv['time_to_complete'].append(time_to_complete)\n",
    "\n",
    "            # Log metrics to a file\n",
    "            log_filename_txt = os.path.join(log_dir, \"lda_metrics.txt\")\n",
    "\n",
    "            with open(log_filename_txt, 'a') as log_file:\n",
    "                log_file.write(f\"Number of Topics: {n_topics}  |  \")\n",
    "                log_file.write(f\"Alpha: {alpha}  |  \")\n",
    "                log_file.write(f\"Beta: {beta}  |  \")\n",
    "                log_file.write(f\"Gamma Threshold: {gamma_threshold}  |  \")\n",
    "                log_file.write(f\"Median Coherence Value (c_v) - Gensim: {c_v_score_gensim}  |  \")\n",
    "                log_file.write(f\"Convergence Score - Gensim: {convergence_score}  |  \")\n",
    "                log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}  |  \")\n",
    "                log_file.write(f\"Time to Complete: {time_to_complete}\\n\")\n",
    "            \n",
    "\n",
    "    pd.DataFrame(metrics_csv).to_pickle('C:/_harvester/data/lda-models/lda-pytorch-2010s/2010s-lda_tuning_results.pkl')\n",
    "    pd.DataFrame(metrics_csv).to_csv('C:/_harvester/data/lda-models/lda-pytorch-2010s/2010s-lda_tuning_results.csv', index=False)   \n",
    "\n",
    "    # Close the Dask client and cluster when done\n",
    "    client.close()\n",
    "    cluster.close(timeout=60)\n",
    "    cluster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations for each saved LDA model\n",
    "for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "    \n",
    "        # Load the saved LDA model\n",
    "    model_filename = os.path.join(model_dir, f\"lda_model_{n_topics}_topics.pth\")\n",
    "    lda_model = torch.load(model_filename)\n",
    "\n",
    "    # Generate pyLDAvis visualization\n",
    "    vis_data = pyLDAvis.sklearn.prepare(lda_model, X.toarray(), vectorizer)\n",
    "    \n",
    "     # Save pyLDAvis visualization as HTML file\n",
    "     vis_html_filename = os.path.join(model_dir, f\"lda_visualization_{n_topics}_topics.html\")\n",
    "     pyLDAvis.save_html(vis_data, vis_html_filename)\n",
    "\n",
    "     # Generate t-SNE plot for topic-word distributions\n",
    "     tsne_plot(lda_model.components_, vectorizer.get_feature_names(), n_topics)\n",
    "\n",
    "     # Generate word cloud for each topic\n",
    "     for topic_idx in range(n_topics):\n",
    "         generate_word_cloud(lda_model.components_[topic_idx], vectorizer.get_feature_names(), topic_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(topic_word_distributions, feature_names, n_topics):\n",
    "    \"\"\"\n",
    "    Generates a t-SNE plot for the given topic-word distributions.\n",
    "    \n",
    "    Args:\n",
    "        topic_word_distributions (ndarray): Topic-word distributions from LDA model.\n",
    "        feature_names (list): List of feature names from CountVectorizer.\n",
    "        n_topics (int): Number of topics in LDA model.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(topic_word_distributions.T)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i in range(n_topics):\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=f\"Topic {i+1}\")\n",
    "        \n",
    "        for j, txt in enumerate(feature_names):\n",
    "            plt.annotate(txt, (tsne_results[j, 0], tsne_results[j, 1]))\n",
    "            \n",
    "    plt.title(\"t-SNE Plot of Topic-Word Distributions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(topic_distribution, feature_names, topic_idx):\n",
    "   \"\"\"\n",
    "   Generates a word cloud based on the given topic distribution and feature names.\n",
    "\n",
    "   Args:\n",
    "       topic_distribution (ndarray): Topic distribution from LDA model.\n",
    "       feature_names (list): List of feature names from CountVectorizer.\n",
    "       topic_idx (int): Index of the topic.\n",
    "   \"\"\"\n",
    "   # Create a dictionary of words and their corresponding weights in the topic distribution\n",
    "   word_weights = {feature_names[i]: weight for i, weight in enumerate(topic_distribution)}\n",
    "\n",
    "   # Generate word cloud visualization\n",
    "   wc = WordCloud(background_color='white')\n",
    "   wc.generate_from_frequencies(word_weights)\n",
    "\n",
    "   # Plot the word cloud\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   plt.imshow(wc, interpolation='bilinear')\n",
    "   plt.axis('off')\n",
    "   plt.title(f\"Word Cloud for Topic {topic_idx + 1}\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def compare_models_sklearn_gensim(sklearn_models, gensim_models, data):\n",
    "    \"\"\"\n",
    "    Compares scikit-learn's LatentDirichletAllocation (LDA) models with gensim's LdaModel.\n",
    "    \n",
    "    Args:\n",
    "        sklearn_models (list): List of scikit-learn LDA models.\n",
    "        gensim_models (list): List of gensim LdaModel.\n",
    "        data (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Convert tokenized sentences to text documents by joining tokens with space separator\n",
    "    documents = [' '.join(tokens) for tokens in data]\n",
    "\n",
    "    # Convert text data to numerical representation using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Create a PyTorch tensor from the sparse matrix and move it to the device\n",
    "    X_tensor = torch.from_numpy(X.toarray()).float()\n",
    "\n",
    "    # Create a Gensim Dictionary from the tokenized sentences\n",
    "    dictionary = Dictionary(data)\n",
    "    \n",
    "    for i, (sk_model, gs_model) in enumerate(zip(sklearn_models, gensim_models)):\n",
    "        print(f\"Comparison for Model {i+1}:\")\n",
    "        \n",
    "        # Compare coherence values using Gensim's CoherenceModel\n",
    "        coherence_sk = sk_model.score(X)\n",
    "        \n",
    "        pbar = tqdm(total=len(data), desc=\"Calculating Coherence Value - Gensim\")\n",
    "        coherence_gs = 0\n",
    "        \n",
    "        for doc in data:\n",
    "            bow = dictionary.doc2bow(doc)\n",
    "            coherence_gs += gs_model.log_perplexity([bow])\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        coherence_gs /= len(data)\n",
    "        \n",
    "        print(f\"Coherence Value - scikit-learn: {coherence_sk}\")\n",
    "        print(f\"Coherence Value - Gensim: {coherence_gs}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "sklearn_models = [lda_model_100_topics, lda_model_200_topics]\n",
    "gensim_models = [lda_gensim_100_topics, lda_gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
