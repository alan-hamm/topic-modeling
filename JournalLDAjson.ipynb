{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    script: JournalLDA.ipynb\n",
    "    @author alan hamm(pqn7)\n",
    "\n",
    "    resources:\n",
    "        Applied Text Analysis with Python by Benjamin Bengfort, Rebecca Bilbro, \n",
    "        and Tony Ojeda(O'Reilly). 978-1-491-96304-3.\n",
    "\n",
    "        https://radimrehurek.com/gensim/auto_examples/howtos/run_compare_lda.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize\n",
    "import en_core_web_lg\n",
    "import gensim\n",
    "from gensim.models import ldamulticore\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "# https://github.com/buriy/python-readability\n",
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "# https://docs.python.org/3/library/time.html\n",
    "import time\n",
    "\n",
    "# https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "import bs4\n",
    "\n",
    "# https://docs.python.org/3/library/codecs.html\n",
    "import codecs\n",
    "\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "import re \n",
    "\n",
    "import os\n",
    "\n",
    "import pprint as pp\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time  # To time our operations\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "#import modin.pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# we create a list to contain the json files that are to be processed\n",
    "\n",
    "#year = 2019\n",
    "#DOC_ID=list()\n",
    "#for x in os.listdir(r\"C:/_harvester/data/html-by-year/10s\"):\n",
    "#    if x.endswith(\".json\") and x[:4] in ['2019']:\n",
    "#        DOC_ID.append(x)\n",
    "#print(DOC_ID)\n",
    "DOC_ID =r'.*([\\d]+_html\\.json)'\n",
    "\n",
    "\n",
    "# we create a list of categories/keywords/tags to\n",
    "#cat_pattern = r'(.*)[\\d]_html\\.json'\n",
    "#cat_pattern = r'(.*?)(\\d{,4}?_html\\.json'\n",
    "#CAT_PATTERN = r'(.*?)\\d{,4}\\.[\\w]+'\n",
    "CAT_PATTERN = r'^(.*?)[\\W]*?\\d{,4}?_html\\.json'\n",
    "\n",
    "\n",
    "# we mark the HTML tags to be used for \n",
    "# extacting the desired article, etc. text\n",
    "# don't include 'li' tag e.g. <li>The Centers for Disease Control and Prevention (CDC) cannot attest to the accuracy of a non-federal website.</li>\n",
    "TAGS = ['p']\n",
    "#TAGS = ['h1']\n",
    "\n",
    "# stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# observed findings \n",
    "stop_words.extend(['icon', 'website', 'mmwr', 'citation', 'author', 'report', 'formatting', \"format\",'regarding',\n",
    "                   'system', 'datum', 'link', 'linking', 'federal', 'data', 'tract', 'census', 'study',\"question\",\n",
    "                   'conduct', 'report', 'including', 'top', 'summary', 'however', 'name', 'known', 'figure', 'return', \n",
    "                   'page', 'view', 'affiliation', 'pdf', 'law', 'version', 'list', 'endorsement', \"review\",\n",
    "                   'article', 'download', 'reference', 'publication', 'discussion', 'table', 'vol', \"message\",\n",
    "                   'information', 'web', 'notification', 'policy', 'policie', #spaCy lemmatization can make errors with pluralization(e.g. rabie for rabies)\n",
    "                   'acknowledgment', 'altmetric',\n",
    "                   'abbreviation', 'figure', \"service\",\"imply\",\"current\",\"source\",\n",
    "                   \"trade\",\"address\", \"addresses\",\"program\",\"organization\" ,\"provided\", \"copyrighted\", \"copyright\",\n",
    "                   \"already\", \"topic\", \"art\", 'e.g', 'eg'])\n",
    "\n",
    "# pretrained model for POS tagging/filtering\n",
    "nlp = en_core_web_lg.load( disable=['parser','ner'])\n",
    "\n",
    "# set encoding for CorpusReader class\n",
    "ENCODING = 'utf8'\n",
    "\n",
    "# SET DIR PATHS\n",
    "JSON_OUT = \"C:/_harvester/data/json-outputs/\"\n",
    "\n",
    "# set the minimum number of topics to find\n",
    "MIN_TOPICS = 100\n",
    "\n",
    "# set the maximum number of topics to find\n",
    "MAX_TOPICS = 505\n",
    "\n",
    "# set the step by value\n",
    "STEP_BY = 2\n",
    "\n",
    "# set value to determine if lemmatization will be performed\n",
    "LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import bs4\n",
    "import re\n",
    "import nltk\n",
    "from time import time\n",
    "\n",
    "class JOURNALCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\" a corpus reader for CDC Journal articles \"\"\"\n",
    "    \n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, encoding=ENCODING, **kwargs):\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "        self.tags = tags\n",
    "\n",
    "    def resolve(self, fileids=None, categories=None):\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        \n",
    "        return fileids\n",
    "\n",
    "    def docs(self,fileids=None, categories=None):\n",
    "        fileids = self.resolve(self.fileids(), self.categories())\n",
    "        \n",
    "        for path, encoding in self.abspaths(self.fileids(), include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                yield json.load(f)\n",
    "\n",
    "    def html(self, fileids=None, categories=None):\n",
    "        for idx, doc in enumerate(self.docs(fileids, categories)):\n",
    "            pp.pprint(f\"The file {self.fileids()[idx]} is being processed in HTML()\")\n",
    "            for sentence in doc:\n",
    "                try:\n",
    "                    yield Paper(sentence).summary()\n",
    "                except Unparseable as e:\n",
    "                    print(\"Could not parse HTML: {}\".format(e))\n",
    "                    print(f\"the fileid {self.fileids()[idx]}\")\n",
    "                    pp.pprint(sentence)\n",
    "                    print(\"\\n\")\n",
    "                    continue\n",
    "\n",
    "    para_dict = dict()       \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        for html in self.html(fileids, categories):\n",
    "            soup=bs4.BeautifulSoup(html,'html.parser')\n",
    "            for element in soup.find_all(TAGS):\n",
    "                if re.search(r'[a-zA-Z]+', element.text):\n",
    "                    yield element.text\n",
    "            soup.decompose()\n",
    "\n",
    "    sent_dict = dict()                \n",
    "    def sents(self,fileids=None,categories=None):\n",
    "        for paragraph in self.paras(fileids,categories):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "                \n",
    "    word_dict = dict() \n",
    "    def words(self,fileids=None,categories=None): \n",
    "        for sentence in self.sents(fileids,categories):\n",
    "            for token in wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "    \n",
    "    def generate(self, fileids=None, categories=None):\n",
    "\n",
    "        # English Articles All Series:\n",
    "        #   2010:142838it, 2011:160805it, 2012:123248it, 2013:137446it, 2014:121235it, 2015:151360it,\n",
    "        #   2016:230592it, 2017:    , 2018:     , 2019: 12642it \n",
    "        para_dict = dict()\n",
    "        for idx, para in tqdm(enumerate(self.paras())):\n",
    "            #pp.pprint(para)\n",
    "            para_dict[idx] = para\n",
    "        \n",
    "        return para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010_html.json',\n",
       " '2011_html.json',\n",
       " '2012_html.json',\n",
       " '2013_html.json',\n",
       " '2014_html.json',\n",
       " '2015_html.json',\n",
       " '2016_html.json',\n",
       " '2017_html.json',\n",
       " '2018_html.json',\n",
       " '2019_html.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_corpus = JOURNALCorpusReader('/_harvester/data/html-by-year/10s')\n",
    "#print(_corpus.categories())\n",
    "_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [00:00, 801.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2010_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "926232it [03:09, 3906.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2011_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1911781it [07:20, 3673.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2012_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2747487it [10:12, 2951.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2013_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3638418it [13:13, 2533.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2014_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4355765it [16:05, 941.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2015_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5291581it [19:28, 425.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2016_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6953824it [25:11, 4140.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2017_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8592480it [31:13, 2965.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2018_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8670894it [31:53, 2523.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The file 2019_html.json is being processed in HTML()'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8686550it [32:22, 4472.56it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_tuple = _corpus.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx, paras in corpus_tuple.items():\n",
    "#    pp.pprint(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# number of stopwords found\u001b[39;00m\n\u001b[0;32m      8\u001b[0m stopword_count \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mFreqDist()\n\u001b[1;32m---> 10\u001b[0m pp\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting POS/LEMMATIZATION for Year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43myear\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m t \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, paras \u001b[38;5;129;01min\u001b[39;00m tqdm(corpus_tuple\u001b[38;5;241m.\u001b[39mitems()):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'year' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import spacy\n",
    "\n",
    "texts_out = []\n",
    "inner_text = []\n",
    "\n",
    "# number of stopwords found\n",
    "stopword_count = nltk.FreqDist()\n",
    "\n",
    "pp.pprint(f\"Executing POS/LEMMATIZATION\")\n",
    "\n",
    "t = time()\n",
    "for key, paras in tqdm(corpus_tuple.items()):\n",
    "    doc = nlp(paras)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            if len(token.text) > 1:\n",
    "                if token.text.lower() not in stop_words and token.lemma_.lower() not in stop_words: \n",
    "                    if LEMMATIZATION == False:\n",
    "                        inner_text.append(token.text) \n",
    "                    else:\n",
    "                        inner_text.append(token.lemma_) \n",
    "                else:\n",
    "                    if LEMMATIZATION == False:\n",
    "                        stopword_count[token.text] += 1\n",
    "                    else:\n",
    "                        stopword_count[token.lemma_] += 1\n",
    "\n",
    "    if len(inner_text) > 0:\n",
    "        texts_out.append(inner_text)\n",
    "    inner_text = []\n",
    "\n",
    "#pp.pprint(texts_out)\n",
    "pp.pprint('Time to finish spaCy filter: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = f'C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-wo-bigrams.json'\n",
    "with open(filename, 'w') as jsonfile:\n",
    "    json.dump(texts_out, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(texts_out, min_count=20)\n",
    "\n",
    "# freqDist object for bigrams\n",
    "bigram_freq = nltk.FreqDist()\n",
    "\n",
    "# print bigrams\n",
    "for ngrams, _ in bigram.vocab.items():\n",
    "    #unicode_ngrams = ngrams.decode('utf-8')\n",
    "    if '_' in ngrams:\n",
    "        bigram_freq[ngrams]+=1\n",
    "        print(ngrams)\n",
    "\n",
    "# add bigrams to texts_out to be included in corpus\n",
    "for idx in range(len(texts_out)):\n",
    "    for token in bigram[texts_out[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_out[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp.pprint(texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_sents = pd.DataFrame(texts_out)\n",
    "#tokenized_sents.to_parquet(r\"C:\\_harvester\\data\\lda-models\\2010s_html.json\\tokenized_sents-w-bigrams.parquet\")\n",
    "#pp.pprint(texts_out)\n",
    "fliename2 = f\"C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-w-bigrams.json\"\n",
    "with open(fliename2, 'w') as jsonfile:\n",
    "    json.dump(texts_out, jsonfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
