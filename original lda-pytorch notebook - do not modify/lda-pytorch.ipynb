{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using CDC AI Chatbot. A variety of prompts were used, including questions and prompts to \n",
    "    correct bugs, memory issues(ie too little resources available), generate comments, etc.\n",
    "\n",
    "maintenance: alan hamm(pqn7)\n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Converts text documents into numerical representations\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # Implements Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import pickle  # Allows objects to be serialized and deserialized to/from disk\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time  # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics for LDA and step size\n",
    "start_topics = 50\n",
    "end_topics = 155\n",
    "step_size = 5\n",
    "\n",
    "# Specify output directories for log file, model outputs, and images generated.\n",
    "log_dir = \"C:/_harvester/data/lda-models/2010s_html.json/\"\n",
    "model_dir = \"C:/_harvester/data/lda-models/2010s_html.json/lda-models/\"\n",
    "image_dir = \"C:/_harvester/data/lda-models/2010s_html.json/visuals/\"\n",
    "\n",
    "# Create directories if they don't exist.\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "\n",
      "GPU Device 0 Properties:\n",
      "Device Name: NVIDIA RTX A3000 12GB Laptop GPU\n",
      "Total Memory: 12.00 GB\n",
      "Multiprocessor Count: 32\n",
      "CUDA Capability Major Version: 8\n",
      "CUDA Capability Minor Version: 6\n",
      "PyTorch is using the GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        # Get the properties of each GPU device\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        \n",
    "        print(f\"\\nGPU Device {i} Properties:\")\n",
    "        print(f\"Device Name: {gpu_properties.name}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Multiprocessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"CUDA Capability Major Version: {gpu_properties.major}\")\n",
    "        print(f\"CUDA Capability Minor Version: {gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# verify if CUDA is being used or the CPU\n",
    "if device is not None:\n",
    "    # Check if PyTorch is currently using the GPU\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        print(\"PyTorch is using the GPU.\")\n",
    "    else:\n",
    "        print(\"PyTorch is using the CPU.\")\n",
    "else:\n",
    "    print(\"The device is neither using the GPU nor CPU. An error has ocurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() - 1 # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_threshold_values = np.arange(0.001, 0.011, 0.001).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset as a list of a list of tokenized sentences or load data from a file\n",
    "with open(r\"C:\\_harvester\\data\\lda-models\\2010s_html.json\\word2vec-2010s\\tokenized-texts-out\\tokenized_sents-w-bigrams.pkl\", \"rb\") as fp:\n",
    "    texts_out = pickle.load(fp)\n",
    "\n",
    "# Convert tokenized sentences to text documents by joining tokens with space separator\n",
    "documents = [' '.join(tokens) for tokens in texts_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Begin the vectorization...'\n",
      "'Vectorization completed in 0.07 minutes.'\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to numerical representation using CountVectorizer\n",
    "pp.pprint(\"Begin the vectorization...\")\n",
    "started = time()\n",
    "\n",
    "# CountVectorizer is a class in scikit-learn used to convert text documents into numerical representations.\n",
    "# It builds a vocabulary of known words from the text data and transforms each document into a sparse matrix\n",
    "# representing the frequency of each word in the document.\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# The fit_transform() method of CountVectorizer performs two steps:\n",
    "# 1. It learns the vocabulary from the input documents and assigns a unique integer index to each word in the vocabulary.\n",
    "# 2. It transforms the input documents into a sparse matrix representation, where each row corresponds to a document,\n",
    "#    and each column represents the count of a specific word in that document.\n",
    "# The resulting X is a sparse matrix (specifically, a scipy.sparse.csr_matrix) where each row represents a document,\n",
    "# and each column represents a unique word in the vocabulary. The values in X indicate the frequency of each word in\n",
    "# the corresponding document.\n",
    "# Note: The fit_transform() method both fits the model to the data (learns vocabulary) and transforms it (creates matrix).\n",
    "X = vectorizer.fit_transform(documents) # Convert text documents into numerical representations using CountVectorizer\n",
    "pp.pprint(f\"Vectorization completed in {round((time() - started) / 60, 2) } minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create COO format sparse tensor from the sparse matrix and move it to the device\n",
    "\n",
    "The following code creates a COO (Coordinate) format sparse tensor from a given sparse matrix, represented by X. \n",
    "This conversion is performed to optimize storage and computation efficiency when working with large-scale data, \n",
    "especially on devices like GPUs that offer parallel processing capabilities.\n",
    "\n",
    "To create the COO format sparse tensor, several steps are involved:\n",
    "1. Extracting the row and column indices of non-zero elements in the sparse matrix X using `nonzero()` method.\n",
    "2. Stacking these row and column indices vertically to create a 2D numpy array where each column represents an index pair.\n",
    "3. Converting this index pairs numpy array into a PyTorch tensor, ensuring that the data type is set as long integer.\n",
    "4. Extracting the non-zero values from the sparse matrix X using `.data` attribute.\n",
    "5. Converting these non-zero values numpy array into a PyTorch tensor, ensuring that the data type is set as float.\n",
    "6. Obtaining the shape of the sparse matrix X as a tuple representing (number of rows, number of columns).\n",
    "7. Creating a `torch.Size` object encapsulating the shape information for creating a COO format sparse tensor.\n",
    "\n",
    "Finally, all these components - COO indices, values, and shape - are used to create a COO format sparse tensor \n",
    "using `torch.sparse_coo_tensor()`. The resulting tensor is then moved to the specified device (e.g., GPU) \n",
    "using `.to(device)` for efficient computation if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Begin creation of sparse tensor...'\n",
      "'Create COO format sparse tensor completed in 0.0 minutes.'\n"
     ]
    }
   ],
   "source": [
    "# Create COO format sparse tensor from the sparse matrix and move it to the device\n",
    "pp.pprint(\"Begin creation of sparse tensor...\")\n",
    "started = time()\n",
    "\n",
    "# The purpose of this code is to create a COO (Coordinate) format sparse tensor from a given sparse matrix, represented by X.\n",
    "# In this specific line, we are extracting the indices of non-zero elements in the sparse matrix X using `nonzero()` method.\n",
    "# The `nonzero()` method returns a tuple of arrays, where each array represents the indices of non-zero elements along a particular dimension.\n",
    "# np.vstack() function vertically stacks these arrays obtained from `nonzero()` to create a 2D array where each column represents the row and \n",
    "# column indices of a non-zero element.\n",
    "# torch.from_numpy() converts this 2D numpy array into a PyTorch tensor. We use `.long()` to ensure that the data type is set as long integer.\n",
    "# Finally, `.to(device)` moves the resulting tensor to the specified device (e.g., GPU) for efficient computation if available.\n",
    "# This conversion to COO format and moving it to the device is often necessary when working with large sparse matrices in deep learning or \n",
    "# other computations.\n",
    "# It allows for efficient storage and computation on devices like GPUs, which can significantly speed up operations involving large-scale data.\n",
    "coo_indices = torch.from_numpy(np.vstack((X.nonzero()[0], X.nonzero()[1]))).long().to(device)\n",
    "\n",
    "\n",
    "# The purpose of this code is to create a COO (Coordinate) format sparse tensor from a given sparse matrix, represented by X, \n",
    "# specifically for the values of non-zero elements.\n",
    "# In this specific line, we are extracting the non-zero values from the sparse matrix X using `.data` attribute.\n",
    "# `X.data` returns an array containing only the non-zero values of X.\n",
    "# torch.from_numpy() converts this numpy array into a PyTorch tensor. We use `.float()` to ensure that the data type is set as float.\n",
    "# Finally, `.to(device)` moves the resulting tensor to the specified device (e.g., GPU) for efficient computation if available.\n",
    "# This conversion to COO format and moving it to the device is often necessary when working with large sparse matrices in deep \n",
    "# learning or other computations.\n",
    "# It allows for efficient storage and computation on devices like GPUs, which can significantly speed up operations involving large-scale data.\n",
    "coo_values = torch.from_numpy(X.data).float().to(device)\n",
    "\n",
    "\n",
    "# The purpose of this code is to create a `torch.Size` object representing the shape of a COO (Coordinate) format sparse tensor.\n",
    "# In this specific line, we are creating a `torch.Size` object using `X.shape`.\n",
    "# `X.shape` returns a tuple representing the shape of the sparse matrix X, where the first element is the number of rows and the second \n",
    "# element is the number of columns.\n",
    "# The resulting `torch.Size` object, `coo_shape`, encapsulates this shape information.\n",
    "# This step is needed when creating a COO format sparse tensor because it requires specifying the shape of the resulting tensor.\n",
    "# By obtaining and storing this shape information in `coo_shape`, we can ensure that our COO format sparse tensor has the correct dimensions.\n",
    "# Having accurate shape information is crucial for performing operations on tensors and ensuring compatibility with other tensors or operations \n",
    "# in subsequent steps.\n",
    "coo_shape = torch.Size(X.shape)\n",
    "\n",
    "\n",
    "# The purpose of this code is to create a COO (Coordinate) format sparse tensor from the given COO indices, values, and shape.\n",
    "# It also moves the resulting tensor to the specified device (e.g., GPU) for efficient computation if available.\n",
    "# In this specific line, we are using `torch.sparse_coo_tensor()` to create a sparse tensor in COO format.\n",
    "# The function takes three arguments:\n",
    "# - `coo_indices`: The indices of non-zero elements in the sparse tensor.\n",
    "# - `coo_values`: The values corresponding to each index in `coo_indices`.\n",
    "# - `coo_shape`: The shape of the resulting sparse tensor.\n",
    "# Finally, `.to(device)` moves the resulting tensor to the specified device (e.g., GPU) for efficient computation if available.\n",
    "# This step is needed when working with large sparse matrices in deep learning or other computations.\n",
    "# Sparse tensors allow us to efficiently represent and operate on matrices that have a significant number of zero elements,\n",
    "# which can be common in many real-world datasets. By creating a COO format sparse tensor and moving it to a device like GPU,\n",
    "# we can perform computations more efficiently and take advantage of parallel processing capabilities offered by GPUs.\n",
    "X_tensor = torch.sparse_coo_tensor(coo_indices, coo_values, coo_shape).to(device)\n",
    "pp.pprint(f\"Create COO format sparse tensor completed in {round((time() - started) / 60, 2) } minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Begin the sklearn LDA modeling...'\n",
      "'The sklearn LDA modelling completed in 0.0 minutes.'\n",
      "'Fit the model on the data...'\n"
     ]
    }
   ],
   "source": [
    "# Create a Gensim Dictionary from the tokenized sentences\n",
    "dictionary = Dictionary(texts_out)\n",
    "\n",
    "for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "    for alpha, beta, gamma_threshold in itertools.product(alpha_values, beta_values, gamma_threshold_values):\n",
    "        # Initialize LDA model using scikit-learn\n",
    "        pp.pprint(\"Begin the sklearn LDA modeling...\")\n",
    "        started = time()\n",
    "        lda_model_sklearn = LatentDirichletAllocation(n_components=n_topics, learning_method='online', n_jobs=-2)\n",
    "        pp.pprint(f\"The sklearn LDA modelling completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "\n",
    "        # Fit the model on your data\n",
    "        pp.pprint(\"Fit the model on the data...\")\n",
    "        started = time()\n",
    "        lda_model_sklearn.fit(X)\n",
    "        # Save the output using pickle\n",
    "        with open(f\"C:/_harvester/data/lda-models/2010s_html.json/sklearn-fit-output/fit-output-{n_topics}.pkl\", 'wb') as file:\n",
    "            pickle.dump(lda_model_sklearn, file)\n",
    "        pp.pprint(f\"Model fitting completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "\n",
    "        # Transfer the trained model to GPU if available\n",
    "        pp.pprint(\"Begin transfer of trained model to GPU if available...\")\n",
    "        if torch.cuda.is_available():\n",
    "            lda_model_torch = lda_model_sklearn.transform(X_tensor.to_dense().to(device))\n",
    "            pp.pprint(\"The trained model was successfully transferred to the GPU\")\n",
    "        else:\n",
    "            lda_model_torch = lda_model_sklearn.transform(X_tensor.to_dense())\n",
    "            pp.pprint(\"The trained model was not transferred to the GPU\")\n",
    "\n",
    "        # Set the topic-word distributions of the scikit-learn LDA model to random values\n",
    "        pp.pprint(\"Begin set the topic-word distributions of the scikit-learn LDA model to random values\")\n",
    "        started = time()\n",
    "        lda_model_sklearn.components_ = torch.randn((n_topics, X.shape[1]), device=device)\n",
    "        pp.pprint(f\"Random value assignment completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "\n",
    "        # Train scikit-learn LDA model with progress bar visualization\n",
    "        #dataloader_sklearn = DataLoader(X_tensor, batch_size=32)\n",
    "        \n",
    "        #for batch in tqdm(dataloader_sklearn, desc=f\"Training scikit-learn LDA with {n_topics} topics\", leave=False):\n",
    "            # Convert batch to sparse tensor\n",
    "        #    batch_sparse = torch.sparse.FloatTensor(batch.indices(), batch.values(), batch.size()).to(device)\n",
    "        #    lda_model_sklearn.partial_fit(batch_sparse)\n",
    "\n",
    "        # Get topic-word distributions from trained scikit-learn LDA model\n",
    "        #pp.pprint(\"Begin get topic-word distributions from trained scikit-learn LDA model\")\n",
    "        #started = time()\n",
    "        #topic_word_distributions_sklearn = lda_model_sklearn.components_\n",
    "        #pp.pprint(f\"Get of topic-word distributions completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "        \n",
    "        # Train scikit-learn LDA model with progress bar visualization\n",
    "        dataloader_sklearn = DataLoader(X_tensor, batch_size=32)\n",
    "        \n",
    "        for batch in tqdm(dataloader_sklearn, desc=f\"Training scikit-learn LDA with {n_topics} topics\", leave=False):\n",
    "            started = time.time()\n",
    "            lda_model_sklearn.partial_fit(batch)\n",
    "            elapsed_time = round(time.time() - started, 2)\n",
    "            pp.pprint(f\"Batch training completed in {elapsed_time} seconds.\")\n",
    "            \n",
    "        # Save the generated scikit-learn LDA model\n",
    "        model_filename = os.path.join(model_dir, f\"lda_model_sklearn_{n_topics}_topics.model\")\n",
    "        started = time()\n",
    "        torch.save(lda_model_sklearn, model_filename)\n",
    "        pp.pprint(f\"Model saving completed in {round(time() - started, 2)} seconds.\")\n",
    "\n",
    "        # Train Gensim LDA model with progress bar visualization\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in texts_out]\n",
    "        \n",
    "        pbar_gensim = tqdm(total=len(corpus), desc=f\"Training Gensim LDA with {n_topics} topics\", leave=False)\n",
    "        \n",
    "        started = time()\n",
    "        lda_model_gensim = LdaModel(corpus=corpus, \n",
    "                                    id2word=dictionary, \n",
    "                                    num_topics=n_topics,\n",
    "                                    alpha=alpha, \n",
    "                                    eta=beta, \n",
    "                                    random_state=75,\n",
    "                                    passes=10,\n",
    "                                    chunksize=int(len(corpus)/cores+1),\n",
    "                                    gamma_threshold=gamma_threshold, \n",
    "                                    per_words_topics=True).to(device)\n",
    "        \n",
    "        for doc in corpus:\n",
    "            lda_model_gensim.update([doc])\n",
    "            pbar_gensim.update(1)\n",
    "\n",
    "        lda_model_gensim.save(f\"C:/_harvester/data/lda-models/2010s_html.json/lda-models/gensim-topics({n_topics}).model\")\n",
    "        pp.pprint(f\"Gensim LDA model for {n_topics} topics completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "\n",
    "        convergence_score = lda_model_gensim.bound(corpus)\n",
    "\n",
    "        perplexity_score = lda_model_gensim.log_perplexity(corpus)\n",
    "\n",
    "        pbar_gensim.close()\n",
    "\n",
    "        # Get topic-word distributions from trained Gensim LDA model\n",
    "        pp.pprint(\"Begin get topic-word distributions from trained Gensim LDA model\")\n",
    "        started = time()\n",
    "        topic_word_distributions_gensim = lda_model_gensim.get_topics()\n",
    "        pp.pprint(f\"Get of topic-word distributions completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "\n",
    "        # Compare metrics and update best model if necessary\n",
    "        pp.pprint(\"Begin comparison of metrics and updating of model if necessary\")\n",
    "        c_v_score_sklearn = coherence_score(X=X.toarray(), topics=topic_word_distributions_sklearn.argmax(axis=1), metric='c_v', vectorizer=vectorizer)\n",
    "        pp.pprint(f\"Metric comparison completed in {round((time() - started) / 60, 2) } minutes.\")\n",
    "    \n",
    "        c_v_score_gensim = 0\n",
    "        pbar_coherence = tqdm(total=len(texts_out), desc=\"Calculating Coherence Value - Gensim\")\n",
    "        \n",
    "        for doc in texts_out:\n",
    "            bow = dictionary.doc2bow(doc)\n",
    "            c_v_score_gensim += coherence_score(topics=lda_model_gensim.get_document_topics(bow), texts=[doc], dictionary=dictionary, coherence='c_v')\n",
    "            pbar_coherence.update(1)\n",
    "            \n",
    "        pbar_coherence.close()\n",
    "            \n",
    "        c_v_score_gensim /= len(texts_out)\n",
    "\n",
    "        print(\"Comparison of scikit-learn LDA and Gensim LDA:\")\n",
    "        print(f\"Coherence Value (c_v) - scikit-learn: {c_v_score_sklearn}\")\n",
    "        print(f\"Coherence Value (c_v) - Gensim: {c_v_score_gensim}\")\n",
    "\n",
    "        # Save the best scikit-learn LDA model\n",
    "        best_model_sklearn_filename = os.path.join(model_dir, f\"best_model_sklearn_{n_topics}_topics.pth\")\n",
    "        torch.save(lda_model_sklearn, best_model_sklearn_filename)\n",
    "\n",
    "        # Save the best Gensim LDA model\n",
    "        best_model_gensim_filename = os.path.join(model_dir, f\"best_model_gensim_{n_topics}_topics.pth\")\n",
    "        lda_model_gensim.save(best_model_gensim_filename)\n",
    "\n",
    "        # Generate and save a visualization for the best Gensim LDA model\n",
    "        vis_data = pyLDAvis.gensim.prepare(lda_model_gensim, corpus, dictionary)\n",
    "        vis_html_filename = os.path.join(image_dir, f\"lda_visualization_{n_topics}_topics.html\")\n",
    "        pyLDAvis.save_html(vis_data, vis_html_filename)\n",
    "\n",
    "        # Log metrics to a file\n",
    "        log_filename = os.path.join(log_dir, \"lda_metrics.log\")\n",
    "\n",
    "        with open(log_filename, 'a') as log_file:\n",
    "            log_file.write(f\"Number of Topics: {n_topics}\\n\")\n",
    "            log_file.write(f\"Alpha: {alpha}\\n\")\n",
    "            log_file.write(f\"Beta: {beta}\\n\")\n",
    "            log_file.write(f\"Gamma Threshold: {gamma_threshold}\\n\")\n",
    "            log_file.write(f\"Coherence Value (c_v) - scikit-learn: {c_v_score_sklearn}\\n\")\n",
    "            log_file.write(f\"Coherence Value (c_v) - Gensim: {c_v_score_gensim}\\n\")\n",
    "            log_file.write(f\"Convergence Score - Gensim: {convergence_score}\\n\")\n",
    "            log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save every generated LDA model\n",
    "pp.pprint(\"Saving all models...\")\n",
    "for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "    \n",
    "    # Initialize LDA model with GPU acceleration using PyTorch tensors\n",
    "    pp.pprint(\"Initialize LDA model with GPU acceleration using PyTorch tensors...\")\n",
    "    started = time.time()\n",
    "    lda_model_sklearn = LatentDirichletAllocation(n_components=n_topics, learning_method='online', n_jobs=-2)\n",
    "    elapsed_time = round(time.time() - started, 2)\n",
    "    pp.pprint(f\"LDA model initialization completed in {elapsed_time} seconds.\")\n",
    "     \n",
    "    # Set the topic-word distributions of the scikit-learn LDA model to random values\n",
    "    lda_model_sklearn.components_ = torch.randn((n_topics, X.shape[1]), device=device)\n",
    "     \n",
    "    # Train scikit-learn LDA model with progress bar visualization\n",
    "    dataloader_sklearn = DataLoader(X_tensor, batch_size=32)\n",
    "     \n",
    "    for batch in tqdm(dataloader_sklearn, desc=f\"Training scikit-learn LDA with {n_topics} topics\", leave=False):\n",
    "        started = time.time()\n",
    "        lda_model_sklearn.partial_fit(batch)\n",
    "        elapsed_time = round(time.time() - started, 2)\n",
    "        pp.pprint(f\"Batch training completed in {elapsed_time} seconds.\")\n",
    "        \n",
    "     # Save the generated scikit-learn LDA model\n",
    "     model_filename = os.path.join(model_dir, f\"lda_model_sklearn_{n_topics}_topics.pth\")\n",
    "     started = time.time()\n",
    "     torch.save(lda_model_sklearn, model_filename)\n",
    "     elapsed_time = round(time.time() - started, 2)\n",
    "     pp.pprint(f\"Model saving completed in {elapsed_time} seconds.\")\n",
    "\n",
    "     # Train Gensim LDA model with progress bar visualization\n",
    "     corpus = [dictionary.doc2bow(doc) for doc in texts_out]\n",
    "\n",
    "     lda_model_gensim = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics,\n",
    "                                alpha=alpha, eta=beta,\n",
    "                                gamma_threshold=_threshold).to(device)\n",
    "\n",
    "     for doc in corpus:\n",
    "        lda_model_gensim.update([doc])\n",
    "        pbar_gensim.update(1)\n",
    "\n",
    "     pbar_gensim.close()\n",
    "\n",
    "     # Save the generated Gensim LDA model\n",
    "     model_filename = os.path.join(model_dir, f\"lda_model_gensim_{n_topics}_topics.model\")\n",
    "     lda_model_gensim.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations for each saved LDA model\n",
    "for n_topics in range(start_topics, end_topics + 1, step_size):\n",
    "    \n",
    "        # Load the saved LDA model\n",
    "    model_filename = os.path.join(model_dir, f\"lda_model_{n_topics}_topics.pth\")\n",
    "    lda_model = torch.load(model_filename)\n",
    "\n",
    "    # Generate pyLDAvis visualization\n",
    "    vis_data = pyLDAvis.sklearn.prepare(lda_model, X.toarray(), vectorizer)\n",
    "    \n",
    "     # Save pyLDAvis visualization as HTML file\n",
    "     vis_html_filename = os.path.join(model_dir, f\"lda_visualization_{n_topics}_topics.html\")\n",
    "     pyLDAvis.save_html(vis_data, vis_html_filename)\n",
    "\n",
    "     # Generate t-SNE plot for topic-word distributions\n",
    "     tsne_plot(lda_model.components_, vectorizer.get_feature_names(), n_topics)\n",
    "\n",
    "     # Generate word cloud for each topic\n",
    "     for topic_idx in range(n_topics):\n",
    "         generate_word_cloud(lda_model.components_[topic_idx], vectorizer.get_feature_names(), topic_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(topic_word_distributions, feature_names, n_topics):\n",
    "    \"\"\"\n",
    "    Generates a t-SNE plot for the given topic-word distributions.\n",
    "    \n",
    "    Args:\n",
    "        topic_word_distributions (ndarray): Topic-word distributions from LDA model.\n",
    "        feature_names (list): List of feature names from CountVectorizer.\n",
    "        n_topics (int): Number of topics in LDA model.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_results = tsne.fit_transform(topic_word_distributions.T)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i in range(n_topics):\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=f\"Topic {i+1}\")\n",
    "        \n",
    "        for j, txt in enumerate(feature_names):\n",
    "            plt.annotate(txt, (tsne_results[j, 0], tsne_results[j, 1]))\n",
    "            \n",
    "    plt.title(\"t-SNE Plot of Topic-Word Distributions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(topic_distribution, feature_names, topic_idx):\n",
    "   \"\"\"\n",
    "   Generates a word cloud based on the given topic distribution and feature names.\n",
    "\n",
    "   Args:\n",
    "       topic_distribution (ndarray): Topic distribution from LDA model.\n",
    "       feature_names (list): List of feature names from CountVectorizer.\n",
    "       topic_idx (int): Index of the topic.\n",
    "   \"\"\"\n",
    "   # Create a dictionary of words and their corresponding weights in the topic distribution\n",
    "   word_weights = {feature_names[i]: weight for i, weight in enumerate(topic_distribution)}\n",
    "\n",
    "   # Generate word cloud visualization\n",
    "   wc = WordCloud(background_color='white')\n",
    "   wc.generate_from_frequencies(word_weights)\n",
    "\n",
    "   # Plot the word cloud\n",
    "   plt.figure(figsize=(8, 6))\n",
    "   plt.imshow(wc, interpolation='bilinear')\n",
    "   plt.axis('off')\n",
    "   plt.title(f\"Word Cloud for Topic {topic_idx + 1}\")\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def compare_models_sklearn_gensim(sklearn_models, gensim_models, data):\n",
    "    \"\"\"\n",
    "    Compares scikit-learn's LatentDirichletAllocation (LDA) models with gensim's LdaModel.\n",
    "    \n",
    "    Args:\n",
    "        sklearn_models (list): List of scikit-learn LDA models.\n",
    "        gensim_models (list): List of gensim LdaModel.\n",
    "        data (list): List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Convert tokenized sentences to text documents by joining tokens with space separator\n",
    "    documents = [' '.join(tokens) for tokens in data]\n",
    "\n",
    "    # Convert text data to numerical representation using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Create a PyTorch tensor from the sparse matrix and move it to the device\n",
    "    X_tensor = torch.from_numpy(X.toarray()).float()\n",
    "\n",
    "    # Create a Gensim Dictionary from the tokenized sentences\n",
    "    dictionary = Dictionary(data)\n",
    "    \n",
    "    for i, (sk_model, gs_model) in enumerate(zip(sklearn_models, gensim_models)):\n",
    "        print(f\"Comparison for Model {i+1}:\")\n",
    "        \n",
    "        # Compare coherence values using Gensim's CoherenceModel\n",
    "        coherence_sk = sk_model.score(X)\n",
    "        \n",
    "        pbar = tqdm(total=len(data), desc=\"Calculating Coherence Value - Gensim\")\n",
    "        coherence_gs = 0\n",
    "        \n",
    "        for doc in data:\n",
    "            bow = dictionary.doc2bow(doc)\n",
    "            coherence_gs += gs_model.log_perplexity([bow])\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        coherence_gs /= len(data)\n",
    "        \n",
    "        print(f\"Coherence Value - scikit-learn: {coherence_sk}\")\n",
    "        print(f\"Coherence Value - Gensim: {coherence_gs}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "sklearn_models = [lda_model_100_topics, lda_model_200_topics]\n",
    "gensim_models = [lda_gensim_100_topics, lda_gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
