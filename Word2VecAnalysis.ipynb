{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import pprint as pp\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize, pos_tag, wordpunct_tokenize\n",
    "import en_core_web_lg\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "# https://github.com/buriy/python-readability\n",
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "# https://docs.python.org/3/library/time.html\n",
    "import time\n",
    "\n",
    "# https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "import bs4\n",
    "\n",
    "# https://docs.python.org/3/library/codecs.html\n",
    "import codecs\n",
    "\n",
    "# https://docs.python.org/3/library/json.html\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "\n",
    "import re\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time  # To time our operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the 2018 and 2019 Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 2018 corpus, 2019 corpus\n",
    "nlp = en_core_web_lg.load( disable=['parser','ner'])\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# we create a list of categories/keywords/tags to\n",
    "# be used to refine searches\n",
    "# CAT_PATTERN = r'([0-9]+\\.htm$)'\n",
    "CAT_PATTERN =r'([\\d]+)_html\\.json'\n",
    "\n",
    "# we mark the HTML tags to be used for \n",
    "# extacting the desired article, etc. text\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "#TAGS = ['h1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_ID = ['2021_html.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JOURNALCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\" a corpus reader for CDC Journal articles \"\"\"\n",
    "    # class nltk.corpus.reader.api.CorpusReader\n",
    "    # we explicitly specify the encoding as utf8 even though\n",
    "    # the default is utf8\n",
    "    def __init__(self, root, tags=TAGS, fileids=DOC_ID, encoding='utf8', **kwargs):\n",
    "            \n",
    "        # we use this check to see if the user specified any\n",
    "        # values in the CAT_PATTERN list\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        # initialize the NLTK  reader objects\n",
    "        # review https://www.nltk.org/api/nltk.corpus.reader.api.html#nltk.corpus.reader.api.CategorizedCorpusReader to see\n",
    "        # how __init__ is defined for each module; for the categorized\n",
    "        # corpus reader, we use it to create categories if none are specified.\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "\n",
    "        # https://www.nltk.org/api/nltk.corpus.reader.api.html#nltk.corpus.reader.api.CorpusReader\n",
    "        # encoding –\n",
    "        # The default unicode encoding for the files that make up the corpus. The value of encoding can \n",
    "        # be any of the following:\n",
    "        #   A string: encoding is the encoding name for all files.\n",
    "        #   A dictionary: encoding[file_id] is the encoding name for the file whose identifier is file_id. If file_id is not in encoding, \n",
    "        #       then the file contents will be processed using non-unicode byte strings.\n",
    "        #   A list: encoding should be a list of (regexp, encoding) tuples. The encoding for a file whose \n",
    "        #       identifier is file_id will be the encoding value for the first tuple whose regexp matches\n",
    "        #        the file_id. If no tuple’s regexp matches the file_id, the file contents will be processed using non-unicode byte strings.\n",
    "        #   None: the file contents of all files will be processed using non-unicode byte strings.\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "        self.fileids = fileids\n",
    "        #self.categories = self.categories()\n",
    "        self.tags = tags\n",
    "\n",
    "        #print(\"From the constructor these are the fileids\", fileids)\n",
    "        #print(\"from the constructor these are the categories\", self.categories)\n",
    "        \n",
    "        \n",
    "\n",
    "    # we create a method that will allow us to filter how we\n",
    "    # read the data from disk, either by specifying a list of categories\n",
    "    # or a list of filenames\n",
    "    def resolve(self, fileids, categories):\n",
    "        if fileids is not None and categories is not None:\n",
    "           raise ValueError(\"Specify fileids or categories, not both\")\n",
    "            \n",
    "        if categories is not None:\n",
    "            #pp.pprint(\"This is a test of the resolve() method where categories is not None:\", self.categories)\n",
    "            return self.fileids(categories)\n",
    "    \n",
    "        #pp.pprint(\"This is a test of the resolve() method where categories IS None:\", self.categories)\n",
    "        return fileids\n",
    "\n",
    "    # we use this method to read all values from the key-value objects,\n",
    "    # concatenating them into a list object which is returned.\n",
    "    def docs(self,fileids=None, categories=None):\n",
    "\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "        #for f in fileids:\n",
    "        #    pp.pprint(\"This is a list of the fileids in doc():\", f)\n",
    "        \n",
    "        # https://docs.python.org/3/library/codecs.html\n",
    "        # This module defines base classes for standard Python codecs \n",
    "        # (encoders and decoders) and provides access to the internal Python \n",
    "        # codec registry, which manages the codec and error handling \n",
    "        # lookup process. Most standard codecs are text encodings, which encode \n",
    "        # text to bytes (and decode bytes to text), but there are also codecs \n",
    "        # provided that encode text to text, and bytes to bytes. Custom codecs \n",
    "        # may encode and decode between arbitrary types, but some module features \n",
    "        # are restricted to be used specifically with text encodings or with codecs \n",
    "        # that encode to bytes.\n",
    "\n",
    "        # A string in Python is a sequence of Unicode code points (in range U+0000–U+10FFFF). To store or \n",
    "        # transfer a string, it needs to be serialized as a sequence of bytes.\n",
    "        # Serializing a string into a sequence of bytes is known as “encoding”, \n",
    "        # and recreating the string from the sequence of bytes is known as “decoding”.\n",
    "        # There are a variety of different text serialization codecs, which are collectively \n",
    "        # referred to as “text encodings”.\n",
    "\n",
    "        # codecs.open(filename, mode='r', encoding=None, errors='strict', buffering=-1)\n",
    "        # Open an encoded file using the given mode and return an instance of StreamReaderWriter, \n",
    "        # providing transparent encoding/decoding. The default file mode is 'r', meaning to open \n",
    "        # the file in read mode.\n",
    "        # Note If encoding is not None, then the underlying encoded files are always opened in binary \n",
    "        # mode. No automatic conversion of '\\n' is done on reading and writing. The mode argument\n",
    "        # may be any binary mode acceptable to the built-in open() function; \n",
    "        # the 'b' is automatically added.\n",
    "        \n",
    "        # abspaths() Return a list of the absolute paths for all fileids in this corpus; \n",
    "        #       or for the given list of fileids, if specified.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            #print(\"This is a test of the docs() method using the codecs module\", path)\n",
    "\n",
    "            with codecs.open(path, 'r', encoding=encoding) as f:\n",
    "                data = json.load(f)\n",
    "                #for key, value in data.items():\n",
    "                #    json_list.append(value)\n",
    "\n",
    "                #return data.values()\n",
    "                return data\n",
    "\n",
    "    # we use this method to iterate over each key-value pair, specifically\n",
    "    # iterating over the value ie HTML content\n",
    "    def html(self, fileids=None, categories=None):\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            try:\n",
    "                yield Paper(doc).summary() # summer() Given a HTML file, extracts the text of the article\n",
    "            except Unparseable as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "            \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        for html in self.html(fileids, categories):\n",
    "            soup=bs4.BeautifulSoup(html,'html.parser')\n",
    "            for element in soup.find_all(TAGS):\n",
    "                #if not any(c.isnumeric() for c in element.text):\n",
    "                yield element.text\n",
    "            soup.decompose()\n",
    "                \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in sent_tokenize(paragraph):\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token in wordpunct_tokenize(sentence):\n",
    "                yield token\n",
    "    \n",
    "    def tokenize(self, fileids=None, categories=None):\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            yield[\n",
    "                    pos_tag(wordpunct_tokenize(sent))\n",
    "                    for sent in sent_tokenize(paragraph)\n",
    "                    ]    \n",
    "            \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time()\n",
    "        \n",
    "        #structures to perform counting\n",
    "        counts = nltk.FreqDist()\n",
    "        tokens = nltk.FreqDist()\n",
    "        \n",
    "        #perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras():\n",
    "            counts['paras'] += 1\n",
    "            \n",
    "        for sent in self.sents():\n",
    "            counts['sents'] += 1\n",
    "                \n",
    "        for word in self.words():\n",
    "            counts['words'] += 1\n",
    "            tokens[word] += 1\n",
    "\n",
    "\n",
    "        #compute  the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids)\n",
    "        #n_fileids = len(fileids)\n",
    "        #n_topics = len(self.categories(self.resolve(fileids, categories)) or self.categories)\n",
    "        #n_topics = len(categories)\n",
    "        \n",
    "        #return data structure with information\n",
    "        return{\n",
    "                # number of files\n",
    "                'files': n_fileids,\n",
    "                # number of topics\n",
    "                #'topics': n_topics,\n",
    "                # number of paragraphs\n",
    "                'paras': counts['paras'],\n",
    "                # number of sentences\n",
    "                'sents': counts['sents'],\n",
    "                # number of words\n",
    "                'words': counts['words'],\n",
    "                # average numer of words per sentence\n",
    "                'awps': counts['words'] / counts['sents'],\n",
    "                # size of vocabulary ie number of unique terms\n",
    "                'vocab': len(tokens),\n",
    "                # lexical diversity, the ratio of unique terms to total words\n",
    "                'lexdiv': float(counts['words']) / float(len(tokens)),\n",
    "                # average number of paragraphs per document\n",
    "                'ppdoc': float(counts['paras']) / float(n_fileids),\n",
    "                # average number of sentences per paragraph\n",
    "                'sppar': float(counts['sents']) / float(counts['paras']),\n",
    "                # total processing time\n",
    "                'secs': time() - started,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_to_numpy (model):\n",
    "  \"\"\" Convert the word2vec model (the embeddings) into numpy arrays.\n",
    "  Also create and return the mapping of words to the row numbers.\n",
    "\n",
    "  Parameters:\n",
    "  ===========\n",
    "  model (gensim.Word2Vec): a trained gensim model\n",
    "\n",
    "  Returns:\n",
    "  ========\n",
    "  embeddings (numpy.ndarray): Embeddings of each word\n",
    "  idx, iidx (tuple): idx is a dictionary mapping word to row number\n",
    "                     iidx is a dictionary mapping row number to word\n",
    "  \"\"\"\n",
    "  embeddings = deepcopy (model.wv.get_normed_vectors())\n",
    "  idx = {w:i for i, w in enumerate (model.wv.index_to_key )}\n",
    "  iidx = {i:w for i, w in enumerate (model.wv.index_to_key )}\n",
    "  return embeddings, (idx, iidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_neighbors (embs, query, word2rownum, rownum2word, k=5):\n",
    "  \"\"\" Get the `k` nearest neighbors for a `query`\n",
    "\n",
    "  Parameters:\n",
    "  ===========\n",
    "  embs (numpy.ndarray): The embeddings.\n",
    "  query (str): Word whose nearest neighbors are being found\n",
    "  word2rownum (dict): Map word to row number in the embeddings array\n",
    "  rownum2word (dict): Map rownum from embeddings array to word\n",
    "  k (int, default=5): The number of nearest neighbors\n",
    "\n",
    "  Returns:\n",
    "  ========\n",
    "  neighbors (list): list of near neighbors;\n",
    "                    size of the list is k and each item is in the form\n",
    "                    of word and similarity.\n",
    "  \"\"\"\n",
    "\n",
    "  sims = np.dot (embs, embs[word2rownum[query]])\n",
    "  indices = np.argsort (-sims)\n",
    "  return [(rownum2word[index], sims[index]) for index in indices[1:k+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes(A, B):\n",
    "    \"\"\"\n",
    "    Learn the best rotation matrix to align matrix B to A\n",
    "    https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "    \"\"\"\n",
    "    # U, _, Vt = np.linalg.svd(B.dot(A.T))\n",
    "    U, _, Vt = np.linalg.svd(B.T.dot(A))\n",
    "    return U.dot(Vt)\n",
    "\n",
    "def intersect_vocab (idx1, idx2):\n",
    "  \"\"\" Intersect the two vocabularies\n",
    "\n",
    "  Parameters:\n",
    "  ===========\n",
    "  idx1 (dict): the mapping for vocabulary in the first group\n",
    "  idx2 (dict): the mapping for vocabulary in the second group\n",
    "\n",
    "  Returns:\n",
    "  ========\n",
    "  common_idx, common_iidx (tuple): the common mapping for vocabulary in both groups\n",
    "  \"\"\"\n",
    "  common = idx1.keys() & idx2.keys()\n",
    "  common_vocab = [v for v in common]\n",
    "\n",
    "  common_idx, common_iidx = {v:i for i,v in enumerate (common_vocab)}, {i:v for i,v in enumerate (common_vocab)}\n",
    "  return common_vocab, (common_idx, common_iidx)\n",
    "\n",
    "def align_matrices (mat1, mat2, idx1, idx2):\n",
    "  \"\"\" Align the embedding matrices and their vocabularies.\n",
    "\n",
    "  Parameters:\n",
    "  ===========\n",
    "  mat1 (numpy.ndarray): embedding matrix for first group\n",
    "  mat2 (numpy.ndarray): embedding matrix for second group\n",
    "\n",
    "  index1 (dict): the mapping dictionary for first group\n",
    "  index2 (dict): the mapping dictionary for the second group\n",
    "\n",
    "  Returns:\n",
    "  ========\n",
    "  remapped_mat1 (numpy.ndarray): the aligned matrix for first group\n",
    "  remapped_mat2 (numpy.ndarray): the aligned matrix for second group\n",
    "  common_vocab (tuple): the mapping dictionaries for both the matrices\n",
    "  \"\"\"\n",
    "  common_vocab, (common_idx, common_iidx) = intersect_vocab (idx1, idx2)\n",
    "  row_nums1 = [idx1[v] for v in common_vocab]\n",
    "  row_nums2 = [idx2[v] for v in common_vocab]\n",
    "\n",
    "  #print (len(common_vocab), len (common_idx), len (common_iidx))\n",
    "  remapped_mat1 = mat1[row_nums1, :]\n",
    "  remapped_mat2 = mat2[row_nums2, :]\n",
    "  #print (mat1.shape, mat2.shape, remapped_mat1.shape, remapped_mat2.shape)\n",
    "  omega = procrustes (remapped_mat1, remapped_mat2)\n",
    "  #print (omega.shape)\n",
    "  # rotated_mat2 = np.dot (omega, remapped_mat2)\n",
    "  rotated_mat2 = np.dot (remapped_mat2, omega)\n",
    "\n",
    "  return remapped_mat1, rotated_mat2, (common_idx, common_iidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2020 = JOURNALCorpusReader(\"C:/_harvester/data/html-by-year/\")\n",
    "texts_out=[]\n",
    "inline_text = []\n",
    "for sent in corpus2020.paras():\n",
    "    #pp.pprint(sent)\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            #pp.pprint(token.text)\n",
    "            texts_out.append(re.sub(r'[^\\w\\s]*', '', token.text))\n",
    "    if len(texts_out) > 0:\n",
    "        inline_text.append(texts_out)\n",
    "    texts_out = []\n",
    "\n",
    "texts2020 = [[t for t in text if len(t) > 3] for text in inline_text]\n",
    "#pp.pprint(inline_text[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2021 = JOURNALCorpusReader(\"C:/_harvester/data/html-by-year/\")\n",
    "texts_out=[]\n",
    "inline_text = []\n",
    "for sent in corpus2021.paras():\n",
    "    #pp.pprint(sent)\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']:\n",
    "            #pp.pprint(token.text)\n",
    "            texts_out.append(re.sub(r'[^\\w\\s]*', '', token.text))\n",
    "    if len(texts_out) > 0:\n",
    "        inline_text.append(texts_out)\n",
    "    texts_out = []\n",
    "\n",
    "texts2021 = [[t for t in text if len(t) > 3] for text in inline_text]\n",
    "#pp.pprint(inline_text[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': 1,\n",
       " 'paras': 31342,\n",
       " 'sents': 88607,\n",
       " 'words': 2136357,\n",
       " 'awps': 24.110476598914307,\n",
       " 'vocab': 47838,\n",
       " 'lexdiv': 44.65815878590242,\n",
       " 'ppdoc': 31342.0,\n",
       " 'sppar': 2.8271010146129796,\n",
       " 'secs': 201.5404977798462}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2020.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': 1,\n",
       " 'paras': 24479,\n",
       " 'sents': 70443,\n",
       " 'words': 1802191,\n",
       " 'awps': 25.583677583294296,\n",
       " 'vocab': 44561,\n",
       " 'lexdiv': 40.44323511590853,\n",
       " 'ppdoc': 24479.0,\n",
       " 'sppar': 2.8776910821520487,\n",
       " 'secs': 150.24201607704163}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2021.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "w2v_2020 = Word2Vec(min_count=20, # (int, optional) – Ignores all words with total frequency lower than this.\n",
    "                     window=20, # Maximum distance between the current and predicted word within a sentence.\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=15,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
      " [(2, 1),\n",
      "  (3, 2),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1)],\n",
      " [(3, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)],\n",
      " [(2, 1), (3, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
      " [(8, 1), (24, 1), (25, 1), (26, 1)],\n",
      " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
      " [(2, 1),\n",
      "  (3, 2),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1)],\n",
      " [(3, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)],\n",
      " [(2, 1), (3, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
      " [(8, 1), (24, 1), (25, 1), (26, 1)],\n",
      " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
      " [(2, 1),\n",
      "  (3, 2),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1)],\n",
      " [(3, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)],\n",
      " [(2, 1), (3, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
      " [(8, 1), (24, 1), (25, 1), (26, 1)],\n",
      " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
      " [(2, 1),\n",
      "  (3, 2),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1)],\n",
      " [(3, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)],\n",
      " [(2, 1), (3, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
      " [(8, 1), (24, 1), (25, 1), (26, 1)],\n",
      " [(27, 1)],\n",
      " [(28, 1),\n",
      "  (29, 1),\n",
      "  (30, 3),\n",
      "  (31, 1),\n",
      "  (32, 1),\n",
      "  (33, 4),\n",
      "  (34, 1),\n",
      "  (35, 1),\n",
      "  (36, 1),\n",
      "  (37, 1),\n",
      "  (38, 1),\n",
      "  (39, 2),\n",
      "  (40, 2),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1),\n",
      "  (45, 2),\n",
      "  (46, 1),\n",
      "  (47, 1),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 1),\n",
      "  (51, 1),\n",
      "  (52, 1),\n",
      "  (53, 1),\n",
      "  (54, 3),\n",
      "  (55, 1),\n",
      "  (56, 1),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 1),\n",
      "  (60, 2),\n",
      "  (61, 1),\n",
      "  (62, 5),\n",
      "  (63, 1),\n",
      "  (64, 1),\n",
      "  (65, 1),\n",
      "  (66, 1)],\n",
      " [(13, 1),\n",
      "  (24, 2),\n",
      "  (30, 2),\n",
      "  (32, 1),\n",
      "  (33, 4),\n",
      "  (34, 1),\n",
      "  (38, 2),\n",
      "  (39, 4),\n",
      "  (43, 2),\n",
      "  (49, 1),\n",
      "  (54, 1),\n",
      "  (56, 1),\n",
      "  (57, 1),\n",
      "  (60, 1),\n",
      "  (64, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 1),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 1),\n",
      "  (73, 1),\n",
      "  (74, 2),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 1),\n",
      "  (78, 1),\n",
      "  (79, 1),\n",
      "  (80, 2),\n",
      "  (81, 1),\n",
      "  (82, 1),\n",
      "  (83, 1),\n",
      "  (84, 1),\n",
      "  (85, 1),\n",
      "  (86, 1),\n",
      "  (87, 1),\n",
      "  (88, 1),\n",
      "  (89, 1),\n",
      "  (90, 1),\n",
      "  (91, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 1),\n",
      "  (96, 1),\n",
      "  (97, 1),\n",
      "  (98, 1),\n",
      "  (99, 1),\n",
      "  (100, 1),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 1),\n",
      "  (104, 1),\n",
      "  (105, 3),\n",
      "  (106, 1),\n",
      "  (107, 1),\n",
      "  (108, 2),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 1),\n",
      "  (112, 1),\n",
      "  (113, 1),\n",
      "  (114, 2),\n",
      "  (115, 1),\n",
      "  (116, 2),\n",
      "  (117, 1),\n",
      "  (118, 1),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 1),\n",
      "  (123, 1),\n",
      "  (124, 1),\n",
      "  (125, 1),\n",
      "  (126, 1),\n",
      "  (127, 1),\n",
      "  (128, 1),\n",
      "  (129, 2),\n",
      "  (130, 1)],\n",
      " [(30, 1),\n",
      "  (36, 1),\n",
      "  (39, 2),\n",
      "  (47, 1),\n",
      "  (49, 3),\n",
      "  (80, 1),\n",
      "  (103, 1),\n",
      "  (118, 1),\n",
      "  (125, 1),\n",
      "  (129, 1),\n",
      "  (131, 1),\n",
      "  (132, 1),\n",
      "  (133, 1),\n",
      "  (134, 1),\n",
      "  (135, 3),\n",
      "  (136, 2),\n",
      "  (137, 1),\n",
      "  (138, 1),\n",
      "  (139, 1),\n",
      "  (140, 1),\n",
      "  (141, 1),\n",
      "  (142, 1),\n",
      "  (143, 1),\n",
      "  (144, 1),\n",
      "  (145, 1),\n",
      "  (146, 1),\n",
      "  (147, 1),\n",
      "  (148, 1),\n",
      "  (149, 1),\n",
      "  (150, 1),\n",
      "  (151, 1),\n",
      "  (152, 1),\n",
      "  (153, 1),\n",
      "  (154, 1),\n",
      "  (155, 1),\n",
      "  (156, 1),\n",
      "  (157, 1),\n",
      "  (158, 1),\n",
      "  (159, 1),\n",
      "  (160, 2)],\n",
      " [(30, 2),\n",
      "  (33, 1),\n",
      "  (39, 1),\n",
      "  (43, 1),\n",
      "  (49, 4),\n",
      "  (51, 1),\n",
      "  (54, 4),\n",
      "  (80, 4),\n",
      "  (93, 1),\n",
      "  (96, 1),\n",
      "  (101, 1),\n",
      "  (111, 1),\n",
      "  (125, 1),\n",
      "  (135, 3),\n",
      "  (136, 3),\n",
      "  (161, 1),\n",
      "  (162, 1),\n",
      "  (163, 1),\n",
      "  (164, 1),\n",
      "  (165, 1),\n",
      "  (166, 1),\n",
      "  (167, 1),\n",
      "  (168, 2),\n",
      "  (169, 1),\n",
      "  (170, 1),\n",
      "  (171, 1),\n",
      "  (172, 1),\n",
      "  (173, 1),\n",
      "  (174, 1),\n",
      "  (175, 1),\n",
      "  (176, 2),\n",
      "  (177, 1),\n",
      "  (178, 1),\n",
      "  (179, 1),\n",
      "  (180, 1),\n",
      "  (181, 2),\n",
      "  (182, 1),\n",
      "  (183, 1),\n",
      "  (184, 1),\n",
      "  (185, 1),\n",
      "  (186, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10204058, 22402410)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create Dictionary\n",
    "id2word2020 = Dictionary(texts2020)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "id2word2020.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus2020 = [id2word2020.doc2bow(text) for text in texts2020]\n",
    "\n",
    "pp.pprint(corpus2020[0:25])\n",
    "\n",
    "w2v_2020.build_vocab(texts2020, progress_per=100)\n",
    "\n",
    "w2v_2020.train(texts2020, total_examples=w2v_2020.corpus_count, epochs=30, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "\n",
    "w2v_2021 = Word2Vec(min_count=20, # (int, optional) – Ignores all words with total frequency lower than this.\n",
    "                     window=20, # Maximum distance between the current and predicted word within a sentence.\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=15,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7887693, 18430860)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word2021 = Dictionary(texts2021)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "id2word2021.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus2021 = [id2word2021.doc2bow(text) for text in texts2021]\n",
    "\n",
    "w2v_2021.build_vocab(texts2021, progress_per=100)\n",
    "\n",
    "w2v_2021.train(texts2021, total_examples=w2v_2021.corpus_count, epochs=30, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_2020.save(f\"C:/_harvester/data/word2vec-models/word2vec-2020.model\")\n",
    "w2v_2021.save(f\"C:/_harvester/data/word2vec-models/word2vec-2021.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'contact: 0.4746364951133728'\n",
      "'compatible: 0.4046168327331543'\n",
      "'close: 0.4037652909755707'\n",
      "'exposure: 0.366028368473053'\n",
      "'confirmed: 0.36508429050445557'\n",
      "'case: 0.36319348216056824'\n",
      "'transmission: 0.3622850477695465'\n",
      "'contacts: 0.3617348372936249'\n",
      "'absence: 0.345529705286026'\n",
      "'source: 0.3453209698200226'\n",
      "\n",
      "\n",
      "'older: 0.4922367334365845'\n",
      "'more: 0.469707727432251'\n",
      "'People: 0.46366068720817566'\n",
      "'less: 0.4067952334880829'\n",
      "'younger: 0.3945811688899994'\n",
      "'poorer: 0.39318808913230896'\n",
      "'adults: 0.3846714496612549'\n",
      "'disadvantaged: 0.3812831938266754'\n",
      "'Black: 0.3796895146369934'\n",
      "'also: 0.35838523507118225'\n",
      "\n",
      "\n",
      "'infection: 0.5655866861343384'\n",
      "'infections: 0.5163565874099731'\n",
      "'respiratory: 0.5100288987159729'\n",
      "'transmission: 0.503086507320404'\n",
      "'spread: 0.4813520312309265'\n",
      "'widespread: 0.47528985142707825'\n",
      "'coronavirus: 0.47013017535209656'\n",
      "'influenza: 0.46524468064308167'\n",
      "'causes: 0.4646841287612915'\n",
      "'CoV2: 0.4604160189628601'\n",
      "\n",
      "\n",
      "'symptoms: 0.566379725933075'\n",
      "'signs: 0.53143709897995'\n",
      "'fever: 0.518904983997345'\n",
      "'nasopharyngeal: 0.4885794222354889'\n",
      "'shortness: 0.48651039600372314'\n",
      "'headache: 0.4304758906364441'\n",
      "'breath: 0.42518383264541626'\n",
      "'chest: 0.41820067167282104'\n",
      "'symptomatic: 0.41377487778663635'\n",
      "'rash: 0.37755367159843445'\n",
      "\n",
      "\n",
      "'acute: 0.538270115852356'\n",
      "'virus: 0.5100290179252625'\n",
      "'illness: 0.49209773540496826'\n",
      "'like: 0.4761068522930145'\n",
      "'influenza: 0.46794208884239197'\n",
      "'severe: 0.4570218622684479'\n",
      "'coronavirus: 0.434680312871933'\n",
      "'pneumonia: 0.41422736644744873'\n",
      "'specimens: 0.4042208194732666'\n",
      "'pathogen: 0.40393975377082825'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = ['person', 'people', 'virus', 'cough', 'respiratory']\n",
    "for word in word_list:\n",
    "    similar_words = w2v_2020.wv.most_similar(positive=[word])\n",
    "    for word, similarity in similar_words:\n",
    "        pp.pprint(f\"{word}: {similarity}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'learning: 0.5907158255577087'\n",
      "'school: 0.5628183484077454'\n",
      "'Schools: 0.5621341466903687'\n",
      "'instruction: 0.5525521039962769'\n",
      "'students: 0.551885724067688'\n",
      "'schools: 0.5310417413711548'\n",
      "'extracurricular: 0.530912458896637'\n",
      "'kindergarten: 0.5203924179077148'\n",
      "'close: 0.5197715759277344'\n",
      "'hybrid: 0.5129963755607605'\n",
      "\n",
      "\n",
      "'African: 0.5883188843727112'\n",
      "'Caribbean: 0.5746883749961853'\n",
      "'immigrants: 0.5463264584541321'\n",
      "'counterparts: 0.541668713092804'\n",
      "'American: 0.49978429079055786'\n",
      "'focuses: 0.479608416557312'\n",
      "'diabetes: 0.46489831805229187'\n",
      "'People: 0.4639800786972046'\n",
      "'Additionally: 0.46270835399627686'\n",
      "'often: 0.46045953035354614'\n",
      "\n",
      "\n",
      "'causes: 0.7586001753807068'\n",
      "'coronavirus: 0.6103140115737915'\n",
      "'host: 0.5569154620170593'\n",
      "'antibodies: 0.5551154613494873'\n",
      "'circulation: 0.5453903079032898'\n",
      "'infectious: 0.5447540283203125'\n",
      "'respiratory: 0.5352216958999634'\n",
      "'surface: 0.5305452346801758'\n",
      "'CoV2: 0.5154407620429993'\n",
      "'transmission: 0.5141725540161133'\n",
      "\n",
      "\n",
      "'shortness: 0.894736111164093'\n",
      "'breath: 0.8729056119918823'\n",
      "'headache: 0.8508797883987427'\n",
      "'fatigue: 0.8153740763664246'\n",
      "'fever: 0.7914939522743225'\n",
      "'chest: 0.7560005187988281'\n",
      "'vomiting: 0.735579252243042'\n",
      "'diarrhea: 0.7352695465087891'\n",
      "'pneumonia: 0.6921139359474182'\n",
      "'symptoms: 0.6542554497718811'\n",
      "\n",
      "\n",
      "'limb: 0.6518383622169495'\n",
      "'cough: 0.6035229563713074'\n",
      "'weakness: 0.5851101279258728'\n",
      "'pneumonia: 0.5816494822502136'\n",
      "'circulation: 0.5773316621780396'\n",
      "'nasopharyngeal: 0.5590355396270752'\n",
      "'psittacosis: 0.558430016040802'\n",
      "'chest: 0.5441216826438904'\n",
      "'virus: 0.5352216362953186'\n",
      "'gastrointestinal: 0.5295103788375854'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = ['person', 'people', 'virus', 'cough', 'respiratory']\n",
    "for word in word_list:\n",
    "    similar_words = w2v_2021.wv.most_similar(positive=[word])\n",
    "    for word, similarity in similar_words:\n",
    "        pp.pprint(f\"{word}: {similarity}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embedding to numpy array\n",
    "\n",
    "\"\"\"\n",
    "  idx, iidx (tuple): idx is a dictionary mapping word to row number\n",
    "                     iidx is a dictionary mapping row number to word\n",
    "\"\"\"\n",
    "embs2020, (idx2020, iidx2020) = w2v_to_numpy(w2v_2020)\n",
    "embs2021, (idx2021, iidx2021) = w2v_to_numpy(w2v_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near neighbors for virus in the 2018 corpus\n",
      "('infection', 0.56558675)\n",
      "('infections', 0.51635665)\n",
      "('respiratory', 0.51002896)\n",
      "('transmission', 0.5030865)\n",
      "('spread', 0.48135212)\n",
      "('widespread', 0.4752899)\n",
      "('coronavirus', 0.4701302)\n",
      "('influenza', 0.4652447)\n",
      "('causes', 0.4646842)\n",
      "('CoV2', 0.46041608)\n",
      "\n",
      "Near neighbors for virus in the 2019 corpus\n",
      "('causes', 0.7586002)\n",
      "('coronavirus', 0.61031395)\n",
      "('host', 0.5569155)\n",
      "('antibodies', 0.55511546)\n",
      "('circulation', 0.54539025)\n",
      "('infectious', 0.544754)\n",
      "('respiratory', 0.53522164)\n",
      "('surface', 0.53054523)\n",
      "('CoV2', 0.51544076)\n",
      "('transmission', 0.5141725)\n"
     ]
    }
   ],
   "source": [
    "query = 'virus'\n",
    "print (f'Near neighbors for {query} in the 2018 corpus')\n",
    "for item in near_neighbors (embs2020, query, idx2020, iidx2020, k=10):\n",
    "  print (item)\n",
    "print ()\n",
    "print (f'Near neighbors for {query} in the 2019 corpus')\n",
    "for item in near_neighbors (embs2021, query, idx2021, iidx2021, k=10):\n",
    "  print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2020_aligned_embs, _2021_aligned_embs, (common_idx, common_iidx) = align_matrices (embs2020, embs2021, idx2020, idx2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contact', 0.47463652),\n",
       " ('compatible', 0.4046168),\n",
       " ('close', 0.40376526),\n",
       " ('exposure', 0.36602837),\n",
       " ('confirmed', 0.36508432)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "near_neighbors(_2020_aligned_embs, 'person', common_idx, common_iidx, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 0.59071594),\n",
       " ('school', 0.5628184),\n",
       " ('Schools', 0.56213427),\n",
       " ('students', 0.55188584),\n",
       " ('schools', 0.5310418)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "near_neighbors(_2021_aligned_embs, 'person', common_idx, common_iidx, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease 0.6502698\n",
      "symptom 0.6223781\n",
      "respiratory 0.62005043\n",
      "cough 0.5967292\n",
      "child 0.42598912\n",
      "adult 0.39087787\n",
      "person 0.36146486\n"
     ]
    }
   ],
   "source": [
    "journal_words = ['disease','person', 'child', 'adult', 'cough', 'respiratory', 'symptom']\n",
    "journal_words = [(w, _2020_aligned_embs[common_idx[w]].dot(_2021_aligned_embs[common_idx[w]])) for w in journal_words]\n",
    "for w,score in sorted (journal_words, key=lambda x:x[1], reverse=True):\n",
    "  print (w, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
