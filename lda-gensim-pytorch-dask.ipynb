{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was written using CDC AI Chatbot. A variety of prompts were used, including questions and prompts to \n",
    "    correct bugs, memory issues(ie too little resources available), generate comments, etc.\n",
    "\n",
    "maintenance: alan hamm(pqn7)\n",
    "apr 2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim  # Library for interactive topic model visualization\n",
    "import torch  # PyTorch library for deep learning and GPU acceleration\n",
    "from torch.utils.data import DataLoader  # Provides an iterator over a dataset for efficient batch processing\n",
    "from tqdm import tqdm  # Creates progress bars to visualize the progress of loops or tasks\n",
    "from gensim.models import LdaModel  # Implements LDA for topic modeling using the Gensim library\n",
    "from gensim.corpora import Dictionary  # Represents a collection of text documents as a bag-of-words corpus\n",
    "from gensim.models import CoherenceModel  # Computes coherence scores for topic models\n",
    "from gensim.models.callbacks import PerplexityMetric, ConvergenceMetric, CoherenceMetric # Callbacks for monitoring model training progress\n",
    "\n",
    "import os  # Provides functions for interacting with the operating system, such as creating directories\n",
    "import itertools  # Provides various functions for efficient iteration and combination of elements\n",
    "import numpy as np  # Library for numerical computing in Python, used for array operations and calculations\n",
    "from time import time  # Measures the execution time of code snippets or functions\n",
    "import pprint as pp  # Pretty-printing library, used here to format output in a readable way\n",
    "import pandas as pd\n",
    "import logging # Logging module for generating log messages\n",
    "import sys # Provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter \n",
    "import shutil # High-level file operations such as copying and removal \n",
    "import zipfile # Provides tools to create, read, write, append, and list a ZIP file\n",
    "from tqdm.notebook import tqdm  # Creates progress bars in Jupyter Notebook environment\n",
    "import dask   # Parallel computing library that scales Python workflows across multiple cores or machines \n",
    "from dask.distributed import Client, LocalCluster   # Distributed computing framework that extends Dask functionality \n",
    "from dask.diagnostics import ProgressBar   # Visualizes progress of Dask computations \n",
    "from dask.delayed import Delayed # Decorator for creating delayed objects in Dask computations\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defensive programming\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask dashboard throws deprecation warnings w.r.t. Bokeh\n",
    "import warnings\n",
    "from bokeh.util.deprecation import BokehDeprecationWarning\n",
    "\n",
    "# Disable Bokeh deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=BokehDeprecationWarning)\n",
    "# Filter out the specific warning message\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"distributed.utils_perf\")\n",
    "\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'circle() method with size value' was deprecated in Bokeh 3.4.0 and will be removed, use 'scatter(size=...) instead' instead.\n",
    "#BokehDeprecationWarning: 'square() method' was deprecated in Bokeh 3.4.0 and will be removed, use \"scatter(marker='square', ...) instead\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics for LDA and step size\n",
    "START_TOPICS = 1\n",
    "END_TOPICS = 2\n",
    "STEP_SIZE = 1\n",
    "\n",
    "# define the decade that is being modelled \n",
    "DECADE = '2010s'\n",
    "\n",
    "# In the case of this machine, since it has an Intel Core i9 processor with 8 physical cores (16 threads with Hyper-Threading), \n",
    "# it would be appropriate to set the number of workers in Dask Distributed LocalCluster to 8 or slightly lower to allow some CPU \n",
    "# resources for other tasks running on your system.\n",
    "CORES = 8\n",
    "\n",
    "# specify the number of passes for Gensim LdaModel\n",
    "PASSES = 15\n",
    "\n",
    "# specify the number of iterations\n",
    "ITERATIONS = 150\n",
    "\n",
    "# specify the chunk size for LdaModel object\n",
    "CHUNKSIZE = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure\n",
    "log_dir = f\"C:/_harvester/data/lda-models/{DECADE}_html/\"\n",
    "model_dir = f\"C:/_harvester/data/lda-models/{DECADE}_html/lda-models/\"\n",
    "image_dir = f\"C:/_harvester/data/lda-models/{DECADE}_html/visuals/\"\n",
    "train_eval_out = f\"C:/_harvester/data/lda-models/{DECADE}_html/train-eval-data/\"\n",
    "\n",
    "# Check if the directories exist and contain data\n",
    "if os.path.exists(log_dir) and os.path.exists(model_dir) and os.path.exists(image_dir):\n",
    "    log_files = os.listdir(log_dir)\n",
    "    model_files = os.listdir(model_dir)\n",
    "    image_files = os.listdir(image_dir)\n",
    "\n",
    "    # Check if the directories are not empty\n",
    "    if log_files or model_files or image_files:\n",
    "        # Find an available filename for the archive\n",
    "        counter = 0\n",
    "        while True:\n",
    "            archive_file = f\"C:/_harvester/data/lda-models/{DECADE}_html/archive{counter:04d}.zip\"\n",
    "            if not os.path.exists(archive_file):\n",
    "                break\n",
    "            counter += 1\n",
    "\n",
    "        # Create the zip file for archiving existing folders\n",
    "        with zipfile.ZipFile(archive_file, 'w') as zipf:\n",
    "            # Add log files to the zip file\n",
    "            for log_file in log_files:\n",
    "                zipf.write(os.path.join(log_dir, log_file), arcname=os.path.join(\"log\", log_file))\n",
    "            \n",
    "            # Add model files to the zip file\n",
    "            for model_file in model_files:\n",
    "                zipf.write(os.path.join(model_dir, model_file), arcname=os.path.join(\"model\", model_file))\n",
    "            \n",
    "            # Add image files to the zip file\n",
    "            for image_file in image_files:\n",
    "                zipf.write(os.path.join(image_dir, image_file), arcname=os.path.join(\"image\", image_file))\n",
    "\n",
    "        # Remove existing subdirectories after archiving them\n",
    "        for subdir in [log_dir, model_dir, image_dir]:\n",
    "            if os.path.exists(subdir):\n",
    "                subfiles = os.listdir(subdir)\n",
    "                for subfile in subfiles:\n",
    "                    filepath = os.path.join(subdir, subfile)\n",
    "                    if os.path.isdir(filepath):\n",
    "                        os.rmdir(filepath)\n",
    "\n",
    "# Create fresh directories for the new run\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(train_eval_out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        # Get the properties of each GPU device\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        \n",
    "        print(f\"\\nGPU Device {i} Properties:\")\n",
    "        print(f\"Device Name: {gpu_properties.name}\")\n",
    "        print(f\"Total Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Multiprocessor Count: {gpu_properties.multi_processor_count}\")\n",
    "        print(f\"CUDA Capability Major Version: {gpu_properties.major}\")\n",
    "        print(f\"CUDA Capability Minor Version: {gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# verify if CUDA is being used or the CPU\n",
    "if device is not None:\n",
    "    # Check if PyTorch is currently using the GPU\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        print(\"PyTorch is using the GPU.\")\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(\"CUDA Version:\", cuda_version)\n",
    "    else:\n",
    "        print(\"PyTorch is using the CPU.\")\n",
    "else:\n",
    "    print(\"The device is neither using the GPU nor CPU. An error has ocurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter `alpha` in Latent Dirichlet Allocation (LDA) represents the concentration parameter of the Dirichlet \n",
    "# prior distribution for the topic-document distribution.\n",
    "# It controls the sparsity of the resulting document-topic distributions.\n",
    "\n",
    "# A lower value of `alpha` leads to sparser distributions, meaning that each document is likely to be associated with fewer topics.\n",
    "# Conversely, a higher value of `alpha` encourages documents to be associated with more topics, resulting in denser distributions.\n",
    "\n",
    "# The choice of `alpha` affects the balance between topic diversity and document specificity in LDA modeling.\n",
    "alpha_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "alpha_values += ['symmetric', 'asymmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Latent Dirichlet Allocation (LDA) topic analysis, the beta parameter represents the concentration \n",
    "# parameter of the Dirichlet distribution used to model the topic-word distribution. It controls the \n",
    "# sparsity of topics by influencing how likely a given word is to be assigned to a particular topic.\n",
    "\n",
    "# A higher value of beta encourages topics to have a more uniform distribution over words, resulting in more \n",
    "# general and diverse topics. Conversely, a lower value of beta promotes sparser topics with fewer dominant words.\n",
    "\n",
    "# The choice of beta can impact the interpretability and granularity of the discovered topics in LDA.\n",
    "beta_values = np.arange(0.01, 1, 0.3).tolist()\n",
    "beta_values += ['symmetric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data_generator function is defined as a generator. It opens the specified JSON file (filename) \n",
    "and iterates over its lines using a for loop. Each line is parsed using json.loads() to convert it \n",
    "into a Python object (e.g., dictionary). The yield keyword is used instead of return to create a \n",
    "generator that produces one parsed JSON object at a time.\n",
    "\n",
    "The num_samples variable counts the total number of lines in the JSON file by opening it (open(filename)) \n",
    "and iterating over its lines using a generator expression (sum(1 for _ in open(filename))). This gives \n",
    "us an estimate of how many samples are present in the dataset.\n",
    "\n",
    "The num_train_samples variable calculates the desired number of samples for training based on the provided \n",
    "train_ratio. It multiplies num_samples by train_ratio, converting it to an integer using int().\n",
    "\n",
    "Two empty lists, train_data and eval_data, are initialized to store training and evaluation datasets, respectively.\n",
    "\n",
    "An instance of the `data_generator\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "def create_lda_datasets(filename, train_ratio):\n",
    "    # Get the file size in bytes\n",
    "    file_size = os.path.getsize(filename)\n",
    "\n",
    "    # Get the last modified timestamp of the file\n",
    "    last_modified = os.path.getmtime(filename)\n",
    "\n",
    "    # Print the metadata\n",
    "    print(\"File Metadata:\")\n",
    "    print(f\"Filename: {filename}\")\n",
    "    print(f\"Size: {file_size} bytes\")\n",
    "    print(f\"Last Modified: {last_modified}\")\n",
    "    \n",
    "    with open(filename, 'r') as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    \n",
    "    num_samples = len(data)  # Count the total number of samples\n",
    "    num_train_samples = int(num_samples * train_ratio)  # Calculate the number of samples for training\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_data = data[:num_train_samples]  # Assign a portion of data for training\n",
    "    eval_data = data[num_train_samples:]  # Assign the remaining data for evaluation\n",
    "    \n",
    "    print(f\"Number of training samples: {len(train_data)}\")\n",
    "    print(f\"Number of eval samples: {len(eval_data)}\")\n",
    "\n",
    "\n",
    "    return train_data, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method trains a Latent Dirichlet Allocation (LDA) model using the Gensim library. Here is a breakdown of the steps involved:\n",
    "\n",
    "    (1)The method takes in parameters such as the number of topics (n_topics), alpha and beta hyperparameters, data (a list of documents), \n",
    "        and train_eval (a boolean indicating whether it's training or evaluation).\n",
    "\n",
    "    (2)If train_eval is True, a logging configuration is set up to log training information to a file named \"train-model.log\". \n",
    "        Otherwise, it logs to \"eval-model.log\".\n",
    "\n",
    "    (3) Two empty lists, combined_corpus and combined_text, are initialized to store the combined corpus and text.\n",
    "\n",
    "    (4) The number of passes for training the LDA model is set to 11.\n",
    "\n",
    "    (5) A loop iterates over each document in the data list. Inside the loop:\n",
    "            - A Gensim Dictionary object is created from the current document.\n",
    "            - The document is converted into a bag-of-words representation using doc2bow().\n",
    "            - A PerplexityMetric object is created to track perplexity during training.\n",
    "            - If combined_text is empty, indicating that it's the first iteration:\n",
    "                The initial LDA model is trained using LdaModel() with parameters such as corpus, \n",
    "                id2word (the dictionary), num_topics, alpha, beta, random_state, passes, iterations, chunksize, and per_word_topics.\n",
    "\n",
    "            - Otherwise:\n",
    "                The existing LDA model is updated with new data using lda_model_gensim.update(corpus).\n",
    "                The current document's text and corpus are added to combined_text and combined_corpus respectively.\n",
    "\n",
    "    (6) Logging is shut down.\n",
    "\n",
    "    (7) Finally, the trained LDA model (lda_model_gensim), combined_corpus, and combined_text are returned.\n",
    "\"\"\"\n",
    "\n",
    "def train_model(n_topics: int, alpha: list, beta: list, data: list, train_eval: bool):\n",
    "\n",
    "    # create a log for either training or eval, dependent upon what data is being processed\n",
    "    logger_name = \"train-model\" if train_eval else \"eval-model\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.NOTSET)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "\n",
    "    file_handler = logging.FileHandler(f\"C:/_harvester/data/lda-models/2010s_html/{logger_name}.log\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "            \n",
    "    combined_corpus = []  # Initialize list to store combined corpus\n",
    "    combined_text = []\n",
    "        \n",
    "\n",
    "    for texts_out in tqdm(data, desc=\"Training LDA models\"):\n",
    "        # Split each document into tokens\n",
    "        tokenized_texts = [text.split(' ') for text in texts_out]\n",
    "\n",
    "        # Create a dictionary from the tokenized texts\n",
    "        dictionary = Dictionary(tokenized_texts)\n",
    "            \n",
    "        # Convert each document to bag-of-words representation\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in tokenized_texts]\n",
    "                \n",
    "        perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "        convergence_logger = ConvergenceMetric(logger='shell')\n",
    "        coherence_cv_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'c_v', texts = tokenized_texts)\n",
    "        if not combined_text:\n",
    "            logging.info(f\"Training the initial model on a single corpus.\\n\")\n",
    "            lda_model_gensim = LdaModel(corpus=corpus,\n",
    "                                        id2word=dictionary,\n",
    "                                        num_topics=n_topics,\n",
    "                                        alpha=alpha,\n",
    "                                        eta=beta,\n",
    "                                        random_state=75,\n",
    "                                        passes=PASSES,\n",
    "                                        iterations=ITERATIONS,\n",
    "                                        chunksize=CHUNKSIZE,\n",
    "                                        per_word_topics=True,\n",
    "                                        callbacks=[convergence_logger, perplexity_logger, coherence_cv_logger],)\n",
    "        else:\n",
    "            logging.info(\"Updating the model with new data.\\n\")\n",
    "            lda_model_gensim.update(corpus)\n",
    "                \n",
    "        combined_text += texts_out\n",
    "        dictionary.add_documents(texts_out)  # Update the dictionary with new documents\n",
    "        combined_corpus.extend(corpus)  # Extend the combined corpus with current year's corpus\n",
    "\n",
    "\n",
    "    # Redundancy check for calculating metrics via alternative method. Note, the above calculates the metrics for each\n",
    "    # iteration while the below only makes calculations on the final model.      \n",
    "    # Compute convergence score\n",
    "    convergence_score = lda_model_gensim.bound(combined_corpus)\n",
    "\n",
    "    # Compute perplexity score\n",
    "    perplexity_score = lda_model_gensim.log_perplexity(combined_corpus)   \n",
    "\n",
    "    #computer the coherence value\n",
    "    coherence_model = CoherenceModel(model=lda_model_gensim, texts=combined_text, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    return lda_model_gensim, combined_corpus, combined_text, convergence_score, perplexity_score, coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    # Specify the local directory path\n",
    "    DASK_DIR = '/_harvester/tmp-dask-out'\n",
    "\n",
    "    # Deploy a Single-Machine Multi-GPU Cluster\n",
    "    # https://medium.com/@aryan.gupta18/end-to-end-recommender-systems-with-merlin-part-1-89fabe2fa05b\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Specify GPU device IDs\n",
    "    protocol = \"tcp\"  # \"tcp\" or \"ucx\"\n",
    "    num_gpus = 1\n",
    "    NUM_GPUS=[0]\n",
    "    visible_devices = \",\".join([str(n) for n in NUM_GPUS])  # Select devices to place workers\n",
    "    device_limit_frac = 0.7  # Spill GPU-Worker memory to host at this limit.\n",
    "    device_pool_frac = 0.8\n",
    "    part_mem_frac = 0.15\n",
    "\n",
    "    # Manually specify the total device memory size (in bytes)\n",
    "    device_size = 10 * 1024 * 1024 * 1024  # GPU has 12GB but setting at 10GB\n",
    "            \n",
    "    ram_memory_limit = \"75GB\" # Set the RAM memory limit (per worker)\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    part_size = int(part_mem_frac * device_size)\n",
    "\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=CORES,\n",
    "            threads_per_worker=2,\n",
    "            #processes=False,\n",
    "            memory_limit=ram_memory_limit,\n",
    "            local_directory=DASK_DIR,\n",
    "            dashboard_address=\":8787\",\n",
    "            protocol=\"tcp\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create the distributed client\n",
    "    client = Client(cluster)\n",
    "\n",
    "    # Get information about workers from scheduler\n",
    "    workers_info = client.scheduler_info()[\"workers\"]\n",
    "\n",
    "    # Iterate over workers and set their memory limits\n",
    "    for worker_id, worker_info in workers_info.items():\n",
    "        worker_info[\"memory_limit\"] = ram_memory_limit\n",
    "\n",
    "    # Verify that memory limits have been set correctly\n",
    "    #for worker_id, worker_info in workers_info.items():\n",
    "    #    print(f\"Worker {worker_id}: Memory Limit - {worker_info['memory_limit']}\")\n",
    "\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "\n",
    "\n",
    "        \n",
    "    # Load data from the JSON file\n",
    "    filename = \"C:/_harvester/data/tokenized-sentences/10s/tokenized_sents-w-bigrams.json\"\n",
    "    train_ratio = 0.8\n",
    "\n",
    "    # create training and evaluation data\n",
    "    print(\"Creating training and evaluation samples...\")\n",
    "    started = time()\n",
    "    train_data, eval_data = create_lda_datasets(filename, train_ratio)\n",
    "    print(f\"Completed creation of training and evaluation samples in {round((time()- started)/60,2)} minutes.\\n\")\n",
    "\n",
    "    # Save training data to a file\n",
    "    print(\"Saving the training data...\")\n",
    "    train_data_filename = \"C:/_harvester/data/lda-models/2010s_html/train-eval-data/train_data.json\"\n",
    "    with open(train_data_filename, 'w') as train_file:\n",
    "        json.dump(train_data, train_file)\n",
    "\n",
    "    # Save evaluation data to a file\n",
    "    print(\"Saving the eval data...\\n\")\n",
    "    eval_data_filename = \"C:/_harvester/data/lda-models/2010s_html/train-eval-data/eval_data.json\"\n",
    "    with open(eval_data_filename , 'w') as eval_file:\n",
    "        json.dump(eval_data , eval_file)\n",
    "\n",
    "    # Scatter the training data\n",
    "    print(f\"Scattering {len(train_data)} training samples...\")\n",
    "    started = time()\n",
    "    train_data_scattered = client.scatter(train_data)\n",
    "    print(f\"Training data scattered successfully in {round((time()- started)/60,2)} minutes.\\n\")\n",
    "\n",
    "    # Scatter the eval data\n",
    "    print(f\"Scattering {len(eval_data)} training samples...\")\n",
    "    started = time()\n",
    "    eval_data_scattered = client.scatter(eval_data)\n",
    "    print(f\"Evaluation data scattered successfully in {round((time()- started)/60,2)} minutes.\\n\")\n",
    "\n",
    "\n",
    "    train_results = []  # List to store delayed objects for training\n",
    "    eval_results = []  # List to store delayed objects for evaluation\n",
    "\n",
    "    # Iterate over the range of topics from START_TOPICS to END_TOPICS with a step size of STEP_SIZE\n",
    "    for n_topics in range(START_TOPICS, END_TOPICS + 1, STEP_SIZE):\n",
    "        topics_message = f\"Topics({n_topics}) are in the model being trained.\"\n",
    "        # Iterate over all combinations of alpha and beta values using itertools.product()\n",
    "        for alpha, beta in itertools.product(alpha_values, beta_values):\n",
    "            # Create delayed objects for training and evaluation using dask.delayed()\n",
    "            future_train = dask.delayed(train_model)(n_topics, alpha, beta, train_data, True)\n",
    "            future_eval = dask.delayed(train_model)(n_topics, alpha, beta, eval_data, False)\n",
    "            \n",
    "            # Append the delayed objects to respective result lists\n",
    "            train_results.append(future_train)\n",
    "            eval_results.append(future_eval)\n",
    "\n",
    "        print(topics_message)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        training data\n",
    "    \"\"\"\n",
    "        # Dictionary to hold the metrics that are generated\n",
    "    train_metrics_csv = {\n",
    "        'n_topics': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'cv_score': [],\n",
    "        'convergence_score': [],\n",
    "        'log_perplexity': [],\n",
    "        'time_to_complete': []\n",
    "    }\n",
    "\n",
    "    # Iterate over the results of lda_models_train and combinations of alpha and beta values\n",
    "    progress_bar_train = tqdm(total=len(train_results), desc=\"Modeling the training data...\")\n",
    "    # Set the scheduler to 'distributed' using dask.config.set()\n",
    "    with dask.config.set(scheduler='distributed'):\n",
    "        lda_models_train = None # Define lda_models_train with a default value\n",
    "        try:\n",
    "            # Compute the delayed objects in train_results using dask.compute()\n",
    "            lda_models_train = dask.compute(*train_results, progressbar=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\n\")\n",
    "            for result in train_results:\n",
    "                if not isinstance(result, Delayed):\n",
    "                    print(\"Invalid element found in train_results:\", result)\n",
    "                # Add a small delay to allow the progress bar to update\n",
    "                time.sleep(0.1)  # Adjust the sleep duration as needed\n",
    "                progress_bar_train.update(1)\n",
    "        else:\n",
    "            for i, ((lda_model_gensim, combined_corpus, combined_text), (alpha, beta)) in enumerate(zip(lda_models_train, itertools.product(alpha_values, beta_values))):\n",
    "                n_topics = list(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE))[i]\n",
    "                lda_model_gensim, combined_corpus, combined_text, convergence_score, perplexity_score, coherence_score = dask.compute(future_train)\n",
    "\n",
    "                # Save metrics to dictionary for training data\n",
    "                train_metrics_csv['n_topics'].append(n_topics)\n",
    "                train_metrics_csv['alpha'].append(alpha)\n",
    "                train_metrics_csv['beta'].append(beta)\n",
    "                train_metrics_csv['cv_score'].append(coherence_score)\n",
    "                train_metrics_csv['convergence_score'].append(convergence_score)\n",
    "                train_metrics_csv['log_perplexity'].append(perplexity_score)\n",
    "\n",
    "                # Log metrics to a file for training data\n",
    "                log_filename_txt = os.path.join(log_dir, f\"train_lda_metrics.txt\")\n",
    "\n",
    "                with open(log_filename_txt, 'a') as log_file:\n",
    "                    log_file.write(f\"Number of Topics: {n_topics}  |  \")\n",
    "                    log_file.write(f\"Alpha: {alpha}  |  \")\n",
    "                    log_file.write(f\"Beta: {beta}  |  \")\n",
    "                    log_file.write(f\"Coherence Value (c_v) - Gensim: {coherence_score}  |  \")\n",
    "                    log_file.write(f\"Convergence Score - Gensim: {convergence_score}  |  \")\n",
    "                    log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}\\n\")\n",
    "\n",
    "                progress_bar_train.update(1)\n",
    "                # Add a small delay to allow the progress bar to update\n",
    "                time.sleep(0.1)  # Adjust the sleep duration as needed\n",
    "    progress_bar_train.close()\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        evalation data\n",
    "    \"\"\"\n",
    "    # Dictionary to hold the metrics that are generated\n",
    "    eval_metrics_csv = {\n",
    "        'n_topics': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'cv_score': [],\n",
    "        'convergence_score': [],\n",
    "        'log_perplexity': [],\n",
    "        'time_to_complete': []\n",
    "    }\n",
    "\n",
    "    # Compute lda_models using Dask for evaluation data\n",
    "    progress_bar_eval = tqdm(total=len(eval_results), desc=\"Modeling evaluation data...\")\n",
    "    with dask.config.set(scheduler='distributed'):\n",
    "        lda_models_eval = None  # Define lda_models_eval with a default value\n",
    "        try:\n",
    "            lda_models_eval = dask.compute(*eval_results)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\n\")\n",
    "            for result in eval_results:\n",
    "                if not isinstance(result, Delayed):\n",
    "                    print(\"Invalid element found in eval_results:\", result)\n",
    "                # Add a small delay to allow the progress bar to update\n",
    "                time.sleep(0.1)  # Adjust the sleep duration as needed\n",
    "                progress_bar_eval.update(1)\n",
    "        else:\n",
    "            for i, ((lda_model_gensim, combined_corpus, combined_text), (alpha, beta)) in enumerate(zip(lda_models_eval, itertools.product(alpha_values, beta_values))):\n",
    "                n_topics = list(range(START_TOPICS, END_TOPICS + 1, STEP_SIZE))[i]\n",
    "                lda_model_gensim, combined_corpus, combined_text, convergence_score, perplexity_score, coherence_score = dask.compute(future_eval)\n",
    "\n",
    "                # Save metrics to dictionary for evaluation data\n",
    "                eval_metrics_csv['n_topics'].append(n_topics)\n",
    "                eval_metrics_csv['alpha'].append(alpha)\n",
    "                eval_metrics_csv['beta'].append(beta)\n",
    "                eval_metrics_csv['cv_score'].append(coherence_score)\n",
    "                eval_metrics_csv['convergence_score'].append(convergence_score)\n",
    "                eval_metrics_csv['log_perplexity'].append(perplexity_score)\n",
    "\n",
    "                # Log metrics to a file for evaluation data\n",
    "                log_filename_txt = os.path.join(log_dir, f\"eval_lda_metrics.txt\")\n",
    "\n",
    "                with open(log_filename_txt, 'a') as log_file:\n",
    "                        log_file.write(f\"Number of Topics: {n_topics}  |  \")\n",
    "                        log_file.write(f\"Alpha: {alpha}  |  \")\n",
    "                        log_file.write(f\"Beta: {beta}  |  \")\n",
    "                        log_file.write(f\"Coherence Value (c_v) - Gensim: {coherence_score}  |  \")\n",
    "                        log_file.write(f\"Convergence Score - Gensim: {convergence_score}  |  \")\n",
    "                        log_file.write(f\"Log Perplexity - Gensim: {perplexity_score}\\n\")\n",
    "                # Add a small delay to allow the progress bar to update\n",
    "                time.sleep(0.1)  # Adjust the sleep duration as needed\n",
    "                progress_bar_eval.update(1)\n",
    "    progress_bar_eval.close()\n",
    "\n",
    "\n",
    "\n",
    "    pd.DataFrame(train_metrics_csv).to_csv(f'C:/_harvester/data/lda-models/{DECADE}_html/train-lda-tuning-results.csv', index=False)   \n",
    "    pd.DataFrame(eval_metrics_csv).to_csv(f'C:/_harvester/data/lda-models/{DECADE}_html/eval-lda-tuning-results.csv', index=False) \n",
    "\n",
    "    # Close the Dask client and cluster when done\n",
    "    client.close()\n",
    "    #cluster.close(timeout=60)\n",
    "    cluster.close()\n",
    "    # Check if the Dask client is connected to a scheduler:\n",
    "    if client.status == \"running\":\n",
    "        print(\"Dask client is connected to a scheduler.\")\n",
    "        # Scatter the embedding vectors across Dask workers\n",
    "    else:\n",
    "        print(\"Dask client is not connected to a scheduler.\")\n",
    "\n",
    "    # Check if Dask workers are running:\n",
    "    if len(client.scheduler_info()[\"workers\"]) > 0:\n",
    "        print(\"Dask workers are running.\")\n",
    "    else:\n",
    "        print(\"No Dask workers are running.\")\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Dask client and cluster when done\n",
    "client.close()\n",
    "cluster.close(timeout=60)\n",
    "logging.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the performance curves\n",
    "iterations = range(START_TOPICS, END_TOPICS + 1, STEP_SIZE)\n",
    "\n",
    "# Plotting the performance curves for training data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, train_metrics_csv['cv_score'], label='Training Coherence Score')\n",
    "plt.plot(iterations, train_metrics_csv['convergence_score'], label='Training Convergence Score')\n",
    "plt.plot(iterations, train_metrics_csv['log_perplexity'], label='Training Log Perplexity')\n",
    "\n",
    "train_data_performance_curve = os.path.join(log_dir, f\"training_visual.png\")\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training Data | Evaluation Metrics Comparison')\n",
    "plt.legend()\n",
    "plt.savefig(train_data_performance_curve)  # Save the figure as an image file\n",
    "plt.show()\n",
    "\n",
    "# Plotting the performance curves for evaluation data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, eval_metrics_csv['cv_score'], label='Evaluation Coherence Score')\n",
    "plt.plot(iterations, eval_metrics_csv['convergence_score'], label='Evaluation Convergence Score')\n",
    "plt.plot(iterations, eval_metrics_csv['log_perplexity'], label='Evaluation Log Perplexity')\n",
    "\n",
    "eval_data_performance_curve = os.path.join(log_dir, f\"evaluation_visual.png\")\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Evaluation Data | Evaluation Metrics Comparison')\n",
    "plt.legend()\n",
    "plt.savefig(eval_data_performance_curve)  # Save the figure as an image file\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
